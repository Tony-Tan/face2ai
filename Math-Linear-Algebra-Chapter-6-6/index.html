<!DOCTYPE html><html class="theme-next pisces use-motion" lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=6.4.0" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png?v=6.4.0"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32.png?v=6.4.0"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16.png?v=6.4.0"><link rel="mask-icon" href="/images/logo.png?v=6.4.0" color="#222"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Pisces",version:"6.4.0",sidebar:{position:"left",display:"post",offset:12,b2t:!1,scrollpercent:!0,onmobile:!1},fancybox:!1,fastclick:!1,lazyload:!1,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><meta name="description" content="Abstract: 本文主要介绍根据矩阵对角化以及特征值引出的相似矩阵的性质和特点Keywords: Similar Matrices,Jordan Form,Eigenvalues,Eigenvectors"><meta name="keywords" content="Similar Matrices,Jordan Form,Eigenvalues,Eigenvectors"><meta property="og:type" content="article"><meta property="og:title" content="【线性代数】6-6:相似矩阵(Similar Matrices)"><meta property="og:url" content="http://www.face2ai.com/Math-Linear-Algebra-Chapter-6-6/index.html"><meta property="og:site_name" content="谭升的博客"><meta property="og:description" content="Abstract: 本文主要介绍根据矩阵对角化以及特征值引出的相似矩阵的性质和特点Keywords: Similar Matrices,Jordan Form,Eigenvalues,Eigenvectors"><meta property="og:locale" content="zh-CN"><meta property="og:updated_time" content="2018-07-12T15:45:31.655Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="【线性代数】6-6:相似矩阵(Similar Matrices)"><meta name="twitter:description" content="Abstract: 本文主要介绍根据矩阵对角化以及特征值引出的相似矩阵的性质和特点Keywords: Similar Matrices,Jordan Form,Eigenvalues,Eigenvectors"><link rel="canonical" href="http://www.face2ai.com/Math-Linear-Algebra-Chapter-6-6/"><script type="text/javascript" id="page.configurations">CONFIG.page={sidebar:""}</script><title>【线性代数】6-6:相似矩阵(Similar Matrices) | 谭升的博客</title><script async src="https://www.googletagmanager.com/gtag/js?id=UA-105335860-3"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-105335860-3")</script><noscript><style type="text/css">.sidebar-inner,.use-motion .brand,.use-motion .collection-title,.use-motion .comments,.use-motion .menu-item,.use-motion .motion-element,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .logo,.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">谭升的博客</span> <span class="logo-line-after"><i></i></span></a></div><h1 class="site-subtitle" itemprop="description">Machine Learning & Computer Vision</h1></div><div class="site-nav-toggle"><button aria-label="切换导航栏"><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-首页"><a href="/" rel="section">首页</a></li><li class="menu-item menu-item-数学"><a href="/categories/Mathematic/" rel="section">数学</a></li><li class="menu-item menu-item-···-集合论"><a href="/categories/Mathematic/Set-Theory/" rel="section">··· 集合论</a></li><li class="menu-item menu-item-···-线性代数基础"><a href="/categories/Mathematic/Linear-Algebra/" rel="section">··· 线性代数基础</a></li><li class="menu-item menu-item-···-概率论基础"><a href="/categories/Mathematic/Probability/" rel="section">··· 概率论基础</a></li><li class="menu-item menu-item-···-数理统计学"><a href="/categories/Mathematic/Statistics/" rel="section">··· 数理统计学</a></li><li class="menu-item menu-item-···-数值分析"><a href="/categories/Mathematic/Numerical-Analysis/" rel="section">··· 数值分析</a></li><li class="menu-item menu-item-机器学习算法"><a href="/categories/Machine-Learning/" rel="section">机器学习算法</a></li><li class="menu-item menu-item-强化学习"><a href="/categories/Reinforcement-Learning/" rel="section">强化学习</a></li><li class="menu-item menu-item-深度学习算法"><a href="/categories/Deep-Learning/" rel="section">深度学习算法</a></li><li class="menu-item menu-item-数字图像处理"><a href="/categories/DIP/" rel="section">数字图像处理</a></li><li class="menu-item menu-item-30天自制操作系统"><a href="/categories/30天自制操作系统/" rel="section">30天自制操作系统</a></li><li class="menu-item menu-item-cuda"><a href="/categories/CUDA/" rel="section">CUDA</a></li><li class="menu-item menu-item-网络爬虫"><a href="/categories/Crawler/" rel="section">网络爬虫</a></li><li class="menu-item menu-item-乱七八糟"><a href="/categories/Other/" rel="section">乱七八糟</a></li></ul></nav><img src="https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/Wechat.jpg" alt="wechat"> <a href="http://www.daily-ai.site"><img border="0" src="/images/daily-ai.site.jpg"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-1194454329688573" data-ad-slot="2491973880" data-ad-format="auto" data-full-width-responsive="true"></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></a></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-1194454329688573" data-ad-slot="9135658886" data-ad-format="auto" data-full-width-responsive="true"></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://www.face2ai.com/Math-Linear-Algebra-Chapter-6-6/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Tony"><meta itemprop="description" content="关注机器学习，深度学习，机器视觉，模式识别"><meta itemprop="image" content="/images/avatar.gif"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="谭升的博客"></span><header class="post-header"><h2 class="post-title" itemprop="name headline">【线性代数】6-6:相似矩阵(Similar Matrices)</h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2017-11-29 09:08:12" itemprop="dateCreated datePublished" datetime="2017-11-29T09:08:12+08:00">2017-11-29</time> </span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Mathematic/" itemprop="url" rel="index"><span itemprop="name">Mathematic</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Mathematic/Linear-Algebra/" itemprop="url" rel="index"><span itemprop="name">Linear Algebra</span></a></span> </span><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> 阅读次数： <span class="busuanzi-value" id="busuanzi_value_page_pv"></span></span></div></header><div class="post-body" itemprop="articleBody"><p><strong>Abstract:</strong> 本文主要介绍根据矩阵对角化以及特征值引出的相似矩阵的性质和特点<br><strong>Keywords:</strong> Similar Matrices,Jordan Form,Eigenvalues,Eigenvectors</p><a id="more"></a><h2 id="开篇废话"><a href="#开篇废话" class="headerlink" title="开篇废话"></a>开篇废话</h2><p>一篇一度的废话开始了，谎言和真想到底有什么区别？其实没什么区别，如果你相信谎言不去怀疑，那么你就可以生活在谎言所构造的世界中，或者你想探索真理，那么你就要接受一个有一个残酷的现实，就是你一直被欺骗，而且很多东西已经形成的错误的习惯，那么这个可能很难纠正，两个世界都能承载人的一生，红药丸，蓝药丸，你可以自己选择，对于什么监控坏了，官方辟谣，这些话可能是真的，也可能是假的，所以可以信也可以不信，至于信与不信不过是吃完饭后的谈资而已。<br>特征值特征向量这一章是线性代数的高潮部分，可以说是高潮迭起，这部分相比四个子空间部分可能逻辑性更强一点，需要前后联通，只看一部分肯定要掉坑，所以这几篇写的都非常多，今天的相似矩阵是对角化引出的另一个重要分支，但是篇幅不大。<br>在研究这一章的时候总感觉Prof. Strang写的很细致，可以很容易的帮你知道什么是什么但是如果想了解点深入的背后的东西，由于篇幅限制（可以看出老先生有意的控制篇幅，并没有长的长的短的短，所有章节长度基本相同）没有深入讨论，也可能线性代数的introduction仅限于这些，更深入的话可能就是另一门课了，所以后续可能出个矩阵论或者矩阵分析类的系列博客。</p><h2 id="Similar-Matrices"><a href="#Similar-Matrices" class="headerlink" title="Similar Matrices"></a>Similar Matrices</h2><p>Similar相似，但又不同，如果说某两件事物相似，那么必然有相似点，也就是这两件事物的某一属性，或者某几个属性一致，那么如果说两个矩阵相似，有可能是形状，比如上三角矩阵，对角矩阵，这些矩阵都有相同的属性，我们这里定义矩阵相似–拥有相同的特征值。</p><p>本章我们研究的主要内容是矩阵的对角化，对角化的前提是有足够的特征向量，也就是说如果某个矩阵特征向量不足，那么就没办法产生特征向量矩阵$S$ 那么我们就不研究他们了，😁我们这里不研究特征向量不够的情况，同样，本文中我们也是研究有足够特征向量的矩阵，如果矩阵$A$ invertible,那么可逆矩阵$M$ 构成的$MAM^{-1}$ 的新矩阵是 $A$ 的相似矩阵，因为$M$ 的多样性，所以相似矩阵不是唯一的相反的，应该是一群（family）。</p><blockquote><p>Definition: Let $M$ be any invertible matrix.Then $B=M^{-1}AM$ is similar to $A$</p></blockquote><p>因为我上面剧透了，相似矩阵是有相同特征值的矩阵，但是如果我们不知道这个性质，我们来观察下面这个过程,假设矩阵 $M$ 可逆：<br>$$<br>B=M^{-1}AM\\<br>A=MBM^{-1}\\<br>set:\;N=M^{-1}\\<br>then:\;N^{-1}=M\\<br>A=N^{-1}BN<br>$$</p><p>要说的是 $B=M^{-1}AM$ 和 $A=N^{-1}BN$ 无论是长相和性质都极为相似，那么根据定义A相似与B，B也相似与A，所以相似是个可逆的，即可以反过来的。<br>对角化后的 $\Lambda$ h和原始矩阵A也是相似的，当$M=S$ 的时候，使得相似矩阵B变成了 $\Lambda$</p><p>线性代数和微分方程关系紧密（线性代数是基础学科，所以跟基本谁都有关系），比如对于微分方程 $\frac{du}{dt}=Au$ ,我们对变量 $u$ 进行代换 $u=Mv$ 其中M是可逆的常数矩阵：<br>$$<br>\frac{du}{dt}=Au\\<br>\frac{dMv}{dt}=AMv\\<br>M\frac{dv}{dt}=AMv\\<br>\frac{dv}{dt}=M^{-1}AMv<br>$$</p><p>通过代换u和v，我们得到了A的一个相似矩阵，也可以通过将M变成S，这样得到的系数矩阵就是对角矩阵，其实前面有一篇专门讲微分方程的课我们略过了，实际上特征值是可以反应出系统是逐渐偏向增加还是减少的（这个地方听不懂没关系，因为出来的太突然了，没有一点点防备，你就这样出现。。）所以相似矩阵必然有一样的特征值，这样才能进行代换后，不影响系统原始的变化方式，那么，又是那句话，相似矩阵家族的特征值是相同的不然不相似。</p><p>下面的关键就是证明特征值没变,假设矩阵A是可对角化的矩阵。</p><p>$$<br>A=S_A\Lambda S_A^{-1}\\<br>B=M^{-1}AM\\<br>B=M^{-1}S_A\Lambda S_A^{-1}M\\<br>B=(M^{-1}S_A)\Lambda (M^{-1}S_A)^{-1}\\<br>S_B=M^{-1}S_A\\<br>B=S_B\Lambda S_B^{-1}<br>$$</p><p>可见B和A的特征值是不变的，然后特征向量矩阵要乘以系数矩阵 $M^{-1}$ ，所以下面这个过程是等价的：<br>$$<br>B=M^{-1}AM\\<br>Bx=\lambda x\\<br>M^{-1}AMx=\lambda x\\<br>AMx=M\lambda x\\<br>AMx=\lambda (Mx)\\<br>set:\; y=Mx\\<br>Ay=\lambda y<br>$$</p><p>又一次证明了相似矩阵的特征值不变，同时特征向量变化等于 $Mx$ 就是原始特征值乘以了系数矩阵。这里面还有一个隐藏的坑，就是特征值相等的时候，如果矩阵A有n个相等的特征值 $\lambda$，那么B重也有n个相等的特征值 $\lambda$ 。这种情况有些困难，下面开始举🌰 ：<br>$$<br>A=\begin{bmatrix}0.5&amp;0.5\\0.5&amp;0.5\end{bmatrix}\\<br>S_A=\begin{bmatrix}1&amp;1\newline -1&amp;1\end{bmatrix}\\<br>S_AAS_A^{-1}=\begin{bmatrix}1&amp;0\\0&amp;0\end{bmatrix}\\<br>Chose:\\<br>M=\begin{bmatrix}1&amp;0\\1&amp;-2\end{bmatrix}\\<br>then:\\<br>B=M^{-1}AM=\begin{bmatrix}1&amp;-1\\0&amp;0\end{bmatrix}\\<br>S_B=\begin{bmatrix}1&amp;1\\0&amp;1\end{bmatrix}\\<br>S_B^{-1}=\begin{bmatrix}1&amp;-1\\0&amp;1\end{bmatrix}\\<br>S_BBS_B^{-1}=\begin{bmatrix}1&amp;0\\0&amp;0\end{bmatrix}<br>$$</p><p>可以通过简单的计算得到与证明中一致的结论，本例子和书中有写区别，用octave进行了验证，可以放心使用</p><p>下面再举一个比较困难的🌰就是两个特征值相等的情况。<br>$$<br>A=\begin{bmatrix}0&amp;1\\0&amp;0\end{bmatrix}\\<br>\lambda_1=0\\<br>\lambda_2=0\\<br>$$<br>这样的话两个特征值都为0，那么特征向量就是A的0空间，又因为矩阵的rank是1，也就是说nullspace的dimension是1，所以两个特征值对应一个个方向上的特征向量,$S=\begin{bmatrix}1&amp;1\\0&amp;0\end{bmatrix}$ 不可逆， A不能对角化，但是我们依然可以找到可逆矩阵$M=\begin{bmatrix}a&amp;b\\c&amp;d\end{bmatrix}$ 他的逆是$M^{-1}=\frac{1}{ad-bc}\begin{bmatrix}d&amp;-b\newline -c&amp;a\end{bmatrix}$ ，那么我们就能得出一个通用的框架，满足$B=M^{-1}AM$ 的所有矩阵与A相似，虽然A没有足够的特征向量（文章开始说不研究的，但是还是被带进坑了）</p><p>$$<br>B=\frac{1}{ad-bc}\begin{bmatrix}d&amp;-b\newline -c&amp;a\end{bmatrix}\begin{bmatrix}0&amp;1\\0&amp;0\end{bmatrix}\begin{bmatrix}a&amp;b\\c&amp;d\end{bmatrix}=\begin{bmatrix}cd&amp;d^2\newline -c^2&amp;-cd\end{bmatrix}<br>$$<br>满足上述公式的所有B（必须可逆，也就是行列式要不等于0）都是A的相似矩阵，当然这个特殊的例子也要强调一下：<strong>除了</strong> $\begin{bmatrix}0&amp;0\\0&amp;0\end{bmatrix}$ ，这个家伙比较讨厌也不可逆，所以我们把它排除了。观察上面B，发现trace是0，行列式也是0，满足检验。<br>B是A的一个通用版本，但是可以看出没办搞成对角矩阵，A已经是最接近对角矩阵的了，我们把A称作是这个family的Godfather，或者学术一点叫做 Jordan Form，后面我们会大概的介绍Jordan form更多内容要看进阶篇了。</p><p>这是一个比较特殊的例子，特征向量不够的情况，但是不论够不够，相似矩阵总有下面这些性质</p><table><thead><tr><th style="text-align:center">Not changed by M</th><th style="text-align:center">Changed by M</th></tr></thead><tbody><tr><td style="text-align:center">Eigenvalues</td><td style="text-align:center">Eigenvectors</td></tr><tr><td style="text-align:center">Trace and determinant</td><td style="text-align:center">Nullspace</td></tr><tr><td style="text-align:center">Rank</td><td style="text-align:center">Column space</td></tr><tr><td style="text-align:center">Number of independent eigenvectors</td><td style="text-align:center">Row space</td></tr><tr><td style="text-align:center">Jordan form</td><td style="text-align:center">Left nullspace</td></tr><tr><td style="text-align:center"></td><td style="text-align:center">Sigular values</td></tr></tbody></table><p>一条一条分析，首先特征值不变，我们已经论证过了，同时也知道特征向量发生了变化，特征值不变以为着Trace和determinant的不变（因为他们和特征值之间有数值关系，所以特征值不变这两个可能不变）；<strong><em>Nullspace发生了变化，因为Nullspace是特征值为0的时候的特征向量span的space，如果特征向量发生变化，那么Nullspace肯定也发生了变化，这里有点疑问，如果M是identity呢？</em></strong> ；Rank是不变的，因为如果特征值不变，0特征值的个数就不变，这样nullspace的维度就不变，那么rank不变；与nullspace同理，非零特征值对应的特征向量发生改变，其span的column space就会发生改变；independent的特征向量的数量不会发生变化，因为对应的子空间的维度不变，所以线性独立的特征向量（也就是子空间的一组基的数量）不会发生变化；row space发生变化，因为行空间的基都乘以了M；Jordan不变，因为不改变相似性，在整个家族中Jordan是最接近对角矩阵的，所以不会发生改变；left nullspace随着行空间改变而被改变；Singular values会改变，这个下一篇会讲到，这里大家还不太明确奇异值是什么。</p><h2 id="Examples-of-the-Jordan-form"><a href="#Examples-of-the-Jordan-form" class="headerlink" title="Examples of the Jordan form"></a>Examples of the Jordan form</h2><p>Jordan Form上文说是最接近对角矩阵的矩阵，这句话有个问题，就是如何评价什么“接近”，也就是没有一个统一的标准，来计算一个矩阵和对角矩阵的差别有多大。所以我们在这段和下一段找找根据，这段主要是🌰 ：<br>Jordan Matrix J:<br>$$<br>J=\begin{bmatrix}5&amp;1&amp;0\\0&amp;5&amp;1\\0&amp;0&amp;5\end{bmatrix}\\<br>then:\\<br>J-5I=\begin{bmatrix}0&amp;1&amp;0\\0&amp;0&amp;1\\0&amp;0&amp;0\end{bmatrix}<br>$$<br>所有与J相似的矩阵B都有三个一样的特征值5，并且 $B-5I$ 的rank不变（是2），也就是nullspace是1，这样的矩阵B与J相似（<strong>这里有个问题，就是在确定特征值相等后，还有反复确认rank和nullspace，这个也是这里第一次见，后面可能有更具体的解释</strong>）</p><blockquote><p>Jordan Theorem :$J^T$ 和 $J$ 相似</p></blockquote><p>$$<br>J^T=M^{-1}JM=<br>\begin{bmatrix}&amp;&amp;1\\&amp;1&amp;\\1&amp;&amp;\end{bmatrix}<br>\begin{bmatrix}5&amp;0&amp;0\\1&amp;5&amp;0\\0&amp;1&amp;5\end{bmatrix}<br>\begin{bmatrix}&amp;&amp;1\\&amp;1&amp;\\1&amp;&amp;\end{bmatrix}<br>$$<br>数值上支持了理论，但是Jordan Theorem具体内容还应该具体的证明，例子只是从表面上让大家知道说的是什么，但本身没有证明真伪的意义。</p><p>另一个例子，这个例子是微分方程，与上面提到的一样，这里不再具体描述了，但是要记住：<br>对变量 $u$ 进行代换 $u=Mv$ 其中M是可逆的常数矩阵：<br>$$<br>\frac{du}{dt}=Au\\<br>\frac{dMv}{dt}=AMv\\<br>M\frac{dv}{dt}=AMv\\<br>\frac{dv}{dt}=M^{-1}AMv<br>$$</p><p>这里是对输入，或者是初始状态进行了变换，下一章将对基进行变化（也就是一个动坐标值，一个动坐标轴）。</p><h2 id="The-Jordan-Form"><a href="#The-Jordan-Form" class="headerlink" title="The Jordan Form"></a>The Jordan Form</h2><p>接下来详细的介绍下Jorda Form也就是Jordan形式，没有详细的推到过程，因为对Jordan形式的严格证明书中就没有，也不是线性代数初级部分的内容，我们要知道的就是Jordan的具体形式，以及部分用途，Jordan存在的最大意义就是解决不能对角化的矩阵在需要对角化的时候的引发的问题，也就是Jordan Form 是更广义的对角化，而之前我们讲的拥有足够特征向量的方阵的对角化只是Jordan Form的一种特例而已，Jordan是更广泛的方式。</p><p>【宏观】Jordan Form：<br>$$<br>M^{-1}AM=\begin{bmatrix}<br>J_1&amp;&amp;\\<br>&amp;\ddots&amp;\\<br>&amp;&amp;J_s<br>\end{bmatrix}=J<br>$$<br>【微观】其中$J_i$ 是一个块：<br>$$<br>J_i=\begin{bmatrix}<br>\lambda_i&amp;1&amp;&amp;&amp;\\<br>&amp;\ddots &amp; \ddots&amp;&amp;\\<br>&amp;&amp; \ddots&amp;1\\<br>&amp;&amp; &amp;\lambda_i<br>\end{bmatrix}<br>$$<br>中文描述，对于整体，就是一些Jordan块在矩阵的对角线上（还记得矩阵分块不？就是这个矩阵块），但是每个块里也有特殊的规定，就是对角线上是一个特征值，注意是一个，然后剩下的，每个特征值头顶上的元素是1，就这样。<br>每个Jordan块上有一个特征值，对应一个特征向量。</p><p>在判断矩阵相似上我们也有了另一种描述矩阵similar中如果A和B相似，他们共享（share）一个Jordan形式，或者说这个定义才是最优版本。<br>Jordan形式在微分方程，求矩阵高次幂的时候都非常有用，但在实际计算中Jordan Form一般不被使用，因为计算量大，我们希望使用计算量最小的求解方法，A slight change对A就可以把A对角化。</p><p>经典名言：</p><blockquote><p>Proved or not,you have caught the central idea of similarity-to make A as simple as possible while preserving its esstntial properties</p></blockquote><p>上面这个可以当做线性代数的至理名言：<br><strong><em>当一个矩阵被搞得非常非常简单的时候，这个简单的矩阵将保持着最基础的性质</em></strong></p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>本文主要介绍了Similar矩阵，距离最后的高潮只差一下了，大家努力吧，明天SVD</p></div><div><div style="padding:10px 0;margin:20px auto;width:90%;text-align:center"><div>可怜可怜我吧</div><button id="rewardButton" disable="enable" onclick='var qr=document.getElementById("QR");"none"===qr.style.display?qr.style.display="block":qr.style.display="none"'><span>打赏</span></button><div id="QR" style="display:none"><div id="wechat" style="display:inline-block"><img id="wechat_qr" src="/images/weixin.png" alt="Tony 微信支付"><p>微信支付</p></div><div id="alipay" style="display:inline-block"><img id="alipay_qr" src="/images/alipay.png" alt="Tony 支付宝"><p>支付宝</p></div></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者： </strong>Tony</li><li class="post-copyright-link"><strong>本文链接：</strong> <a href="http://www.face2ai.com/Math-Linear-Algebra-Chapter-6-6/" title="【线性代数】6-6:相似矩阵(Similar Matrices)">http://www.face2ai.com/Math-Linear-Algebra-Chapter-6-6/</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/Math-Linear-Algebra-Chapter-6-5/" rel="next" title="【线性代数】6-5:正定矩阵(Positive Definite Matrices)"><i class="fa fa-chevron-left"></i> 【线性代数】6-5:正定矩阵(Positive Definite Matrices)</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/Math-Linear-Algebra-Chapter-6-7/" rel="prev" title="【线性代数】6-7:SVD分解(Singular Value Decomposition-SVD)">【线性代数】6-7:SVD分解(Singular Value Decomposition-SVD) <i class="fa fa-chevron-right"></i></a></div></div></footer></div></article></div><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-1194454329688573" data-ad-slot="2491973880" data-ad-format="auto" data-full-width-responsive="true"></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">站点概览</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/avatar.gif" alt="Tony"><p class="site-author-name" itemprop="name">Tony</p><p class="site-description motion-element" itemprop="description">关注机器学习，深度学习，机器视觉，模式识别</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives"><span class="site-state-item-count">258</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/index.html"><span class="site-state-item-count">17</span> <span class="site-state-item-name">分类</span></a></div></nav><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/tony-tan" target="_blank" title="GitHub" rel="external nofollow"><i class="fa fa-fw fa-github"></i>GitHub</a> </span><span class="links-of-author-item"><a href="mailto:tony.face2ai@gmail.com" target="_blank" title="E-Mail" rel="external nofollow"><i class="fa fa-fw fa-envelope"></i>E-Mail</a> </span><span class="links-of-author-item"><a href="https://twitter.com/Tony_Face2AI" target="_blank" title="Twitter" rel="external nofollow"><i class="fa fa-fw fa-twitter"></i>Twitter</a></span></div></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#开篇废话"><span class="nav-number">1.</span> <span class="nav-text">开篇废话</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Similar-Matrices"><span class="nav-number">2.</span> <span class="nav-text">Similar Matrices</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Examples-of-the-Jordan-form"><span class="nav-number">3.</span> <span class="nav-text">Examples of the Jordan form</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#The-Jordan-Form"><span class="nav-number">4.</span> <span class="nav-text">The Jordan Form</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Conclusion"><span class="nav-number">5.</span> <span class="nav-text">Conclusion</span></a></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2018</span> <span class="with-love" id="animate"><i class="fa fa-"></i> </span><span class="author" itemprop="copyrightHolder">Tony</span></div><div class="busuanzi-count"><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="site-uv" title="总访客量"><i class="fa fa-user"></i> <span class="busuanzi-value" id="busuanzi_value_site_uv"></span> </span><span class="site-pv" title="总访问量"><i class="fa fa-eye"></i> <span class="busuanzi-value" id="busuanzi_value_site_pv"></span></span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span id="scrollpercent"><span>0</span>%</span></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/js/src/utils.js?v=6.4.0"></script><script type="text/javascript" src="/js/src/motion.js?v=6.4.0"></script><script type="text/javascript" src="/js/src/affix.js?v=6.4.0"></script><script type="text/javascript" src="/js/src/schemes/pisces.js?v=6.4.0"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=6.4.0"></script><script type="text/javascript" src="/js/src/post-details.js?v=6.4.0"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=6.4.0"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      TeX: {equationNumbers: { autoNumber: "AMS" }}
    });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });</script><script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>