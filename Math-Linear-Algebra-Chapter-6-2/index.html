<!DOCTYPE html><html class="theme-next pisces use-motion" lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=6.4.0" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png?v=6.4.0"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32.png?v=6.4.0"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16.png?v=6.4.0"><link rel="mask-icon" href="/images/logo.png?v=6.4.0" color="#222"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Pisces",version:"6.4.0",sidebar:{position:"left",display:"post",offset:12,b2t:!1,scrollpercent:!0,onmobile:!1},fancybox:!1,fastclick:!1,lazyload:!1,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><meta name="description" content="Abstract: 矩阵对角化，以及对角化过程中引入的知识，以及对角化的应用Keywords: Eigenvalues,Eigenvectors,Diagonalizing,Fibonacci Numbers, $A^k$ ,Nondiagonalizable Matrix"><meta name="keywords" content="Eigenvalues,Eigenvectors,Diagonalizing,Fibonacci Numbers,$A^k$,Nondiagonalizable Matrix"><meta property="og:type" content="article"><meta property="og:title" content="【线性代数】6-2:对角化(Diagonalizing a Matrix)"><meta property="og:url" content="http://www.face2ai.com/Math-Linear-Algebra-Chapter-6-2/index.html"><meta property="og:site_name" content="谭升的博客"><meta property="og:description" content="Abstract: 矩阵对角化，以及对角化过程中引入的知识，以及对角化的应用Keywords: Eigenvalues,Eigenvectors,Diagonalizing,Fibonacci Numbers, $A^k$ ,Nondiagonalizable Matrix"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/Math-Linear-Algebra-Chapter-6-2/python.png"><meta property="og:updated_time" content="2018-07-12T15:45:31.641Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="【线性代数】6-2:对角化(Diagonalizing a Matrix)"><meta name="twitter:description" content="Abstract: 矩阵对角化，以及对角化过程中引入的知识，以及对角化的应用Keywords: Eigenvalues,Eigenvectors,Diagonalizing,Fibonacci Numbers, $A^k$ ,Nondiagonalizable Matrix"><meta name="twitter:image" content="https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/Math-Linear-Algebra-Chapter-6-2/python.png"><link rel="canonical" href="http://www.face2ai.com/Math-Linear-Algebra-Chapter-6-2/"><script type="text/javascript" id="page.configurations">CONFIG.page={sidebar:""}</script><title>【线性代数】6-2:对角化(Diagonalizing a Matrix) | 谭升的博客</title><script async src="https://www.googletagmanager.com/gtag/js?id=UA-105335860-3"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-105335860-3")</script><noscript><style type="text/css">.sidebar-inner,.use-motion .brand,.use-motion .collection-title,.use-motion .comments,.use-motion .menu-item,.use-motion .motion-element,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .logo,.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">谭升的博客</span> <span class="logo-line-after"><i></i></span></a></div><h1 class="site-subtitle" itemprop="description">Machine Learning & Computer Vision</h1></div><div class="site-nav-toggle"><button aria-label="切换导航栏"><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-首页"><a href="/" rel="section">首页</a></li><li class="menu-item menu-item-数学"><a href="/categories/Mathematic/" rel="section">数学</a></li><li class="menu-item menu-item-···-集合论"><a href="/categories/Mathematic/Set-Theory/" rel="section">··· 集合论</a></li><li class="menu-item menu-item-···-线性代数基础"><a href="/categories/Mathematic/Linear-Algebra/" rel="section">··· 线性代数基础</a></li><li class="menu-item menu-item-···-概率论基础"><a href="/categories/Mathematic/Probability/" rel="section">··· 概率论基础</a></li><li class="menu-item menu-item-···-数理统计学"><a href="/categories/Mathematic/Statistics/" rel="section">··· 数理统计学</a></li><li class="menu-item menu-item-···-数值分析"><a href="/categories/Mathematic/Numerical-Analysis/" rel="section">··· 数值分析</a></li><li class="menu-item menu-item-机器学习算法"><a href="/categories/Machine-Learning/" rel="section">机器学习算法</a></li><li class="menu-item menu-item-强化学习"><a href="/categories/Reinforcement-Learning/" rel="section">强化学习</a></li><li class="menu-item menu-item-深度学习算法"><a href="/categories/Deep-Learning/" rel="section">深度学习算法</a></li><li class="menu-item menu-item-数字图像处理"><a href="/categories/DIP/" rel="section">数字图像处理</a></li><li class="menu-item menu-item-30天自制操作系统"><a href="/categories/30天自制操作系统/" rel="section">30天自制操作系统</a></li><li class="menu-item menu-item-cuda"><a href="/categories/CUDA/" rel="section">CUDA</a></li><li class="menu-item menu-item-网络爬虫"><a href="/categories/Crawler/" rel="section">网络爬虫</a></li><li class="menu-item menu-item-乱七八糟"><a href="/categories/Other/" rel="section">乱七八糟</a></li></ul></nav><img src="https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/Wechat.jpeg" alt="wechat"> <a href="http://www.daily-ai.site"><img border="0" src="/images/daily-ai.site.jpg"></a></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-1194454329688573" data-ad-slot="9135658886" data-ad-format="auto" data-full-width-responsive="true"></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://www.face2ai.com/Math-Linear-Algebra-Chapter-6-2/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Tony"><meta itemprop="description" content="关注机器学习，深度学习，机器视觉，模式识别"><meta itemprop="image" content="/images/avatar.gif"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="谭升的博客"></span><header class="post-header"><h2 class="post-title" itemprop="name headline">【线性代数】6-2:对角化(Diagonalizing a Matrix)</h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2017-11-21 11:48:42" itemprop="dateCreated datePublished" datetime="2017-11-21T11:48:42+08:00">2017-11-21</time> </span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Mathematic/" itemprop="url" rel="index"><span itemprop="name">Mathematic</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Mathematic/Linear-Algebra/" itemprop="url" rel="index"><span itemprop="name">Linear Algebra</span></a></span> </span><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> 阅读次数： <span class="busuanzi-value" id="busuanzi_value_page_pv"></span></span></div></header><div class="post-body" itemprop="articleBody"><p><strong>Abstract:</strong> 矩阵对角化，以及对角化过程中引入的知识，以及对角化的应用<br><strong>Keywords:</strong> Eigenvalues,Eigenvectors,Diagonalizing,Fibonacci Numbers, $A^k$ ,Nondiagonalizable Matrix</p><a id="more"></a><h2 id="开篇废话"><a href="#开篇废话" class="headerlink" title="开篇废话"></a>开篇废话</h2><p>这几天没写博客，也没干正事，原因有很多，内心极度崩溃的状态，有些好转，所以继续写博客，算是在乱世中寻得一片宁静的天地。<br>很多事是不以意志而转移的，说白了就是你可能极力的不去惹事而希望能专心去做一件事，但是突然发现条件根本不允许，各种事情会来找到你，其实这些事都是你之前做的事的后续连续效果，也可以说成蝴蝶效应，没办法，这些事情你躲不开的，只能一件件自己处理好，“出来混迟早要还的”。</p><h2 id="Diagonalizing-a-Matrix"><a href="#Diagonalizing-a-Matrix" class="headerlink" title="Diagonalizing a Matrix"></a>Diagonalizing a Matrix</h2><p>对角化一个矩阵，和之前个种各样的分解有一个同样的思路，当矩阵从原始形态通过各种计算性质变形成为各种有规则的，或者在数值上有特殊的性质，这些特殊的形状都可以用在不同问题上，比如LDR分解可以直接求出pivot值，求解方程，QR分解可以是通过变换向量空间的基来使向量某些方面的性质凸显出来。<br>今天说的对角化就是利用了特征值特征向量的计算性质，通过对 $Ax=\lambda x$ 进行变形引申得到的。而这个diagonalizing后的矩阵对于矩阵求幂有非常简单的计算。<br>假设 $n \times n$ 的矩阵 $A$ 有n个特征向量，那么我们把每个特征向量按照每列一个特征向量的组合方式形成一个矩阵，那么这个矩阵我们称之为 $S$</p><p>$$<br>AS=<br>A\begin{bmatrix}<br>\vdots &amp;\dots &amp;\vdots\\<br>x_1&amp;\dots &amp;x_n\\<br>\vdots &amp;\dots &amp;\vdots<br>\end{bmatrix}=<br>\begin{bmatrix}<br>\vdots &amp;\dots &amp;\vdots\\<br>Ax_1&amp;\dots &amp;Ax_n\\<br>\vdots &amp;\dots &amp;\vdots<br>\end{bmatrix}=<br>\begin{bmatrix}<br>\vdots &amp;\dots &amp;\vdots\\<br>\lambda_1 x_1&amp;\dots &amp;\lambda_n x_n\\<br>\vdots &amp;\dots &amp;\vdots<br>\end{bmatrix}\\<br>\begin{bmatrix}<br>\vdots &amp;\dots &amp;\vdots\\<br>\lambda_1 x_1&amp;\dots &amp;\lambda_n x_n\\<br>\vdots &amp;\dots &amp;\vdots<br>\end{bmatrix}=<br>\begin{bmatrix}<br>\vdots &amp;\dots &amp;\vdots\\<br>x_1&amp;\dots &amp;x_n\\<br>\vdots &amp;\dots &amp;\vdots<br>\end{bmatrix}\begin{bmatrix}<br>\lambda_1 &amp; &amp;\\<br>&amp;\ddots &amp;\\<br>&amp;&amp;\lambda_n<br>\end{bmatrix}=S\Lambda\\<br>so:<br>AS=S\Lambda\\<br>\Lambda=S^{-1}AS\\<br>A=S\Lambda S^{-1}<br>$$<br>$\Lambda$ 是 $\lambda$ 的大写，表示的是对角矩阵，每个元素都是eigenvalue。<br>如果矩阵A没有n个independence的eigenvector也是无法对角化的，上面的推到过程是属于两头堵的方式，先正向求出 $AS$ 的结果发现其结果和 $S\Lambda$ 结果一样，所以就得到了 $\Lambda$ 的表达式，下面我们我们就可以来计算 $A^k$ 了，利用上面推到过程中的最后一步，这个简直非常完美了<br>$$<br>A^k=A\cdot A\dots A=S \Lambda S^{-1} S \Lambda S^{-1} \cdots S \Lambda S^{-1}=S \Lambda \Lambda \cdots \Lambda S^{-1}=S \Lambda^k S^{-1}<br>$$<br>一个矩阵的k次幂等于其对角矩阵的k次幂– $S \Lambda^k S^{-1}$<br>我们可以回忆下上一篇，我们求过一个矩阵的k次方乘以一个向量 $A^ky$ ,用特征向量来作为 $y$ 的基，然后写成<br>$$<br>A^k:\\<br>suppose: \;C=\begin{bmatrix}c_1 &amp;\dots &amp; c_n\end{bmatrix}\\<br>y=c_1 x_1+c_2 x_2+\dots +c_n x_n=SC \\<br>A^k y=A^k(c_1 x_1+c_2 x_2+\dots +c_n x_n)\\<br>=c_1A^kx_1+c_2A^kx_2+\dots +c_nA^kx_n\\<br>=c_1\lambda_1^k x_1+c_2\lambda_2^k x_2+\dots + c_n\lambda_n^k x_n\\<br>=S\Lambda^k C<br>$$</p><p>上面这个是回忆上一篇的内容同时通过这篇的内容加以结合，也是 $A^k$ 小节的主要过程，就得到了对角矩阵的一个应用，原理一致，方法不同而已，最终的理论根基都是 $Ax=\lambda x$这个是本章最核心的方程，没有之一，就是最核心的，而且本章作为线性代数的高潮部分，这个方程也可以称之为线性代数中最重要的方程之一。</p><p>在使用 $\Lambda$ 之前，我们有几个remark需要强调一下：</p><ol><li>没有重复特征值的矩阵可以被对角化</li><li>特征向量可以任意乘以一个非零常数（长度可以缩放）</li><li>对角矩阵中特征值的顺序与S中特征向量的排列顺序对应</li><li>有些矩阵没有足够的eigenvalue，所以也没有足够的eigenvector来组成S，这类矩阵不能被对角化。</li></ol><p><strong>注意</strong> 矩阵对角化和是否可逆没有直接关系</p><blockquote><p>可逆表示需要行列式非零，行列式非零对特征值是有影响的，表明特征值不能为0<br>可对角化是说特征向量必须足够，也就是不能有特征向量缺失，$n \times n$ 就应该有n个特征向量</p></blockquote><p>一个重要的结论：<strong>如果有n个不同的eigenvalues，那么就会对应有n个independence 的eigenvectors，那么这个矩阵可以被对角化;也就是说，如果矩阵有n个不同的eigenvalue，那么矩阵可以被对角化</strong><br>证明 $2\times 2$ 矩阵的情况 :<br>$$<br>Suppose :\\<br>c_1x_1+c_2x_2=0\\<br>A(c_1x_1+c_2x_2)=0\\<br>c_1Ax_1+c_2Ax_2=0\\<br>\lambda_1 c_1x_1+\lambda_2 c_2x_2=0\\<br>\lambda_1(c_1x_1+c_2x_2)=0\\<br>so:\\<br>\lambda_1 c_1x_1+\lambda_2 c_2x_2 - \lambda_1(c_1x_1+c_2x_2)=0\\<br>(\lambda_2 - \lambda_1 )c_2x_2=0\\<br>for:\\<br>\lambda_2 \neq \lambda_1 \\<br>so:\\<br>c_2=0\\<br>similarly:\\<br>c_1=0<br>$$<br>那么一开始的假设中只有 $c_1=0$ 和 $c_2=0$ 满足 $c_1x_1+c_2x_2=0$ 也就是说 $x_1,x_2$ 线性无关<br>证明可以直接被推广到多维。<br>这个证明很是巧妙，以至于我也看了半天才明白套路，通过利用特征向量和矩阵相乘得到特征值的性质，以及nullspace的性质来证明，不同的特征值对应的特征向量彼此之间独立。<br>通过对角化的方法可以轻松得出markov矩阵的性质，一个特征值为1，那么它对应的特征向量在k次幂后是稳定的，另一个小于1的将会被消灭。<br>那么什么时候$A^k$会自我毁灭，没错，如果矩阵的所有特征值都小于1，那么就毁灭了 $|\lambda|&lt;1$</p><h2 id="Fibonacci-Numbers"><a href="#Fibonacci-Numbers" class="headerlink" title="Fibonacci Numbers"></a>Fibonacci Numbers</h2><p>斐波那契数列，高中的时候学的生兔子什么的，之前新闻上还说有个女博士用这玩意炒股，赚到翻，然后，C语言刚学了一个月的时候，老师说能把这个做出来说明学的不错了，不过好像确实不太好写，我们来段代码，我们用python来写一下试试：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">N=10</span><br><span class="line">a=1</span><br><span class="line">b=1</span><br><span class="line">print a</span><br><span class="line">print b</span><br><span class="line">for i in range(N-2):</span><br><span class="line">    c=a+b</span><br><span class="line">    a=b</span><br><span class="line">    b=c</span><br><span class="line">    print c</span><br></pre></td></tr></table></figure><p></p><p>输出：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">1</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">5</span><br><span class="line">8</span><br><span class="line">13</span><br><span class="line">21</span><br><span class="line">34</span><br><span class="line">55</span><br><span class="line">89</span><br><span class="line">144</span><br></pre></td></tr></table></figure><p></p><p>通过计算机程序可以很快的得到任意项的结果，但是从数学的角度，我们也很关心他的增长率，也就是他是怎么增长的？线性？二次？还是指数增长？<br>Fibonacci Numbers用递归公式来表示：<br>$$<br>a_n=a_{n-1}+a_{n-2} \,\, where\,\,n&gt;2<br>$$<br>如果我们把这个递归关系写成矩阵形式，首先，我们需要弄个方阵出来，搞两个向量不太靠谱，没办法对角化，所以我们加一个<br>$$<br>\begin{bmatrix}a_n\\a_{n-1}\end{bmatrix}=<br>\begin{bmatrix}1&amp;1\\1&amp;0\end{bmatrix}\begin{bmatrix}a_{n-1}\\a_{n-2}\end{bmatrix}<br>$$<br>这个可以叫做矩阵形式的递归，通过 $\begin{bmatrix}a_{n-1}\\a_{n-2}\end{bmatrix}$ 来推导出 $\begin{bmatrix}a_n\\a_{n-1}\end{bmatrix}$ 递归得出，而中间的系数就比较有趣了，因为我们可以得出<br>$$<br>\begin{bmatrix}a_n\\a_{n-1}\end{bmatrix}=<br>\begin{bmatrix}1&amp;1\\1&amp;0\end{bmatrix}^{n-1}\begin{bmatrix}a_2\\a_1\end{bmatrix} \, where \, n&gt;2<br>$$<br>这样问题就转换到 $A^k$ 的问题上了<br>$$<br>A=\begin{bmatrix}1&amp;1\\1&amp;0\end{bmatrix}\\<br>$$<br>求A的特征值，特征向量<br>$$<br>Ax=\lambda x\\<br>det(\begin{bmatrix}1-\lambda &amp;1\\1&amp;0-\lambda\end{bmatrix})=0\\<br>\lambda^2-\lambda-1=0\\<br>\lambda_1=\frac{1+\sqrt{5}}{2} \approx 1.618 \\<br>\lambda_2=\frac{1-\sqrt{5}}{2} \approx -.618 \\<br>x_1=\begin{bmatrix}\frac{1+\sqrt{5}}{2}\\1\end{bmatrix}\\<br>x_2=\begin{bmatrix}\frac{1-\sqrt{5}}{2}\\1\end{bmatrix}<br>$$<br>Fibonnaci 数列的递归系数矩阵的特征值是黄金分割比！是不是很神奇！也就是说矩阵在k次后 $\lambda_1$ 将会成为主要的增长系数 $\lambda_2$ 由于小于1 将会被消灭。<br>比如100次方后将约等于<br>$$<br>\begin{bmatrix}1\\0\end{bmatrix}=c_1x_1+c_2x_2=<br>c_1\begin{bmatrix}\frac{1+\sqrt{5}}{2}\\1\end{bmatrix}+<br>c_2\begin{bmatrix}\frac{1-\sqrt{5}}{2}\\1\end{bmatrix}\\<br>c_1=\frac{1}{\sqrt{5}}\\<br>c_2=-\frac{1}{\sqrt{5}}\\<br>\begin{bmatrix}a_{100}\\a_{99}\end{bmatrix}=\frac{1}{\sqrt{5}} \lambda_1^{99}x_1-\frac{1}{\sqrt{5}} \lambda_2^{99}x_2=\frac{1}{\sqrt{5}} \lambda_1^{99}\begin{bmatrix}\frac{1+\sqrt{5}}{2}\\1\end{bmatrix}\\<br>a_{100}=\frac{1}{\sqrt{5}}(\frac{1+\sqrt{5}}{2})^{99}\frac{1+\sqrt{5}}{2}=3.534\times 10^{20}<br>$$<br>检验一下：<br><img src="https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/Math-Linear-Algebra-Chapter-6-2/python.png" alt=""><br>基本一致</p><h2 id="Matrix-Powers-A-k"><a href="#Matrix-Powers-A-k" class="headerlink" title="Matrix Powers $A^k$"></a>Matrix Powers $A^k$</h2><p>Fibonnaci Numbers是一个典型的差分方程 $u_{k+1}=Au_k$ Solution is $u_k=A^ku_0$ 这就是一个典型的解过程，关键环节就是 $A^k$ ,过程和上面解决问题的关键步骤一般分为三步：</p><ol><li>分解成以特征向量为基的线性组合 $u_0=Sc$</li><li>Multiplies $\Lambda^k$</li><li>$u_k=\sum c_i(\lambda_i)^kx_i$ 求解S和 $\Lambda^k$ 和 $S^{-1}u_0$ 的积</li></ol><p>所以:<br>$$<br>A^ku_0=S\Lambda^kS^{-1}u_0=S\Lambda^kc\\<br>u_k=c_1(\lambda_1)^kx_1+\dots + c_n(\lambda_n)^kx_n<br>$$<br>这个就是 $u_k=Au_{k-1}$的解</p><p>所以我们的对于这种迭代关系是线性的差分方程，解法就是通过将初始条件分解成特征向量的线性组成，然后通过特征值的幂和特征向量矩阵的组合，得到解。<br>transforming to an eigenvector basis 是一种非常经典的做法，比如傅里叶级数都是典型应用。</p><h2 id="Nondiagonalizable-Matrices"><a href="#Nondiagonalizable-Matrices" class="headerlink" title="Nondiagonalizable Matrices"></a>Nondiagonalizable Matrices</h2><p>并不是所有的矩阵都能对角化的，前面也说了，有些特征值重复或者有些解不存在的时候，那么如何判断是否可以对角化呢？（就像判断是否可逆的那种方法）<br>我们可以考察两种指标来确定是否能对角化</p><ol><li>Geometric Multiplicity=GM ,计算线性独立的eigenvector 的数量，也就是$A-\lambda I$ 的nullspace 的维度</li><li>Algebra Multiplicity=AM ,计算重复的 $\lambda$ 主要考察 $det(A-\lambda I)=0$ 的解</li></ol><p>GM和AM保持关系 $GM \leq AM$<br>当$GM &lt; AM$ 时 矩阵不可对角化</p><h2 id="Eigenvalues-of-AB-and-A-B"><a href="#Eigenvalues-of-AB-and-A-B" class="headerlink" title="Eigenvalues of $AB$ and $A+B$"></a>Eigenvalues of $AB$ and $A+B$</h2><p>特征值是否满足线性呢？不满足，笨方法也能看出来这根本不是一路的：<br>$$<br>ABx=A\beta x=\beta Ax=\beta \lambda x\\<br>$$<br>上面明显有问题，看出来问题在哪了么？<br>式子$Ax=\lambda x$当且仅当 x是A的特征向量的时候<br>所以 $A\beta x=\beta Ax=\beta \lambda x$ 这一步并不是对于所有矩阵都满足，只有A，B矩阵的特征向量相同的时候才能相等</p><blockquote><p>Commuting Matrix share eigenvectors ,假设，如果A和B能够被对角化，他们拥有完全相同的特征向量，当且仅当 $AB=BA$</p></blockquote><p>证明方法：<br>$$<br>ABx=A\beta x=\beta Ax=\beta \lambda x \\<br>\beta \lambda x=\lambda\beta x=\lambda Bx=B\lambda x=BAx\\<br>ABx=BAx\\<br>AB=BA<br>$$</p><p>对于A+B同理，只有当A和B的特征向量一致的时候，才能完成加法。<br>另外一个重要应用可以参考量子力学，Heisenberg’s uncertainty principle</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>本文主要讲解矩阵对角化，对角化的应用，对角化应该是在应用中最长用到的矩阵处理方法，所以用了一天的时间写了这一篇文章，希望能帮助理解。</p></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者： </strong>Tony</li><li class="post-copyright-link"><strong>本文链接：</strong> <a href="http://www.face2ai.com/Math-Linear-Algebra-Chapter-6-2/" title="【线性代数】6-2:对角化(Diagonalizing a Matrix)">http://www.face2ai.com/Math-Linear-Algebra-Chapter-6-2/</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/Math-Set-Theory-2-Operations-with-Sets/" rel="next" title="【集合论】2 Set Theory:Operation with sets"><i class="fa fa-chevron-left"></i> 【集合论】2 Set Theory:Operation with sets</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/Math-Linear-Algebra-Chapter-6-3/" rel="prev" title="【线性代数】6-3:微分方程的应用(Applications to Differential Equations)">【线性代数】6-3:微分方程的应用(Applications to Differential Equations) <i class="fa fa-chevron-right"></i></a></div></div></footer></div></article></div><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-1194454329688573" data-ad-slot="2491973880" data-ad-format="auto" data-full-width-responsive="true"></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-1194454329688573" data-ad-slot="2491973880" data-ad-format="auto" data-full-width-responsive="true"></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">站点概览</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/avatar.gif" alt="Tony"><p class="site-author-name" itemprop="name">Tony</p><p class="site-description motion-element" itemprop="description">关注机器学习，深度学习，机器视觉，模式识别</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives"><span class="site-state-item-count">257</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/index.html"><span class="site-state-item-count">17</span> <span class="site-state-item-name">分类</span></a></div></nav><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/tony-tan" target="_blank" title="GitHub" rel="external nofollow"><i class="fa fa-fw fa-github"></i>GitHub</a> </span><span class="links-of-author-item"><a href="mailto:tony.face2ai@gmail.com" target="_blank" title="E-Mail" rel="external nofollow"><i class="fa fa-fw fa-envelope"></i>E-Mail</a> </span><span class="links-of-author-item"><a href="https://twitter.com/Tony_Face2AI" target="_blank" title="Twitter" rel="external nofollow"><i class="fa fa-fw fa-twitter"></i>Twitter</a></span></div><div class="links-of-blogroll motion-element links-of-blogroll-inline"><div class="links-of-blogroll-title"><i class="fa fa-fw fa-link"></i> [object Object]</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"><a href="http://daily-ai.site/" title="更多人工智能博客请关注“每日AI”" target="_blank">更多人工智能博客请关注“每日AI”</a></li></ul></div><a href="http://www.daily-ai.site"><img border="0" src="/images/daily-ai.site.jpg"></a></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#开篇废话"><span class="nav-number">1.</span> <span class="nav-text">开篇废话</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Diagonalizing-a-Matrix"><span class="nav-number">2.</span> <span class="nav-text">Diagonalizing a Matrix</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Fibonacci-Numbers"><span class="nav-number">3.</span> <span class="nav-text">Fibonacci Numbers</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Matrix-Powers-A-k"><span class="nav-number">4.</span> <span class="nav-text">Matrix Powers $A^k$</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Nondiagonalizable-Matrices"><span class="nav-number">5.</span> <span class="nav-text">Nondiagonalizable Matrices</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Eigenvalues-of-AB-and-A-B"><span class="nav-number">6.</span> <span class="nav-text">Eigenvalues of $AB$ and $A+B$</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Conclusion"><span class="nav-number">7.</span> <span class="nav-text">Conclusion</span></a></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2018</span> <span class="with-love" id="animate"><i class="fa fa-"></i> </span><span class="author" itemprop="copyrightHolder">Tony</span></div><div class="busuanzi-count"><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="site-uv" title="总访客量"><i class="fa fa-user"></i> <span class="busuanzi-value" id="busuanzi_value_site_uv"></span> </span><span class="site-pv" title="总访问量"><i class="fa fa-eye"></i> <span class="busuanzi-value" id="busuanzi_value_site_pv"></span></span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span id="scrollpercent"><span>0</span>%</span></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/js/src/utils.js?v=6.4.0"></script><script type="text/javascript" src="/js/src/motion.js?v=6.4.0"></script><script type="text/javascript" src="/js/src/affix.js?v=6.4.0"></script><script type="text/javascript" src="/js/src/schemes/pisces.js?v=6.4.0"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=6.4.0"></script><script type="text/javascript" src="/js/src/post-details.js?v=6.4.0"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=6.4.0"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      TeX: {equationNumbers: { autoNumber: "AMS" }}
    });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });</script><script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>