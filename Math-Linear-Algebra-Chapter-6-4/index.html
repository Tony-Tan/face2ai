<!DOCTYPE html><html class="theme-next pisces use-motion" lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=6.4.0" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png?v=6.4.0"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32.png?v=6.4.0"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16.png?v=6.4.0"><link rel="mask-icon" href="/images/logo.png?v=6.4.0" color="#222"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Pisces",version:"6.4.0",sidebar:{position:"left",display:"post",offset:12,b2t:!1,scrollpercent:!0,onmobile:!1},fancybox:!1,fastclick:!1,lazyload:!1,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><meta name="description" content="Abstract: 本篇继续线性代数的高潮部分，对称矩阵，以及对称矩阵的一些性质Keywords: Eigenvalues，Eigenvectors，Symmetric Matrices，Projection Matrices，Spectral Theorem，Principal Axis Theorem"><meta name="keywords" content="Eigenvalues,Eigenvectors,Symmetric Matrices,Projection Matrices,Spectral Theorem,Principal Axis Theorem"><meta property="og:type" content="article"><meta property="og:title" content="【线性代数】6-4:对称矩阵(Symmetric Matrices)"><meta property="og:url" content="http://www.face2ai.com/Math-Linear-Algebra-Chapter-6-4/index.html"><meta property="og:site_name" content="谭升的博客"><meta property="og:description" content="Abstract: 本篇继续线性代数的高潮部分，对称矩阵，以及对称矩阵的一些性质Keywords: Eigenvalues，Eigenvectors，Symmetric Matrices，Projection Matrices，Spectral Theorem，Principal Axis Theorem"><meta property="og:locale" content="zh-CN"><meta property="og:updated_time" content="2018-07-12T15:45:31.654Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="【线性代数】6-4:对称矩阵(Symmetric Matrices)"><meta name="twitter:description" content="Abstract: 本篇继续线性代数的高潮部分，对称矩阵，以及对称矩阵的一些性质Keywords: Eigenvalues，Eigenvectors，Symmetric Matrices，Projection Matrices，Spectral Theorem，Principal Axis Theorem"><link rel="canonical" href="http://www.face2ai.com/Math-Linear-Algebra-Chapter-6-4/"><script type="text/javascript" id="page.configurations">CONFIG.page={sidebar:""}</script><title>【线性代数】6-4:对称矩阵(Symmetric Matrices) | 谭升的博客</title><script async src="https://www.googletagmanager.com/gtag/js?id=UA-105335860-3"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-105335860-3")</script><noscript><style type="text/css">.sidebar-inner,.use-motion .brand,.use-motion .collection-title,.use-motion .comments,.use-motion .menu-item,.use-motion .motion-element,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .logo,.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">谭升的博客</span> <span class="logo-line-after"><i></i></span></a></div><h1 class="site-subtitle" itemprop="description">Machine Learning & Computer Vision</h1></div><div class="site-nav-toggle"><button aria-label="切换导航栏"><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-首页"><a href="/" rel="section">首页</a></li><li class="menu-item menu-item-数学"><a href="/categories/Mathematic/" rel="section">数学</a></li><li class="menu-item menu-item-···-集合论"><a href="/categories/Mathematic/Set-Theory/" rel="section">··· 集合论</a></li><li class="menu-item menu-item-···-线性代数基础"><a href="/categories/Mathematic/Linear-Algebra/" rel="section">··· 线性代数基础</a></li><li class="menu-item menu-item-···-概率论基础"><a href="/categories/Mathematic/Probability/" rel="section">··· 概率论基础</a></li><li class="menu-item menu-item-···-数理统计学"><a href="/categories/Mathematic/Statistics/" rel="section">··· 数理统计学</a></li><li class="menu-item menu-item-···-数值分析"><a href="/categories/Mathematic/Numerical-Analysis/" rel="section">··· 数值分析</a></li><li class="menu-item menu-item-机器学习算法"><a href="/categories/Machine-Learning/" rel="section">机器学习算法</a></li><li class="menu-item menu-item-强化学习"><a href="/categories/Reinforcement-Learning/" rel="section">强化学习</a></li><li class="menu-item menu-item-深度学习算法"><a href="/categories/Deep-Learning/" rel="section">深度学习算法</a></li><li class="menu-item menu-item-数字图像处理"><a href="/categories/DIP/" rel="section">数字图像处理</a></li><li class="menu-item menu-item-30天自制操作系统"><a href="/categories/30天自制操作系统/" rel="section">30天自制操作系统</a></li><li class="menu-item menu-item-cuda"><a href="/categories/CUDA/" rel="section">CUDA</a></li><li class="menu-item menu-item-网络爬虫"><a href="/categories/Crawler/" rel="section">网络爬虫</a></li><li class="menu-item menu-item-乱七八糟"><a href="/categories/Other/" rel="section">乱七八糟</a></li></ul></nav><img src="https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/Wechat.jpeg" alt="wechat"> <a href="http://www.daily-ai.site"><img border="0" src="/images/daily-ai.site.jpg"></a></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-1194454329688573" data-ad-slot="9135658886" data-ad-format="auto" data-full-width-responsive="true"></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://www.face2ai.com/Math-Linear-Algebra-Chapter-6-4/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Tony"><meta itemprop="description" content="关注机器学习，深度学习，机器视觉，模式识别"><meta itemprop="image" content="/images/avatar.gif"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="谭升的博客"></span><header class="post-header"><h2 class="post-title" itemprop="name headline">【线性代数】6-4:对称矩阵(Symmetric Matrices)</h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2017-11-22 15:18:03" itemprop="dateCreated datePublished" datetime="2017-11-22T15:18:03+08:00">2017-11-22</time> </span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Mathematic/" itemprop="url" rel="index"><span itemprop="name">Mathematic</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Mathematic/Linear-Algebra/" itemprop="url" rel="index"><span itemprop="name">Linear Algebra</span></a></span> </span><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> 阅读次数： <span class="busuanzi-value" id="busuanzi_value_page_pv"></span></span></div></header><div class="post-body" itemprop="articleBody"><p><strong>Abstract:</strong> 本篇继续线性代数的高潮部分，对称矩阵，以及对称矩阵的一些性质<br><strong>Keywords:</strong> Eigenvalues，Eigenvectors，Symmetric Matrices，Projection Matrices，Spectral Theorem，Principal Axis Theorem</p><a id="more"></a><h2 id="开篇废话"><a href="#开篇废话" class="headerlink" title="开篇废话"></a>开篇废话</h2><p>这几篇在难度上确实要比前面的内容大很多，所以看书理解和总结都变得不那么流畅了，但是慢慢看下来收获还是有很大的，而且我发现不管学的多认真，还是会有遗漏，所以我觉得之前的想法就是一次性把什么什么学会是不可能的，只能学到自己觉得达到自己能发现的最大限度，等到应用之时还是要回来查阅，这样往往会有进一步的更大发现，</p><h2 id="Symmetric-Matrices"><a href="#Symmetric-Matrices" class="headerlink" title="Symmetric Matrices"></a>Symmetric Matrices</h2><p>对称矩阵我们在最早的知识里面就学过 $A^T=A$ 的矩阵叫做<a href="http://face2ai.com/Math-Linear-Algebra-Chapter-4-2/" target="_blank" rel="noopener">对称矩阵</a>，我们也学过<a href="http://face2ai.com/Math-Linear-Algebra-Chapter-4-2/" target="_blank" rel="noopener">投影矩阵</a>,但是当时我们并没有强调过一点就是投影矩阵都是对称的，这个性质今天在这里会有很大的用途。<br>我们继续说投影矩阵，所谓投影矩阵，就是在和向量 $\vec{c}$ 相乘的时候，投影到矩阵A的列空间内，那么其中，投影 $p$ 和 原向量 $\vec{c}$ 的差 $\vec{e} =\vec{c}-\vec{p}$ 与子空间正交。</p><p>举个例子，在三维空间内，A的列空间是一个二维平面那么，A对应的投影矩阵P能够把任何方向的向量投影到平面上，那么如果向量本身属于平面那么 $Px=x$ 显然是不用质疑的（我们之前在投影那篇文章中也讲过） 但是，同志们，看看这个有木有很面熟啊，这个明显就是投影矩阵 $P$ 的特征值和特征向量么？没错，$P$ 有一个平面的特征向量，可以随便选！能选多少个呢、当然是无数个，但是问题又来了，这无数多个并不是独立的，因为一共就二维，选出来三个线性独立的向量都是不可能的，所以这个平面能选出两个线性独立的特征向量，并且对应的特征值都是1，这里有人可能疑惑为啥要选两个，因为我们<a href="http://face2ai.com/Math-Linear-Algebra-Chapter-6-2/" target="_blank" rel="noopener">6-2</a>的时候说过只有特诊向量足够的情况下才能对角化，投影矩阵明显是个3x3的矩阵，那么特征向量也应该有三个呀！我们的子空间是二维的，所以理论上应该有两个特征向量在上面，剩下一维存在一个，那么这一个也能很好找，$\vec{e}$ 就是 也就是和子空间正交的向量都行 $Px=0x$ 表明 $\vec{x}$ 和子空间正交，那么这是个特征值为0的特征向量，这样我们又进一步规范一下，选择三个特征向量相互正交，这个也是可以做到的，也就是对于矩阵P我们找到了三个相互正交的特征向量，并且长度缩放到单位长度。</p><p>以上三维投影到二维平面可以通过几何来解释，但为了能让大家从线性空间来理解，就没用几何方法，大家可以自己脑补。</p><p>得出结论，对称矩阵 $P^T=P$ 的特征向量相互正交并且为单位向量。<br>对称矩阵”It is no exaggeration to say that these are the most important matrices the world will ever see – in the theory of linear algebra and alos in the applications” 翻译成中文：“对称矩阵是史上最牛B的矩阵，无论在理论还是应用”<br>这个我们目前还无法考证，还没做过应用呢？不是么，但是我知道PCA中确实用了对称矩阵，SVD等一些列相关技术。<br>一个矩阵能被如此称赞，不外乎几点原因，首先是其本身拥有较好的性质，其次这个矩阵在自然生活中经常出现，就像正态分布，那么难的公式，却能准确的描述自然届的现象。最后就是如果表现形式简单，那么这个就是非常有用的东西啦。</p><p>下面我们开始探索对称矩阵的性质。<br>如果一个对称矩阵满足：<br>$$<br>suppose:\\<br>A^T=A\\<br>A=S\Lambda S^{-1}\\<br>then:\\<br>A^T=(S^{-1})^T\Lambda^T S^T<br>$$<br>这种情况下就有下面这种<strong><em>可能</em></strong>了，也就是对应的 $S=(S^{-1})^T$ 注意我们这里说的是可能，并不排除不可能的情况，原文书上用的也是possibly，也就是说我们目前假设:<br>$$<br>S^T=S^{-1}\\<br>S^TS=I<br>$$<br>这里我们可以预报一下：</p><blockquote><ol><li>对称矩阵只有实数特征值</li><li>对称矩阵特征向量可以选择正交单位向量 <em>orthonormal</em></li></ol></blockquote><p>对于 $S^TS=I$ 面熟么？还有印象么？我们认识啊，<a href="http://face2ai.com/Math-Linear-Algebra-Chapter-4-4/" target="_blank" rel="noopener">正交矩阵</a><br>$Q^TQ=I$ 矩阵Q中每列之间相互正交，也就是我们对于对称矩阵可以写成：<br>$$<br>A=Q\Lambda Q^{-1}=Q\Lambda Q^T\\<br>with:\\<br>Q^{-1}=Q^T<br>$$</p><p>这个就是著名的普定理 “Spectral Theorem”:</p><blockquote><p>Every symmetric matrix has the factorization $A=Q\Lambda Q^T$ with real eigenvalues in $\Lambda$ and orthonormal eigenvectors in $S=Q$</p></blockquote><p>对于所有对称矩阵都能分解成 $A=Q\Lambda Q^T$ 的形式并且在 $\Lambda$ 中的所有特征值都是实数，其对应的特征向量是正交单位矩阵，即 $S=Q$</p><p>普定理在数学中很重要，对应的主轴定理 “Principle Axis Theorem”在集合物理中有重要地位。</p><p>这个定理对于从后到前 $A=Q\Lambda Q^T$ 很好证明，但是对于对称矩阵的特征值都是实数和特征向量相互正交比较难证明，也就是说，对角化时，特征向量矩阵是正交矩阵的时候很容易证明原始矩阵是对称的，但是对称矩阵不太容易证明，可以对角化，并且对角化的特征矩阵 $S$ 是正交矩阵 $Q$</p><p><strong><em>插播一句，orthonormal，翻译成中文是正交，而且normal，因为特征向量是可以随意缩放的，所以主要强调正交性</em></strong><br>我们接下来要做的伟大的证明，分为下面三步来证明：</p><ol><li>首先举个例子，来展示下特征值都是实数，特征向量orthonormal</li><li>当特征值不重复的时候的证明</li><li>当特征值重复的时候的证明</li></ol><hr><p>我们来先看个🌰：<br>$$<br>A=\begin{bmatrix}1&amp;2\\2&amp;4\end{bmatrix}\\<br>\begin{vmatrix}1-\lambda &amp;2\\2&amp;4-\lambda\end{vmatrix}=0\\<br>\lambda^2-5\lambda=0\\<br>\lambda_1=0\\<br>\lambda_2=5\\<br>x_1=\begin{bmatrix}2\newline -1\end{bmatrix}\\<br>x_2=\begin{bmatrix}1\newline 2\end{bmatrix}<br>$$<br>可以看出$x_1$ 属于矩阵的nullspace，而 $x_2$ 属于矩阵的column space，但是我们学四个子空间的时候有nullspace和rowspace是正交的，但是这里的 $x_2$ 属于矩阵的column space，为什么呢？因为矩阵实对称的，所以对称矩阵的rowspace和columnspace一致。<br>$$<br>Q^{-1}AQ=<br>\frac{1}{\sqrt{5}}<br>\begin{bmatrix}2&amp;1\newline -1&amp;2\end{bmatrix}<br>\begin{bmatrix}1&amp;2\\2&amp;4\end{bmatrix}<br>\frac{1}{\sqrt{5}}<br>\begin{bmatrix}2&amp;1\newline -1&amp;2\end{bmatrix}=<br>\begin{bmatrix}0&amp;0\newline 0&amp;5\end{bmatrix}=<br>\Lambda<br>$$<br>这就是个简单的例子，但是证明全体元素成立不能靠举一个例子来证明，但证明不成立可以靠举个反例来证明不成立。</p><hr><p>下面证明 <em>所有实数对称矩阵特征值都是实数</em></p><p>怎么证明一个数是实数呢，只能用点实数的性质，实数的性质不少但是能证明实数是实数的不多，可以想到一个就是实数的共轭是其本身，这也可以说是复数的性质，证明是实数也就是说证明不是复数。<br>复数的共轭<br>$$<br>\lambda=a+bi\\<br>\bar{\lambda}=a-bi<br>$$</p><p>根据复数的性质，以及向量的基本计算，对于实数矩阵A，满足：</p><p>$$<br>Ax=\lambda x\\<br>\bar{Ax}=\bar{\lambda x}\\<br>A\bar{x}=\bar{\lambda}\bar{x}\\<br>Transpose:\\<br>\bar{x}^TA=\bar{x}^T\bar{\lambda}\\<br>for:\\<br>\bar{x}^TAx=\bar{x}^T\lambda x\\<br>\bar{x}^TAx=\bar{x}^T\bar{\lambda}x\\<br>and:\\<br>\bar{x}^Tx=|x|^2&gt;0\\<br>so:\\<br>\bar{\lambda}=\lambda<br>$$</p><p>QED</p><p>整个过程证明了特征值的共轭等于原特征值，故特征值是实数被证明了。<br>证明思路就是通过构造出 $\bar{\lambda}=\lambda$ 的结构来证明，主要用到了A是实数矩阵的性质，通过转置和乘法等来完成这个过程。<br>$(A-\lambda I)x=0$ 可以得出既然特征值是实数，A也是实数，那么x肯定是实数（实数没办法仅通过乘法加法得出复数）</p><hr><p>下面证明 <em>所有实数对称矩阵特征向量相互正交,当特征值不相等的时候</em></p><p>$$<br>suppose:\\<br>Ax=\lambda_1x\\<br>Ay=\lambda_2y\\<br>A^T=A\\<br>(Ax)^T=\lambda_1x^T\\<br>(Ax)^Ty=x^TAy=\lambda_1x^Ty\\<br>x^TAy=x^T\lambda_2 y\\<br>then:\\<br>\lambda_1x^Ty=x^T\lambda_2 y\\<br>for:\\<br>\lambda_1\neq \lambda_2\\<br>so:\\<br>x^Ty=0<br>$$<br>QED</p><hr><p>插播一个小栗子：<br>$$<br>A=Q\Lambda Q^T=<br>\begin{bmatrix}x_1 &amp;x_2\end{bmatrix}<br>\begin{bmatrix}\lambda_1 &amp;\\&amp;\lambda_2\end{bmatrix}<br>\begin{bmatrix}x_1^T \\x_2^T\end{bmatrix}=\lambda_1x_1x_1^T+\lambda_2x_2x_2^T<br>$$<br>$A=\lambda_1x_1x_1^T+\lambda_2x_2x_2^T$ 把A写成了两个rank=1的矩阵的线性组合，并且这个rank=1的矩阵还是投影矩阵，当然也是对称矩阵，是不是很神奇，这个矩阵分解是比较有意思的，基是矩阵，而且基实对称的rank=1的投影矩阵，是不是很多头衔啊，头衔越多越厉害，不信你去看看大大有多少头衔。这个投影矩阵可以理解为投影到特征向量组成的空间(Eigenspace)</p><hr><p>其实写到这我有点疑惑了，本来写博客一个是自己总结，另一个是给大家一个参考，但是当我写了上面那个头衔了以后，我发现，如果没有基础或者不从头看起，直接看本文可能会感到疑惑，这也是知识的一个性质，就是连续性和扩展性，那么没有基础，基本都是空中楼阁（这种大牛太多了，基础没用，直接上算法的比比皆是，我以前也是，我现在改邪归正了）</p><h2 id="Complex-Eigenvalues-of-Real-Matrix"><a href="#Complex-Eigenvalues-of-Real-Matrix" class="headerlink" title="Complex Eigenvalues of Real Matrix"></a>Complex Eigenvalues of Real Matrix</h2><p>本文主要说对称矩阵的特征值，特征向量，对称矩阵的特征值和特征向量一定是实数，我们上面基本都为证明这个结论，那么什么样的矩阵会产生复数特征值特征向量呢？<br>如果一个矩阵包含一个复数特征值，那么一定是成对出现的，所谓成对就是如果 $\lambda=a+bi$ 是一个特征值，那么 $\bar{\lambda}=a-bi$ 也一定是A的一个特征值，证明：<br>$$<br>Ax=\lambda x\\<br>A\bar{x}=\bar{\lambda x}=\bar{\lambda} \bar{x}<br>$$<br>上面的取共轭操作hexo渲染有点问题，在两个字母中间的$\bar{\lambda x}$ 其实是个长的,他画的有点短<br>我们后面有一章专门介绍复数矩阵，所以这里有点迷糊的不要紧，放过自己，继续看下面。</p><h2 id="Eigenvalues-vs-Pivots"><a href="#Eigenvalues-vs-Pivots" class="headerlink" title="Eigenvalues vs Pivots"></a>Eigenvalues vs Pivots</h2><p>Pivots是我们这章之前主要研究的对象，因为我们主要研究的是矩阵用于方程组的求解，而本章开始Eigenvalues的研究，那么我们的惯性思维就是，既然是一个体系下的知识重点，那么他们有联系么？</p><blockquote><p>product of pivots = determinant = product of Eigenvalues</p></blockquote><p>这个是个特殊的性质，结合前面trace的性质我们知道了特征值和矩阵相关的两个算数性质，但是这个具体的证明我还没学会，包括下面的这个结论，证明我也没看懂。</p><blockquote><p>The number of positive eigenvalues of $A=A^T$ equals the number of positive Pivots</p></blockquote><p>这个结论书上给出了证明，但是说实话，我没明白，所以这个地方必须先留个空白，把书上的东西抄过来没意义</p><p><strong><em>上述两个结论不会证明，后续补上，此处留坑</em></strong></p><p>这就是pivots和eigenvalues可以通过上述两个结论产生联系，不过联系应该也就这么多了。</p><h2 id="All-Symmetric-Matrices-are-Diagonalizable"><a href="#All-Symmetric-Matrices-are-Diagonalizable" class="headerlink" title="All Symmetric Matrices are Diagonalizable"></a>All Symmetric Matrices are Diagonalizable</h2><p>接下来我们要证明最后一个结论，就是在有重复特征值的情况下，对称矩阵依然可以被对角化，其实本文第一个例子中就包含相同的特征值，两个 $\lambda=1$ 虽然如此我们还是在平面中找到了两个相互正交的特征向量，一个特殊的例子没办法证明全部情况，下面我们系统证明一下：<br>正式的证明之前有个小trick,就是给矩阵对角线上的每个元素加一个扰动 $nc$ 这样所有的特征值不同（<strong><em>具体为啥我也没想明白,有明白人请指教一下</em></strong>）所有有不同的特征向量，当 $c\to 0$ 得到原始特征值，和一组不同特征向量（这个思路是Prof. Strang写在书上的，他说这个有点不严谨，但是I am sure this is true）<br>接下来将正规的证明方法<br>首先用到一个理论：</p><blockquote><p><strong>Schur’s Theorem</strong>: Every square matrix factors into $A=QTQ^{-1}$ where T is upper trangular and $\bar{Q}^T=Q^{-1}$ If A has real eigenvalues the Q and T can be chosen real:$Q^TQ=I$</p></blockquote><p>这个定理给出了详细的证明，但比较复杂，我试着简单的证明了一下（如果有不严谨的地方，请各位指出）:<br>pf:<br>对于一个矩阵A，它代表的是A的列空间的基的矩阵，我们可以把它进行 $QR$ 分解，得到以Q为基（正交基）的同样的子空间，那么 $AQ$ 将还是这个子空间的基，所以我们可以得到：<br>$$<br>AQ=QT\\<br>so:<br>A=QTQ^{-1}<br>$$<br>上述过程中T是和R类似的上三角矩阵,对于所有方阵成立。<br>也就是说如果我们把Schur 定理稍微进行变形:<br>$$<br>A^T=Q^TT^TQ\\<br>when:\\<br>A^T=A\\<br>T^T=T<br>$$<br>因为T是三角矩阵，所以当它也是对称矩阵的时候，必然是对角的。QED<br>以上是我刚发明的简单的证明方法，对于Schur’s Theorem 我们需要找的就是是否存在T使得 $AQ=QT$ 成立，其中Q是正交矩阵（研究证明题必须先把目标搞明白，别看了一路都是对的，就是不知道要干嘛，我刚才就犯了这个毛病，看哪句都是真命题，然后迷迷糊糊不知道要证明啥），下面描述下书上的方法：<br>我们要寻找Q，满足$AQ=QT$<br>观察T是上三角矩阵，所以第一列只有一个元素$t_{11}$ 其中 $q_1$ 是Q的第一列,也就是说 $Aq_1=t_{11}q_1$ (这个要是不明白就自己拿笔画个矩阵比划一下子，就知道了)</p><p>$$<br>\bar{Q}^T_1AQ_1=<br>\begin{bmatrix}<br>\bar{q}^T_1\\<br>\vdots \\<br>\bar{q}_n^T<br>\end{bmatrix}<br>\begin{bmatrix}<br>&amp;&amp;\\<br>Aq_1&amp;\dots&amp;Aq_n\\<br>&amp;&amp;<br>\end{bmatrix}=<br>\begin{bmatrix}<br>t_{11}&amp;\dots&amp;\dots&amp;\dots \\<br>0&amp;&amp;&amp; \\<br>0&amp;&amp;A_2&amp;\\<br>0&amp;&amp;&amp;<br>\end{bmatrix}<br>$$<br>观察上面的整个过程$Aq_1=t_{11}q_1$ 其实是A的特征值和特征向量，那么$q_2\dots q_n$这些值怎么确定？答案是无所谓，只要找一组和$q_1$ 都正交的基就可以填充出其他部分（因为$t_{12}\dots t_{1n}$ $q_1\dots q_n$ 相乘得到A的其他部分，使得等式成立），我们的目的是为了找T，所以第一步我们算是迈出去了，下一步就是找$A_2$ 中的T了，根据假设，我们可以认为 $A_2Q_2=Q_2T_2$ 这里面的矩阵都比上一步少小一号(size=n-1)递归调用上面的证明方法，可以得出$Q_2$ , $t_{22}$ 以及$A_3$<br>如此递归下去可以找到所有A的第一个特征向量，然后组合成Q，T<br>$$<br>Q=Q_1\begin{bmatrix}1&amp;0\\0&amp;Q_2\end{bmatrix}\\<br>T=\begin{bmatrix}t_{11}&amp;0\\0&amp;T_2\end{bmatrix}\\<br>AQ=QT<br>$$<br>以上可证存在T使得 $AQ=QT$ 成立，那么如果A是symmetric的，那么:<br>$$<br>A=QTQ^T\\<br>A^T=Q^TT^TQ\\<br>A=A^T\\<br>so:\\<br>T=T^T<br>$$<br>T是上三角矩阵，得出结论，T是对角矩阵<br>QED<br>上面这一小段其实在证明Schur定理，因为Schur定理一旦得到证明，那么自然可以得到我们想要的结论，所有对称矩阵可以被对角化，Schur矩阵以复数形式给出，因为我们前面已经证明了对称矩阵的特征值都是实数，所以这里可以用实数表达，当然复数是对于非对称矩阵的，因为非对称实数矩阵可能得到复数特征值和特征向量。</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>这篇文章扎扎实实写了24小时，而且中间确实有不太清楚的地方，至今没动，所以都高亮标注了，提醒读者也提醒自己要来填坑，schur定理的证明方法还有别的，这个是Prof. Strang 书上的方法，后面如果有新发现继续补充。</p></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者： </strong>Tony</li><li class="post-copyright-link"><strong>本文链接：</strong> <a href="http://www.face2ai.com/Math-Linear-Algebra-Chapter-6-4/" title="【线性代数】6-4:对称矩阵(Symmetric Matrices)">http://www.face2ai.com/Math-Linear-Algebra-Chapter-6-4/</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/Math-Linear-Algebra-Chapter-6-3/" rel="next" title="【线性代数】6-3:微分方程的应用(Applications to Differential Equations)"><i class="fa fa-chevron-left"></i> 【线性代数】6-3:微分方程的应用(Applications to Differential Equations)</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/Math-Linear-Algebra-Chapter-6-5/" rel="prev" title="【线性代数】6-5:正定矩阵(Positive Definite Matrices)">【线性代数】6-5:正定矩阵(Positive Definite Matrices) <i class="fa fa-chevron-right"></i></a></div></div></footer></div></article></div><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-1194454329688573" data-ad-slot="2491973880" data-ad-format="auto" data-full-width-responsive="true"></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-1194454329688573" data-ad-slot="2491973880" data-ad-format="auto" data-full-width-responsive="true"></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">站点概览</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/avatar.gif" alt="Tony"><p class="site-author-name" itemprop="name">Tony</p><p class="site-description motion-element" itemprop="description">关注机器学习，深度学习，机器视觉，模式识别</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives"><span class="site-state-item-count">258</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/index.html"><span class="site-state-item-count">17</span> <span class="site-state-item-name">分类</span></a></div></nav><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/tony-tan" target="_blank" title="GitHub" rel="external nofollow"><i class="fa fa-fw fa-github"></i>GitHub</a> </span><span class="links-of-author-item"><a href="mailto:tony.face2ai@gmail.com" target="_blank" title="E-Mail" rel="external nofollow"><i class="fa fa-fw fa-envelope"></i>E-Mail</a> </span><span class="links-of-author-item"><a href="https://twitter.com/Tony_Face2AI" target="_blank" title="Twitter" rel="external nofollow"><i class="fa fa-fw fa-twitter"></i>Twitter</a></span></div><div class="links-of-blogroll motion-element links-of-blogroll-inline"><div class="links-of-blogroll-title"><i class="fa fa-fw fa-link"></i> [object Object]</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"><a href="http://daily-ai.site/" title="更多人工智能博客请关注“每日AI”" target="_blank">更多人工智能博客请关注“每日AI”</a></li></ul></div></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#开篇废话"><span class="nav-number">1.</span> <span class="nav-text">开篇废话</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Symmetric-Matrices"><span class="nav-number">2.</span> <span class="nav-text">Symmetric Matrices</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Complex-Eigenvalues-of-Real-Matrix"><span class="nav-number">3.</span> <span class="nav-text">Complex Eigenvalues of Real Matrix</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Eigenvalues-vs-Pivots"><span class="nav-number">4.</span> <span class="nav-text">Eigenvalues vs Pivots</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#All-Symmetric-Matrices-are-Diagonalizable"><span class="nav-number">5.</span> <span class="nav-text">All Symmetric Matrices are Diagonalizable</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Conclusion"><span class="nav-number">6.</span> <span class="nav-text">Conclusion</span></a></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2018</span> <span class="with-love" id="animate"><i class="fa fa-"></i> </span><span class="author" itemprop="copyrightHolder">Tony</span></div><div class="busuanzi-count"><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="site-uv" title="总访客量"><i class="fa fa-user"></i> <span class="busuanzi-value" id="busuanzi_value_site_uv"></span> </span><span class="site-pv" title="总访问量"><i class="fa fa-eye"></i> <span class="busuanzi-value" id="busuanzi_value_site_pv"></span></span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span id="scrollpercent"><span>0</span>%</span></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/js/src/utils.js?v=6.4.0"></script><script type="text/javascript" src="/js/src/motion.js?v=6.4.0"></script><script type="text/javascript" src="/js/src/affix.js?v=6.4.0"></script><script type="text/javascript" src="/js/src/schemes/pisces.js?v=6.4.0"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=6.4.0"></script><script type="text/javascript" src="/js/src/post-details.js?v=6.4.0"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=6.4.0"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      TeX: {equationNumbers: { autoNumber: "AMS" }}
    });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });</script><script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>