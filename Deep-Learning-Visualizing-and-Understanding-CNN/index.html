<!DOCTYPE html><html class="theme-next mist use-motion" lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=6.4.0" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png?v=6.4.0"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32.png?v=6.4.0"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16.png?v=6.4.0"><link rel="mask-icon" href="/images/logo.png?v=6.4.0" color="#222"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Mist",version:"6.4.0",sidebar:{position:"left",display:"remove",offset:12,b2t:!1,scrollpercent:!0,onmobile:!0},fancybox:!1,fastclick:!1,lazyload:!1,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><meta name="description" content="Abstract: 研究卷积神经网络，把阅读到的一些文献经典的部分翻译一下Keywords: CNN Visualizing"><meta name="keywords" content="CNN Visualizing,CNN"><meta property="og:type" content="article"><meta property="og:title" content="Visualizing and Understanding CNN"><meta property="og:url" content="http://www.face2ai.com/Deep-Learning-Visualizing-and-Understanding-CNN/index.html"><meta property="og:site_name" content="谭升的博客"><meta property="og:description" content="Abstract: 研究卷积神经网络，把阅读到的一些文献经典的部分翻译一下Keywords: CNN Visualizing"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/Deep-Learning-Visualizing-and-Understanding-CNN/20160913190123281.png"><meta property="og:image" content="https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/Deep-Learning-Visualizing-and-Understanding-CNN/20160913191649067.png"><meta property="og:image" content="https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/Deep-Learning-Visualizing-and-Understanding-CNN/20160913221329283.png"><meta property="og:image" content="https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/Deep-Learning-Visualizing-and-Understanding-CNN/20160913214421030.png"><meta property="og:image" content="https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/Deep-Learning-Visualizing-and-Understanding-CNN/20160913215022775.png"><meta property="og:image" content="https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/Deep-Learning-Visualizing-and-Understanding-CNN/20160913214923055.png"><meta property="og:image" content="https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/Deep-Learning-Visualizing-and-Understanding-CNN/20160913215441245.png"><meta property="og:image" content="https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/Deep-Learning-Visualizing-and-Understanding-CNN/20160913220428886.png"><meta property="og:image" content="https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/Deep-Learning-Visualizing-and-Understanding-CNN/20160913220143576.png"><meta property="og:image" content="https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/Deep-Learning-Visualizing-and-Understanding-CNN/20160913215615949.png"><meta property="og:image" content="https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/Deep-Learning-Visualizing-and-Understanding-CNN/20160913220319983.png"><meta property="og:updated_time" content="2018-09-24T06:58:20.006Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Visualizing and Understanding CNN"><meta name="twitter:description" content="Abstract: 研究卷积神经网络，把阅读到的一些文献经典的部分翻译一下Keywords: CNN Visualizing"><meta name="twitter:image" content="https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/Deep-Learning-Visualizing-and-Understanding-CNN/20160913190123281.png"><link rel="canonical" href="http://www.face2ai.com/Deep-Learning-Visualizing-and-Understanding-CNN/"><script type="text/javascript" id="page.configurations">CONFIG.page={sidebar:""}</script><title>Visualizing and Understanding CNN | 谭升的博客</title><script async src="https://www.googletagmanager.com/gtag/js?id=UA-105335860-3"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-105335860-3")</script><noscript><style type="text/css">.sidebar-inner,.use-motion .brand,.use-motion .collection-title,.use-motion .comments,.use-motion .menu-item,.use-motion .motion-element,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .logo,.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">谭升的博客</span> <span class="logo-line-after"><i></i></span></a></div><h1 class="site-subtitle" itemprop="description">人工智能基础</h1></div><div class="site-nav-toggle"><button aria-label="切换导航栏"><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-首页"><a href="/" rel="section">首页</a></li><li class="menu-item menu-item-集合论"><a href="/categories/Mathematic/Set-Theory/" rel="section">集合论</a></li><li class="menu-item menu-item-线性代数"><a href="/categories/Mathematic/Linear-Algebra/" rel="section">线性代数</a></li><li class="menu-item menu-item-概率论"><a href="/categories/Mathematic/Probability/" rel="section">概率论</a></li><li class="menu-item menu-item-数理统计学"><a href="/categories/Mathematic/Statistics/" rel="section">数理统计学</a></li><li class="menu-item menu-item-数值分析"><a href="/categories/Mathematic/Numerical-Analysis/" rel="section">数值分析</a></li><li class="menu-item menu-item-机器学习算法"><a href="/categories/Machine-Learning/" rel="section">机器学习算法</a></li><li class="menu-item menu-item-强化学习"><a href="/categories/Reinforcement-Learning/" rel="section">强化学习</a></li><li class="menu-item menu-item-深度学习算法"><a href="/categories/Deep-Learning/" rel="section">深度学习算法</a></li><li class="menu-item menu-item-数字图像处理"><a href="/categories/DIP/" rel="section">数字图像处理</a></li><li class="menu-item menu-item-30天自制操作系统"><a href="/categories/30天自制操作系统/" rel="section">30天自制操作系统</a></li><li class="menu-item menu-item-cuda"><a href="/categories/CUDA/" rel="section">CUDA</a></li><li class="menu-item menu-item-网络爬虫"><a href="/categories/Crawler/" rel="section">网络爬虫</a></li><li class="menu-item menu-item-乱七八糟"><a href="/categories/Other/" rel="section">乱七八糟</a></li></ul></nav><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-1194454329688573" data-ad-slot="2491973880" data-ad-format="auto" data-full-width-responsive="true"></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-1194454329688573" data-ad-slot="9135658886" data-ad-format="auto" data-full-width-responsive="true"></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://www.face2ai.com/Deep-Learning-Visualizing-and-Understanding-CNN/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="谭升"><meta itemprop="description" content="本站包括强化学习算法，机器学习算法，人工智能，CUDA编程，模式识别算法，线性代数，概率论，数理统计等人工智能原创博客"><meta itemprop="image" content="/images/avatar.gif"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="谭升的博客"></span><header class="post-header"><h2 class="post-title" itemprop="name headline">Visualizing and Understanding CNN</h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2017-09-13 16:46:09" itemprop="dateCreated datePublished" datetime="2017-09-13T16:46:09+08:00">2017-09-13</time> </span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a></span> </span><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> 阅读次数： <span class="busuanzi-value" id="busuanzi_value_page_pv"></span></span></div></header><div class="post-body" itemprop="articleBody"><p><strong>Abstract:</strong> 研究卷积神经网络，把阅读到的一些文献经典的部分翻译一下<br><strong>Keywords:</strong> CNN Visualizing<br><a id="more"></a></p><h1 id="卷积神经网络的可视化"><a href="#卷积神经网络的可视化" class="headerlink" title="卷积神经网络的可视化"></a>卷积神经网络的可视化</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>研究卷积神经网络，把阅读到的一些文献经典的部分翻译一下，写成博客，代码后续给出，不足之处还请大家指出。<br>大型卷积神经网络在图片分类上很成功，然而我们不知道他为什么能表现的如此不错，或者如何提高。</p><h2 id="Abstract："><a href="#Abstract：" class="headerlink" title="Abstract："></a>Abstract：</h2><blockquote><p>In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of inter-mediate feature layers and the operation of the classifier<br>我们研究一个优秀的可视化技术，能够给出函数内部特征层以及分类层的信息<br>Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al. on the ImageNet classification benchmark.<br>可视化使我们找到比Kri在ImageNet分类更好的网络架构。<br>We also perform an ablation study to discover the performance contribution from different model layers.<br>我们通过切块研究发现不同层对分类的作用。<br>We show our ImageNet model generalizes well to other datasets: when&gt;the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.<br>我们展示了我们的 ImageNet 模型在其他数据集上获得优秀的表现：当我们重新训练SoftMax分类器。其结果信服的打败了当前SOTA结果，在Caltech-101和Caltech-256数据集上</p></blockquote><p>评论：作者要解决的是可视化深度学习模型，来给出内部的结构，工作原理，以及内在结构的相关性等。并且在这个基础上反向选择优化不同的深度架构（模型）来得到更好的模型，并给出了监督学习的Pre-training方法，在不同测试数据集上表现不俗。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>卷积神经网络很牛，在各种分类比赛上获得state-of-the-art的结果。</p><blockquote><p>卷积神经网络在各大测试集上获得好结果的原因：<br>Several factors are responsible for this renewed interest in convnet<br>models:</p></blockquote><blockquote><p>(i) the availability of much larger training sets, with<br>大量的训练数据</p></blockquote><blockquote><p>(ii) powerful GPU implementations, making the training of very large models practical<br>GPU的高效计算</p></blockquote><blockquote><p>(iii) better model regularization strategies, such as Dropout (Hinton et al., 2012).<br>更优秀的网络结构（例如Dropout）</p></blockquote><blockquote><p>Without clear understanding of how and why they work, the development of better models is reduced to trial-and-error.<br>如果不知道内部原因，我们的新模型只能停留在实验，观察的基础上</p></blockquote><blockquote><p>In this paper we introduce a visualization technique that reveals the&gt;input stimuli that excite individual feature maps at any layer in the&gt;model. It also allows us to observe the evolution of features during training and to diagnose potential problems with the model.<br>本文提出了一种可视化技术，其能够揭示输入是如何激活那些独立的特征映射在模型中的任一层。这项技术也允许我们来观察特征在训练过程中的进化过程来判断模型潜在的问题。</p></blockquote><blockquote><p>The visualization technique we propose uses a multi-layered&gt;Deconvolutional Network (deconvnet), as proposed by (Zeiler et al., 2011), to project the feature activations back to the input pixel space.<br>我们提出了使用多层逆卷积网络，（Zeiler et al 2014年）提出的，将特征反向映射会到输入层观察结果</p></blockquote><blockquote><p>We also perform a sensitivity analysis of the classifier output by occluding portions of the input image, revealing which parts of the scene are important for classification<br>通过遮挡输入图片的部分，对分类器进行分析，来揭示哪些部分对分类结果产生相对重的影响</p></blockquote><blockquote><p>Using these tools, we start with the architecture of (Krizhevsky et al., 2012) and explore different architectures, discovering ones that&gt;outperform their results on ImageNet.<br>使用这些工具，我们开始使用此架构探索不同的架构，认识在ImageNet上表现出色的结构</p></blockquote><blockquote><p>We then explore the generalization ability of the model to other datasets, just retraining the softmax classifier on top.<br>我们随后探索架构对于其他数据集的范化能力，在只重新训练softmax分类器的基础上。</p></blockquote><blockquote><p>As such, this is a form of supervised pre-training, which contrasts with the unsupervised pre-training methods popularized by (Hinton et&gt;al., 2006) and others (Bengio et al., 2007; Vincent et al., 2008)<br>监督学习的Pre-training来对比无监督的Pre-training方法（Hinton et al., 2006 Bengio et al., 2007; Vincent et al., 2008）</p></blockquote><p>评论：主要就是说，以前都是不知道为啥深度学习会工作，不知道如何优化，只是考实验观察，现在我们能牛x的知道为啥能工作了，虽然没有数学证明，但我们知道怎么调了，知道工作原理，知道怎么Pre-training。。。</p><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><blockquote><p>Our approach, by contrast, provides a non-parametric view of invariance, showing which patterns from the training set activate the feature map.<br>我们的工作提出了一种无参数的不变性观点，来展示训练数据的哪些部分激活了特征映射</p></blockquote><p>评论：没有评论</p><h2 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h2><blockquote><p>We use standard fully supervised convnet models throughout the paper, as defined by (LeCun et al., 1989) and (Krizhevsky et al., 2012).<br>我们在整篇文章使用标准完全监督卷积网络模型，在 (LeCun et al., 1989) and (Krizhevsky et al., 2012)定义的。<br>(i) convolution of the previous layer output (or, in the case of the 1st layer, the input image) with a set of learned filters;</p></blockquote><blockquote><p>(ii) passing the responses through a rectified linear function (relu(x) = max(x, 0));</p></blockquote><blockquote><p>(iii) max pooling over local neighborhoodsand</p></blockquote><blockquote><p>(iv) a local contrast operation that normalizes the responses across feature maps.</p></blockquote><blockquote><p>The top few layers of the network are conventional fully-connected</p></blockquote><blockquote><p>networks and the final layer is a softmax classifie</p></blockquote><p><img src="https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/Deep-Learning-Visualizing-and-Understanding-CNN/20160913190123281.png" alt=""></p><h3 id="Visualization-with-a-Deconvnet"><a href="#Visualization-with-a-Deconvnet" class="headerlink" title="Visualization with a Deconvnet"></a>Visualization with a Deconvnet</h3><blockquote><p>We present a novel way to map these activities back to the input pixel space, showing what input pattern originally caused a given activation in the feature maps.<br>我们提出了一个高级的方法来映射激活反向到输入像素空间，来展示在特征空间哪一部分输入引起了这个给定的激活<br>In (Zeiler et al., 2011), deconvnets were proposed as a way of&gt;performing unsupervised learning. Here, they are not used in any learning capacity, just as a probe of an already trained convent.<br>在(Zeiler et al., 2011)，Deconvnets 被提出作为一种表现非监督学习的方法。这里他们不再用于任何其学习能力，只是用于研究已经训练好的Convnet<br>To examine a given convnet activation, we set all other activations in the layer to zero and pass the feature maps as input to the attached deconvnet layer.<br>为了测试一个给定的神经元激活，我们设置所有其他的同层神经元激活值为零，传导特征映射作为输入来激活deconvnet层<br>Then we successively (i) unpool, (ii) rectify and (iii) filter to reconstruct the activity in the layer beneath that gave rise to the chosen activation. This is then repeated until input pixel space is&gt;reached.<br>在激活选定特征的神经元后Unpool，rectify，filter来重建本层的激活。</p></blockquote><h4 id="Unpooling："><a href="#Unpooling：" class="headerlink" title="Unpooling："></a>Unpooling：</h4><blockquote><p>In the convnet, the max pooling operation is non-invertible, however we can obtain an approximate inverse by recording the locations of the maxima within each pooling region in a set of switch variables. In the&gt;deconvnet, the unpooling operation uses these switches to place the reconstructions from the layer above into appropriate locations, preserving the structure of the stimulus. See Fig. 1(bottom) for an illustration of the procedure<br>最大池化不可逆，我们通过记录位置来进行近似，记录被称为一组switch值，在deconvnet中，逆池化使用这些switch值来定位重建上一层，保留激活分布。在Fig1中说明</p></blockquote><p><img src="https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/Deep-Learning-Visualizing-and-Understanding-CNN/20160913191649067.png" alt=""></p><h4 id="Rectification："><a href="#Rectification：" class="headerlink" title="Rectification："></a>Rectification：</h4><blockquote><p>The convnet uses relu non-linearities, which rectify the feature maps<br>thus ensuring the feature maps are always positive. To obtain valid feature reconstructions at each layer (which also should be positive),we pass the reconstructed signal through a relu non-linearity.<br>Convnet使用ReLu非线性函数，保证激活值非负；为保证每层特征可重建，我们让所有重建信号经过ReLu层。</p></blockquote><h4 id="Filtering："><a href="#Filtering：" class="headerlink" title="Filtering："></a>Filtering：</h4><blockquote><p>The convnet uses learned filters to convolve the feature maps from the previous layer. To invert this, the deconvnet uses transposed versions of the same filters, but applied to the rectified maps, not&gt;the output of the layer beneath.<br>Convnet使用学习到的Filters从前一层来获取特征映射。相反，deconvnet使用同一filter的转置，但是操作的对象是整流结果(Rectification)，而不是之前的层。</p></blockquote><h4 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h4><blockquote><p>Since the model is trained discriminatively, they implicitly show which parts of the input image are discriminative<br>由于模型是训练成有区别的，因此他们理所应当的展示输入图片的那些部分是有区别的。<br>Note that these projections are not samples from the model, since there is no generative process involved<br>注意这些映射不是从模型中采样，因为没有范化处理涉及。</p></blockquote><p>评论：此段描述了具体如何反向将特征映射到像素空间。</p><h2 id="Training-Details"><a href="#Training-Details" class="headerlink" title="Training Details"></a>Training Details</h2><blockquote><p>The architecture, shown in Fig. 3, is similar to that used by (Krizhevsky et al., 2012) for ImageNet classification<br>结构在Fig 3中（本文第一张图）。。。</p></blockquote><p>训练方法：</p><blockquote><p>The model was trained on the ImageNet 2012 training set (1.3 million images, spread over 1000 different classes). Each RGB image was preprocessed by resizing the smallest dimension to 256, cropping the center 256x256 region, subtracting the per-pixel mean (across all images) and then using 10 different sub-crops of size 224x224 (corners+ center with(out) horizontal flips). Stochastic gradient descent with a mini-batch size of 128 was used to update the parameters, starting<br>with a learning rate of 10−2, in conjunction with a momentum term of 0.9. We anneal the learning rate throughout training manually when the validation error plateaus. Dropout (Hinton et al., 2012) is used in the fully connected layers (6 and 7) with a rate of 0.5.<br>此处翻译略过，描述了卷积神经网络的训练方法。<br>Visualization of the first layer filters during training reveals that a few of them dominate, as shown in Fig. 6(a). To combat this, we&gt;renormalize each filter in the convolutional layers whose RMS value exceeds a fixed radius of 10−1 to this fixed radius<br>第一层 Filter的可视化在训练过程揭示，其中一部分起支配作用，如Fig 6 a 所示，为了对抗这种情况，我们重新归一化RMS值超过fixed-radius的0.1倍的每一个在卷基层的Filter</p></blockquote><p><img src="https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/Deep-Learning-Visualizing-and-Understanding-CNN/20160913221329283.png" alt=""><br>评论：详细的训练过程</p><h2 id="Convnet-Visualization"><a href="#Convnet-Visualization" class="headerlink" title="Convnet Visualization"></a>Convnet Visualization</h2><p>关于特征：</p><blockquote><p>Feature Visualization: Fig. 2 shows feature visualizations from our model once training is complete. However, instead of showing the single strongest activation for a given feature map, we show the top 9 activations.<br>特征可视化：图2显示的特征可视化是当模型训练完成时就确定的，然而不显示对于给定特征映射的单一强刺激而显示top9</p></blockquote><p><img src="https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/Deep-Learning-Visualizing-and-Understanding-CNN/20160913214421030.png" alt=""></p><blockquote><p>Alongside these visualizations we show the corresponding image patches. These have greater variation than visualizations as the latter solely focus on the discriminant structure within each patch.<br>沿着这个可视化我们可以观察到相当的当前图像区域。这有相当大的可视化成都相对于单独把注意力放到每一个path。</p></blockquote><blockquote><p>The projections from each layer show the hierarchical nature of the features in the network.<br>不同层的映射表现出网络中不同自然层级的特征</p></blockquote><blockquote><p>Feature Evolution during Training: Fig. 4 visualizes the progression during training of the strongest activation (across all training examples) within a given feature map projected back to pixel space.<br>特征在训练过程中的进化：图4，训练较强反应的神经元（在所有训练样本中）在给定特征映射逆向投影到像素空间的过程中的可视化。</p></blockquote><p><img src="https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/Deep-Learning-Visualizing-and-Understanding-CNN/20160913215022775.png" alt=""></p><blockquote><p>Sudden jumps in appearance result from a change in the image from which the strongest activation originates.<br>表面上突然的跳跃来自图像最强的激活区域（此区域能够激发网络中的部分神经元产生大的特征变化）<br>The lower layers of the model can be seen to converge within a few&gt;epochs. However, the upper layers only develop after a considerable number of epochs (40-50), demonstrating the need to let the models train until fully converged.<br>模型的较低层可以在一定周期内观察到。然而，高层的网络只在相当大的周期后才能被建立起来，表明模型需要继续训练到完全收敛<br>Feature Invariance: Fig. 5 shows 5 sample images being translated,rotated and scaled by varying degrees while looking at the changes in the feature vectors from the top and bottom layers of the model,relative to the untransformed feature.<br>特征独立性：图5，显示五个样本图经过变换，旋转，缩放多种随机模型，然后从底层到高层观察特征向量与未变换的特征向量进行对比</p></blockquote><p><img src="https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/Deep-Learning-Visualizing-and-Understanding-CNN/20160913214923055.png" alt=""><br>小的变换对于模型的第一层有显著影响，但是对于顶层特征影响不大，对于变换和缩放大致呈线性</p><blockquote><p>The network output is stable to translations and scalings.<br>网络输出对于变换和尺度缩放稳定。<br>In general, the output is not invariant to rotation, except for&gt;object with rotational symmetry (e.g. entertainment center).<br>然而，输出对于旋转变换不稳定，除非是对齐式的旋转。</p></blockquote><p>评论：训练过程中的特征是怎么来的。</p><h3 id="Architecture-Selection"><a href="#Architecture-Selection" class="headerlink" title="Architecture Selection"></a>Architecture Selection</h3><blockquote><p>While visualization of a trained model gives insight into its operation, it can also assist with selecting good architectures in the first place.<br>当可视化一个训练好的模型给出了其内部的操作，也能帮助我们选取更好的架构。<br>The first layer filters are a mix of extremely high and low frequency information, with little coverage of the mid frequencies.<br>第一层filter是混合了极其高频和极其低频的信息，只有少量的中频信息。<br>Additionally, the 2nd layer visualization shows aliasing artifacts caused by the large stride 4 used in the 1st layer convolutions.<br>第二层可视化展示了混淆的手工结果由在第一层中较大的步长（4）引起的<br>To remedy these problems, we<br>解决办法：<br>(i) reduced the 1st layer filter size from 11x11 to 7x7<br>(ii) made the stride of the convolution 2, rather than 4.</p></blockquote><blockquote><p>This new architecture retains much more information in&gt;the 1st and 2nd layer fea- tures, as shown in Fig. 6(c) &amp; (e). More importantly, it also improves the classification performance as shown in Section 5.1.</p></blockquote><p>评论：本段讲如何选取架构，说明步长在其中的影响</p><h3 id="Occlusion-Sensitivity"><a href="#Occlusion-Sensitivity" class="headerlink" title="Occlusion Sensitivity"></a>Occlusion Sensitivity</h3><blockquote><p>With image classification approaches, a natural question is if themodel is truly identifying the location of the object in the image, or just using the surrounding context.<br>对于图像分类的应用，一个自然的问题是模型是否只利用图片中的物体，还是使用周围的上下文信息<br>Fig. 7 attempts to answer this question by systematically occluding different portions of the input image with a grey square, and monitoring the output of the classifier.<br>图7试图回答这个问题，通过系统的遮挡输入图片不同的位置，使用一个灰色方框，然后监视分类器的输出</p></blockquote><p><img src="https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/Deep-Learning-Visualizing-and-Understanding-CNN/20160913215441245.png" alt=""></p><blockquote><p>When the occluder covers the image region that appears in the visualization, we see a strong drop in activity in the feature map.<br>当遮挡覆盖到可视化中出现的区域，我们发现在特征映射层有一个强烈的drop<br>Fig. 4 and Fig. 2. 图4和图2</p></blockquote><p>评论：遮挡不同区域的影响，不同区域敏感度不同。</p><h3 id="Correspondence-Analysis"><a href="#Correspondence-Analysis" class="headerlink" title="Correspondence Analysis"></a>Correspondence Analysis</h3><p>一致性分析</p><blockquote><p>Deep models differ from many existing recognition approaches in that there is no explicit mechanism for establishing correspondence between&gt;specific object parts in different images (e.g. faces have a particular spatial configuration of the eyes and nose)<br>深度学习模型与现存其他识别机制不同在于：其不存在对于在不同图片之间某些物体的特殊部分之间的准确的区别关系（例如：脸部存在一个鼻子和脸的特别空间关系）<br>不同的特征向量计算公式：</p></blockquote><p><img src="https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/Deep-Learning-Visualizing-and-Understanding-CNN/20160913220428886.png" alt=""></p><blockquote><p>We then measure the consistency of this difference vector delta between all related image pairs (i, j):</p></blockquote><p><img src="https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/Deep-Learning-Visualizing-and-Understanding-CNN/20160913220143576.png" alt=""></p><blockquote><p>我们然后计算所有图片对之间的不同。<br>where H is Hamming distance.<br>H为汉明距离<br>A lower value indicates greater consistency in the change resulting<br>from the masking operation, hence tighter correspondence between the<br>same object parts in different images (i.e. blocking the left eye)<br>在遮挡操作的变换结果中，一个较低的值表示部件之间较大的相关性fig8：</p></blockquote><p><img src="https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/Deep-Learning-Visualizing-and-Understanding-CNN/20160913215615949.png" alt=""><br>Table 1<br><img src="https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/Deep-Learning-Visualizing-and-Understanding-CNN/20160913220319983.png" alt=""><br>评论：不同特征的独立性验证，如果你有鼻子眼睛嘴的脸部特征，遮住鼻子对最后的特征向量影响不大，说明他们之间的相关性比较强，类似于一张图如果有鼻子，基本也有眼睛，所以你遮住眼睛也会得到差不多的特征向量。<br>总结：简单的学习了一下这篇文章，后面第五部分讲的是经验，关于如何训练高质量的网络，会在下一篇推出，欢迎收看。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>又是一片翻译文，哈哈哈哈，有需要的可以读读，营养还是有的，只是我现在不太喜欢这种文章</p><p>原文地址1：<a href="https://www.face2ai.com/Deep-Learning-Visualizing-and-Understanding-CNN">https://www.face2ai.com/Deep-Learning-Visualizing-and-Understanding-CNN</a>转载请标明出处</p></div><div><div style="padding:10px 0;margin:20px auto;width:90%;text-align:center"><div>关注公众号或添加博主微信，反馈问题，获取资讯（暗号:face2ai）</div><button id="rewardButton" disable="enable" onclick='var qr=document.getElementById("QR");"none"===qr.style.display?qr.style.display="block":qr.style.display="none"'><span>联系博主</span></button><div id="QR" style="display:none"><div id="wechat" style="display:inline-block"><img id="wechat_qr" src="/images/weixin.png" alt="谭升 博主微信"><p>博主微信</p></div><div id="alipay" style="display:inline-block"><img id="alipay_qr" src="/images/alipay.jpg" alt="谭升 博客公众号"><p>博客公众号</p></div></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者： </strong>谭升</li><li class="post-copyright-link"><strong>本文链接：</strong> <a href="http://www.face2ai.com/Deep-Learning-Visualizing-and-Understanding-CNN/" title="Visualizing and Understanding CNN">http://www.face2ai.com/Deep-Learning-Visualizing-and-Understanding-CNN/</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/Deep-Learning-LeNet/" rel="next" title="LeNet论文解读"><i class="fa fa-chevron-left"></i> LeNet论文解读</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/Math-Linear-Algebra-Chapter-3-1/" rel="prev" title="\[线性代数\]3-1:向量空间(Space of Vectors)">\[线性代数\]3-1:向量空间(Space of Vectors) <i class="fa fa-chevron-right"></i></a></div></div></footer></div></article></div><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-1194454329688573" data-ad-slot="2491973880" data-ad-format="auto" data-full-width-responsive="true"></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-1194454329688573" data-ad-slot="2491973880" data-ad-format="auto" data-full-width-responsive="true"></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div></div></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2018</span> <span class="with-love" id="animate"><i class="fa fa-"></i> </span><span class="author" itemprop="copyrightHolder">谭升</span></div><div class="busuanzi-count"><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="site-uv" title="总访客量"><i class="fa fa-user"></i> <span class="busuanzi-value" id="busuanzi_value_site_uv"></span> </span><span class="site-pv" title="总访问量"><i class="fa fa-eye"></i> <span class="busuanzi-value" id="busuanzi_value_site_pv"></span></span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span id="scrollpercent"><span>0</span>%</span></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/js/src/utils.js?v=6.4.0"></script><script type="text/javascript" src="/js/src/motion.js?v=6.4.0"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=6.4.0"></script><script type="text/javascript" src="/js/src/post-details.js?v=6.4.0"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=6.4.0"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      TeX: {equationNumbers: { autoNumber: "AMS" }}
    });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });</script><script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>