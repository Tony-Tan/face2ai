<!DOCTYPE html><html class="theme-next pisces use-motion" lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=6.4.0" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png?v=6.4.0"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32.png?v=6.4.0"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16.png?v=6.4.0"><link rel="mask-icon" href="/images/logo.png?v=6.4.0" color="#222"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Pisces",version:"6.4.0",sidebar:{position:"left",display:"post",offset:12,b2t:!1,scrollpercent:!0,onmobile:!0},fancybox:!1,fastclick:!1,lazyload:!1,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><meta name="description" content="Abstract: 本文主要介绍超几何分布Keywords: Hypergeomtirc Distribution,Finite Population Correction"><meta name="keywords" content="Hypergeomtirc Distribution,Finite Population Correction"><meta property="og:type" content="article"><meta property="og:title" content="\[概率论\]5-3:超几何分布(The Hypergeomtric Distribution)"><meta property="og:url" content="http://www.face2ai.com/Math-Probability-5-3-The-Hypergeomtirc-Distribution/index.html"><meta property="og:site_name" content="强化学习，机器学习，人工智能"><meta property="og:description" content="Abstract: 本文主要介绍超几何分布Keywords: Hypergeomtirc Distribution,Finite Population Correction"><meta property="og:locale" content="zh-CN"><meta property="og:updated_time" content="2018-09-24T04:56:06.426Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="\[概率论\]5-3:超几何分布(The Hypergeomtric Distribution)"><meta name="twitter:description" content="Abstract: 本文主要介绍超几何分布Keywords: Hypergeomtirc Distribution,Finite Population Correction"><link rel="canonical" href="http://www.face2ai.com/Math-Probability-5-3-The-Hypergeomtirc-Distribution/"><script type="text/javascript" id="page.configurations">CONFIG.page={sidebar:""}</script><title>\[概率论\]5-3:超几何分布(The Hypergeomtric Distribution) | 强化学习，机器学习，人工智能</title><script async src="https://www.googletagmanager.com/gtag/js?id=UA-105335860-3"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-105335860-3")</script><noscript><style type="text/css">.sidebar-inner,.use-motion .brand,.use-motion .collection-title,.use-motion .comments,.use-motion .menu-item,.use-motion .motion-element,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .logo,.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">强化学习，机器学习，人工智能</span> <span class="logo-line-after"><i></i></span></a></div><h1 class="site-subtitle" itemprop="description">谭升的博客</h1></div><div class="site-nav-toggle"><button aria-label="切换导航栏"><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-首页"><a href="/" rel="section">首页</a></li><li class="menu-item menu-item-数学"><a href="/categories/Mathematic/" rel="section">数学</a></li><li class="menu-item menu-item-···-集合论"><a href="/categories/Mathematic/Set-Theory/" rel="section">··· 集合论</a></li><li class="menu-item menu-item-···-线性代数"><a href="/categories/Mathematic/Linear-Algebra/" rel="section">··· 线性代数</a></li><li class="menu-item menu-item-···-概率论"><a href="/categories/Mathematic/Probability/" rel="section">··· 概率论</a></li><li class="menu-item menu-item-···-数理统计学"><a href="/categories/Mathematic/Statistics/" rel="section">··· 数理统计学</a></li><li class="menu-item menu-item-···-数值分析"><a href="/categories/Mathematic/Numerical-Analysis/" rel="section">··· 数值分析</a></li><li class="menu-item menu-item-机器学习算法"><a href="/categories/Machine-Learning/" rel="section">机器学习算法</a></li><li class="menu-item menu-item-强化学习"><a href="/categories/Reinforcement-Learning/" rel="section">强化学习</a></li><li class="menu-item menu-item-深度学习算法"><a href="/categories/Deep-Learning/" rel="section">深度学习算法</a></li><li class="menu-item menu-item-数字图像处理"><a href="/categories/DIP/" rel="section">数字图像处理</a></li><li class="menu-item menu-item-30天自制操作系统"><a href="/categories/30天自制操作系统/" rel="section">30天自制操作系统</a></li><li class="menu-item menu-item-cuda"><a href="/categories/CUDA/" rel="section">CUDA</a></li><li class="menu-item menu-item-网络爬虫"><a href="/categories/Crawler/" rel="section">网络爬虫</a></li><li class="menu-item menu-item-乱七八糟"><a href="/categories/Other/" rel="section">乱七八糟</a></li></ul></nav><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-1194454329688573" data-ad-slot="2491973880" data-ad-format="auto" data-full-width-responsive="true"></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-1194454329688573" data-ad-slot="9135658886" data-ad-format="auto" data-full-width-responsive="true"></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://www.face2ai.com/Math-Probability-5-3-The-Hypergeomtirc-Distribution/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="谭升"><meta itemprop="description" content="本站包括强化学习算法，机器学习算法，人工智能，CUDA编程，模式识别算法，线性代数，概率论，数理统计等人工智能原创博客"><meta itemprop="image" content="/images/avatar.gif"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="强化学习，机器学习，人工智能"></span><header class="post-header"><h2 class="post-title" itemprop="name headline">\[概率论\]5-3:超几何分布(The Hypergeomtric Distribution)</h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2018-03-28 09:27:39" itemprop="dateCreated datePublished" datetime="2018-03-28T09:27:39+08:00">2018-03-28</time> </span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Mathematic/" itemprop="url" rel="index"><span itemprop="name">Mathematic</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Mathematic/Probability/" itemprop="url" rel="index"><span itemprop="name">Probability</span></a></span> </span><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> 阅读次数： <span class="busuanzi-value" id="busuanzi_value_page_pv"></span></span></div></header><div class="post-body" itemprop="articleBody"><p><strong>Abstract:</strong> 本文主要介绍超几何分布<br><strong>Keywords:</strong> Hypergeomtirc Distribution,Finite Population Correction</p><a id="more"></a><h2 id="开篇废话"><a href="#开篇废话" class="headerlink" title="开篇废话"></a>开篇废话</h2><p>实力这个东西是不能被完全表现出来的，中华民族传统文化告诉我们，有十分的能力，只显示一分，但是我们现在是有一分能力要显示出十分，这叫推销自己，而且我们自己根本不知道自己只有一分能力，人心浮躁，我们还是憋着看书学习吧，外面的是是非非自然有人去解决，我们要做的是推动人类文明的发展😆<br>上文书我们说到（改成单口相声了）二项分布就是若干个独立同分布的伯努利分布的随机变量的和的结果，而伯努利分布如果对应最原始的抽样的话应该是这样的场景：<br>如果我们有一个不透明的箱子，里面有 $A$ 个红球， $B$ 个蓝球，其被拿出来的可能性相等，在我们拿出之前我们不知道我们会拿到什么（也就是保证随机性）那么我们拿出一个球是红球(称为事件R)的概率是 $Pr(R)=\frac{A}{A+B}$ ，如果我们连续进行本实验，那么就有两种取样方式，而这就导致了从伯努利到二项分布，和从伯努利到超几何分布的变化</p><h2 id="超几何分布定义和例子-Definition-and-Examples"><a href="#超几何分布定义和例子-Definition-and-Examples" class="headerlink" title="超几何分布定义和例子 Definition and Examples"></a>超几何分布定义和例子 Definition and Examples</h2><p>首先我们用一个具体的例子来看。</p><hr><p>继续上面说的拿球的例子，假设我们要连续拿出n个球，$n\geq 0$ （这里我们只考虑 $n\geq 2$ 的情况，$n=0$ 的时候说明试验不进行， $n=1$ 的时候是伯努利分布，上一课学习的东西，我们这里也不再说了） 我们假设每次取出时，拿到红球的随机变量为 $X_i=1$ 拿到蓝球的随机变量是 $X_i=0$ 并且每次试验是独立的，如果我们采用不放回的抽取方式，那么我们可以得出结论 $Pr(X_2=1|X_1=0)\neq Pr(X_2=1|X_1=1)$ ，因为我们第一次拿球，里面一共有 $A+B$ 个球包含 $A$ 个红球，如果第一次取出了红球，那么第二次我们再取相当于从 $A+B-1$ 个球包含 $A-1$ 红球中取，或者如果第一次取到的是蓝球，那么第二次相当于从 $A+B-1$ 个球中含 $A$ 个红球中取。<br>$$<br>Pr(X_2=1|X_1=0)=\frac{A}{A+B-1}&gt;Pr(X_2=1|X_1=1)=\frac{A-1}{A+B-1}<br>$$</p><hr><p>这就是简单的不放回抽样，前面我们研究过，如果从抽取范围非常大的样本中抽取少量的时候，可以不考虑其概率变化，但是如果样本集本来就不是很大，那么就要考虑这个概率变化了。<br>从有限的样本集中不放回的抽样，这就是我们今天研究的对象超几何分布的背景。<br>如果考虑从包含两种情况的样本集合中抽取n个样本，其中是某一情况的样本个数X，其就是一个超几何分布。</p><blockquote><p>Theorem Probability Function .The distribuiton of $X$ is Example has the p.f.<br>$$<br>f(x|n,A,B)=<br>\begin{cases}<br>\frac{<br>\begin{pmatrix}A\\x\end{pmatrix}<br>\begin{pmatrix}B\\n-x\end{pmatrix}<br>}{<br>\begin{pmatrix}A+B\\n\end{pmatrix}<br>}&amp;\text{for }max(0,n-B)\leq x\leq min(n,A)\\<br>0&amp;\text{otherwise}<br>\end{cases}<br>$$</p></blockquote><p>这个定理的证明只是用到了计数法则中的乘法法则，简化成分阶段的试验，但是 $x$ 范围要要注意一下，就是确保从A中来的和从B中来的结果总数是在范围内的，或者说，以取出红球的数量为x看来说，$x$ 不能超过 $A$，$n-x$ 是蓝球的数量不能超过 $B$ 写成数学形式就是 $max(0,n-B)\leq x\leq min(n,A)$</p><blockquote><p>Definition Hypergeomtric Distribution.Let $A,B$ and $n$ be nonnegative integers with $n\leq A+B$ .If a random variable $X$ has a discrete distribution with p.f. as in upside,then it is said that $X$ has the hypergeometric distribution with parameters $A,B$ and $n$</p></blockquote><p>这个定义依旧简单明了，和前面二项分布的定义基本一个套路，给一个p.f.然后告诉你这个就是xx分布。</p><p>我们分析下超几何分布有什么特点，与二项分布有什么不同，首先他们的基础实验都是伯努利分布，但是不同的在于二项分布每一步的试验都是恒定分布的伯努利试验，但是超几何分布不是，其每一步的伯努利分布都有不同的分布，且当前分布和前面试验结果有关，如果按照这种思路，其实超几何分布的每一次取出都是一次和前面所有取出相关的伯努利分布（而二项分布是相互独立的）。</p><p>为什么叫超几何分布？<br>超几何分布的名字和二项分布的名字来源相似都是和某个展开式的系数有关系，超几何函数的级数展开系数和我们今天的超几何分布有关系，所以就命名为超几何分布。</p><p>几何分布和超几何分布的关系？<br>几何分布是二项分布中的一种极端情况，就是前 $k-1$ 次必须失败，第 $k$ 次成功，然后试验结束，换句话说就是在伯努利过程中当结果是 $X_k=1$ 时完成整个过程，这时的概率是 $p(1-p)^{k-1}$ 。那么如果 $X$ 是几何分布，那么其p.f.就是 $f(x|1,p)=p(1-p)^x$ for $x=0,1,2,\dots$</p><h2 id="超几何分布的均值和方差-The-Mean-and-Variance-for-a-Hypergeomtirc-Distribution"><a href="#超几何分布的均值和方差-The-Mean-and-Variance-for-a-Hypergeomtirc-Distribution" class="headerlink" title="超几何分布的均值和方差 The Mean and Variance for a Hypergeomtirc Distribution"></a>超几何分布的均值和方差 The Mean and Variance for a Hypergeomtirc Distribution</h2><p>基础了解的差不多了我们就开始研究下数字特征了。</p><blockquote><p>Theorem Mean and Variance.Let X have a hypergeometric distribution with strictly positive parameters A,B and n.Then:<br>$$<br>E(X)=\frac{nA}{A+B}\\<br>Var(X)=\frac{nAB}{(A+B)^2}\times \frac{A+B-n}{A+B-1}<br>$$</p></blockquote><p>证明有点复杂了，要用到我们前面用到的一种思想，因为是有限的离散分布，我们总能把它转换成穷举的模式<br>首先证明期望：<br>思路，以拿球为例子说明，$A$ 个红球， $B$ 个蓝球 如果第i次试验拿到了红球 随机变量 $X_i=1$ 一共抽取 $n$ 个球。<br>我们假设所有球都不同，把他们所有可能的排列都列出来，然后把我们抽取过程等效成从所有可能的排列中选一个，观察前n个球中红球的数量。那么我们会有 $(A+B)!$ 行，每一个行对应一个随机结果，且等可能性，那么我们关心的是前 $n$ 个球是怎么排布样的（因为我们就抽取 $n$ 个球），这时候我们看列，第一列到第 $n$ 列，其中红球的平均数就是超几何分布的期望，每一列红球出现的比例都是 $\frac{A}{A+B}$ 原因是 红球和蓝球等可能出现，<br>有点复杂，我们来举个例子，三个球，一个红球，两个蓝球，我们按照上面的思路把所有可能的出场情况都写出来，共 $3!=6$ 种，我们只观察前两个球（假设从三个球中拿两个）的分布<br>$$<br>R_1,B_1,B_2\\<br>R_1,B_2,B_1\\<br>B_1,R_1,B_2\\<br>B_1,B_2,R_1\\<br>B_2,R_1,B_1\\<br>B_2,B_1,R_1<br>$$</p><p>我们现在观察第一列，也就是我们取出第一个球的时候，红球的期望是 $1\times\frac{2}{6}=1\times\frac{1}{1+2}$ ，同样第二列和第一列相同，那么前n列都是这样的，所以期望是 $\frac{nA}{A+B}$<br>证毕<br>这里用到了两个思想，</p><ol><li>把全部的取球过程列举出来，避免加权，我们把所有球都当做不同球。</li><li>当把所有的结果列举出来，每行之间是互斥的。把前面每步取球相互影响的试验变成了互不影响的试验</li></ol><p>这个证明使用了等价问题转化的思想。<br>同理我们可以证明每一列的方差是：<br>$$<br>Var(X_i)=\frac{AB}{(A+B)^2}<br>$$<br>因为每列之间并不独立，所以要用到前面方差求和的公式：<br>$$<br>Var(X)=\sum^{n}_{i=1}Var(X_i)+2{\sum\sum}_{i&lt;j}Cov(X_i,X_j)<br>$$<br>因为考虑到所有 $Cov(X_i,X_j)=Coc(X_1,X_2)\text{for }i\neq j$<br>最后的结果就是：<br>$$<br>\begin{aligned}<br>Cov(X_1,X_2)&amp;=-\frac{AB}{(A+B)^2(A+B-1)}\\<br>Var(X_i,X_j)&amp;=\frac{nAB}{(A+B)^2}-\frac{n(n-1)AB}{(A+B)^2(A+B-1)}\\<br>&amp;=\frac{nAB(A+B-1)-n(n-1)AB}{(A+B)^2(A+B-1)}\\<br>&amp;=\frac{nAB}{(A+B)^2}\frac{A+B-n}{(A+B-1)}<br>\end{aligned}<br>$$<br>证毕</p><p>纯计算过程，但可以看出我们的证明也越来越复杂了。</p><h2 id="抽样方法比较-Comparison-of-Sampling-Methods"><a href="#抽样方法比较-Comparison-of-Sampling-Methods" class="headerlink" title="抽样方法比较 Comparison of Sampling Methods"></a>抽样方法比较 Comparison of Sampling Methods</h2><p>上面我们就大致完成了超几何分布的知识，但是要说的是我们又涉及到了采样，我在上面说过，当我们不放回采样的对象，数量不多的时候，我们要用超几何分布来建模，当数量非常大的时候，不放回采样每次采集出现目标的概率变化非常小的时候，我们可以把超几何分布当做二项分布处理，而且我们仔细观察超几何分布的期望和方差，期望和二项分布一致，方差只差一个系数 $\alpha=\frac{T-n}{T-1}$ 这个系数是有名字 “finite population correction” 有限整体校正参数。有限就证明了超几何分布的采样是从有限个样本中进行的，如果样本数量无限大，这个参数接近于1，就变成了二项分布。而当T较小的时候，这个参数对整体结果影响较大，这时超几何分布和二项分布差异较大。<br>分析系数 $\alpha$ 也有很多有趣的，比如$n=T$ 的时候，结果是0，只有一种方法，方差是0.并且 $\alpha\in [0,1]$ 其越接近1，证明放回抽样和不放回抽样差距越小，越接近零表示其差距越大。</p><blockquote><p>Theorem $a_n$ and $c_n$ be sequences of real numbers such that $a_n$ converges to $0$ ,and $c_na_n^2$ converges to $0$ .Then<br>$$<br>lim_{n\to \infty}(1-a_n)^{c_n}e^{-a_nc_n}=1<br>$$<br>In particular,if $a_nc_n$ converges to $b$ ,then $(1+a_n)^{c_n}$ converges to $e^b$</p></blockquote><p>这个定理看起来跟我们前面写的没什么关系，而且证明也没有给出，但是用微积分的知识可以进行证明，我们下面还是看看二项分布和超几何分布之间的区别吧。</p><blockquote><p>Theorem Closeness of Binomial and Hypergeometric Distribution .Let $0&lt; p &lt; 1$ ,and let $n$ be a positive integer.Let $Y$ have the binomial distribution with parameters $n$ and $p$ .For each positive integer $T$ ,let $A_T$ and $B_T$ be integers such that $lim_{T\to \infty}A_{T}=\infty$ , $lim_{T\to \infty}B_{T}=\infty$ ,and $lim_{T\to \infty} A_T/(A_T+B_T)=p$ .Let $X_T$ have the hypergeometric distribution with parameters $A_T,B_T$ ,and $n$ .For each fixed $n$ and each $x=0,\dots,n$<br>$$<br>lim_{T\to \infty}\frac{Pr(Y=x)}{Pr(X_T=x)}=1<br>$$</p></blockquote><p>这个证明用到了上面未证明的定理，而整个证明也比较复杂，书上有详细的过程，我就不写了，想了解详细过程的同学去看看书吧，今天大概就这样了。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文介绍了超几何分布的相关知识，下一篇我们继续研究Poison分布。<br>待续。。。</p><p>原文地址1：<a href="https://www.face2ai.com/Math-Probability-5-3-The-Hypergeomtirc-Distribution">https://www.face2ai.com/Math-Probability-5-3-The-Hypergeomtirc-Distribution</a>转载请标明出处</p></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者： </strong>谭升</li><li class="post-copyright-link"><strong>本文链接：</strong> <a href="http://www.face2ai.com/Math-Probability-5-3-The-Hypergeomtirc-Distribution/" title="\[概率论\]5-3:超几何分布(The Hypergeomtric Distribution)">http://www.face2ai.com/Math-Probability-5-3-The-Hypergeomtirc-Distribution/</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/Math-Probability-5-2-the-Bernoulli-and-Binomial-Distributions/" rel="next" title="\[概率论\]5-2:伯努利和二项分布(The Bernoulli and Binomial Distributions)"><i class="fa fa-chevron-left"></i> \[概率论\]5-2:伯努利和二项分布(The Bernoulli and Binomial Distributions)</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/Math-Probability-5-4-The-Poisson-Distribution/" rel="prev" title="\[概率论\]5-4:泊松分布(The Poisson Distribution)">\[概率论\]5-4:泊松分布(The Poisson Distribution) <i class="fa fa-chevron-right"></i></a></div></div></footer></div></article></div><img src="https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/weixingongzhonghao.jpg" alt="wechat"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-1194454329688573" data-ad-slot="2491973880" data-ad-format="auto" data-full-width-responsive="true"></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div id="sidebar-dimmer"></div><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">站点概览</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/avatar.gif" alt="谭升"><p class="site-author-name" itemprop="name">谭升</p><p class="site-description motion-element" itemprop="description">本站包括强化学习算法，机器学习算法，人工智能，CUDA编程，模式识别算法，线性代数，概率论，数理统计等人工智能原创博客</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives"><span class="site-state-item-count">260</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/index.html"><span class="site-state-item-count">18</span> <span class="site-state-item-name">分类</span></a></div></nav><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/tony-tan" target="_blank" title="GitHub" rel="external nofollow"><i class="fa fa-fw fa-github"></i>GitHub</a> </span><span class="links-of-author-item"><a href="mailto:tony.face2ai@gmail.com" target="_blank" title="E-Mail" rel="external nofollow"><i class="fa fa-fw fa-envelope"></i>E-Mail</a> </span><span class="links-of-author-item"><a href="https://twitter.com/Tony_Face2AI" target="_blank" title="Twitter" rel="external nofollow"><i class="fa fa-fw fa-twitter"></i>Twitter</a></span></div></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#开篇废话"><span class="nav-number">1.</span> <span class="nav-text">开篇废话</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#超几何分布定义和例子-Definition-and-Examples"><span class="nav-number">2.</span> <span class="nav-text">超几何分布定义和例子 Definition and Examples</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#超几何分布的均值和方差-The-Mean-and-Variance-for-a-Hypergeomtirc-Distribution"><span class="nav-number">3.</span> <span class="nav-text">超几何分布的均值和方差 The Mean and Variance for a Hypergeomtirc Distribution</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#抽样方法比较-Comparison-of-Sampling-Methods"><span class="nav-number">4.</span> <span class="nav-text">抽样方法比较 Comparison of Sampling Methods</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#总结"><span class="nav-number">5.</span> <span class="nav-text">总结</span></a></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2018</span> <span class="with-love" id="animate"><i class="fa fa-"></i> </span><span class="author" itemprop="copyrightHolder">谭升</span></div><div class="busuanzi-count"><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="site-uv" title="总访客量"><i class="fa fa-user"></i> <span class="busuanzi-value" id="busuanzi_value_site_uv"></span> </span><span class="site-pv" title="总访问量"><i class="fa fa-eye"></i> <span class="busuanzi-value" id="busuanzi_value_site_pv"></span></span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span id="scrollpercent"><span>0</span>%</span></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/js/src/utils.js?v=6.4.0"></script><script type="text/javascript" src="/js/src/motion.js?v=6.4.0"></script><script type="text/javascript" src="/js/src/affix.js?v=6.4.0"></script><script type="text/javascript" src="/js/src/schemes/pisces.js?v=6.4.0"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=6.4.0"></script><script type="text/javascript" src="/js/src/post-details.js?v=6.4.0"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=6.4.0"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      TeX: {equationNumbers: { autoNumber: "AMS" }}
    });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });</script><script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>