<!DOCTYPE html><html class="theme-next pisces use-motion" lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=6.4.0" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png?v=6.4.0"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32.png?v=6.4.0"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16.png?v=6.4.0"><link rel="mask-icon" href="/images/logo.png?v=6.4.0" color="#222"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Pisces",version:"6.4.0",sidebar:{position:"left",display:"hide",offset:12,b2t:!1,scrollpercent:!0,onmobile:!0},fancybox:!1,fastclick:!1,lazyload:!1,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><meta name="description" content="Abstract: 本文介绍CUDA编程模型的简要结构，包括写一个简单的可执行的CUDA程序，一个正确的CUDA核函数，以及相应的调整设置内存，线程来正确的运行程序。Keywords: CUDA编程模型，CUDA编程结构，内存管理，线程管理，CUDA核函数，CUDA错误处理"><meta name="keywords" content="CUDA编程模型,CUDA编程结构,内存管理,线程管理"><meta property="og:type" content="article"><meta property="og:title" content="【CUDA 基础】2.0 CUDA编程模型概述（一）"><meta property="og:url" content="http://www.face2ai.com/CUDA-F-2-0-CUDA编程模型概述1/index.html"><meta property="og:site_name" content="谭升的博客"><meta property="og:description" content="Abstract: 本文介绍CUDA编程模型的简要结构，包括写一个简单的可执行的CUDA程序，一个正确的CUDA核函数，以及相应的调整设置内存，线程来正确的运行程序。Keywords: CUDA编程模型，CUDA编程结构，内存管理，线程管理，CUDA核函数，CUDA错误处理"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/CUDA-F-2-0-CUDA编程模型概述1/gou.png"><meta property="og:image" content="https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/CUDA-F-2-0-CUDA编程模型概述1/1.png"><meta property="og:image" content="https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/CUDA-F-2-0-CUDA编程模型概述1/2.png"><meta property="og:image" content="https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/CUDA-F-2-0-CUDA编程模型概述1/3.png"><meta property="og:image" content="https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/CUDA-F-2-0-CUDA编程模型概述1/4.png"><meta property="og:updated_time" content="2018-09-26T12:01:15.233Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="【CUDA 基础】2.0 CUDA编程模型概述（一）"><meta name="twitter:description" content="Abstract: 本文介绍CUDA编程模型的简要结构，包括写一个简单的可执行的CUDA程序，一个正确的CUDA核函数，以及相应的调整设置内存，线程来正确的运行程序。Keywords: CUDA编程模型，CUDA编程结构，内存管理，线程管理，CUDA核函数，CUDA错误处理"><meta name="twitter:image" content="https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/CUDA-F-2-0-CUDA编程模型概述1/gou.png"><link rel="canonical" href="http://www.face2ai.com/CUDA-F-2-0-CUDA编程模型概述1/"><script type="text/javascript" id="page.configurations">CONFIG.page={sidebar:""}</script><title>【CUDA 基础】2.0 CUDA编程模型概述（一） | 谭升的博客</title><script async src="https://www.googletagmanager.com/gtag/js?id=UA-105335860-3"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-105335860-3")</script><noscript><style type="text/css">.sidebar-inner,.use-motion .brand,.use-motion .collection-title,.use-motion .comments,.use-motion .menu-item,.use-motion .motion-element,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .logo,.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">谭升的博客</span> <span class="logo-line-after"><i></i></span></a></div><h1 class="site-subtitle" itemprop="description">人工智能基础</h1></div><div class="site-nav-toggle"><button aria-label="切换导航栏"><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-首页"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class="menu-item menu-item-数学类博客目录"><a href="/math/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>数学类博客目录</a></li><li class="menu-item menu-item-算法类博客目录"><a href="/ai/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>算法类博客目录</a></li><li class="menu-item menu-item-编程类博客目录"><a href="/program/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>编程类博客目录</a></li><li class="menu-item menu-item-其他笔记目录"><a href="/other/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>其他笔记目录</a></li></ul></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-1194454329688573" data-ad-slot="2491973880" data-ad-format="auto" data-full-width-responsive="true"></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://www.face2ai.com/CUDA-F-2-0-CUDA编程模型概述1/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="谭升"><meta itemprop="description" content="强化学习 机器学习 人工智能"><meta itemprop="image" content="/images/avatar.gif"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="谭升的博客"></span><header class="post-header"><h2 class="post-title" itemprop="name headline">【CUDA 基础】2.0 CUDA编程模型概述（一）</h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2018-02-15 17:25:38" itemprop="dateCreated datePublished" datetime="2018-02-15T17:25:38+08:00">2018-02-15</time> </span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/CUDA/" itemprop="url" rel="index"><span itemprop="name">CUDA</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/CUDA/Freshman/" itemprop="url" rel="index"><span itemprop="name">Freshman</span></a></span> </span><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> 阅读次数： <span class="busuanzi-value" id="busuanzi_value_page_pv"></span></span></div></header><div class="post-body" itemprop="articleBody"><p><strong>Abstract:</strong> 本文介绍CUDA编程模型的简要结构，包括写一个简单的可执行的CUDA程序，一个正确的CUDA核函数，以及相应的调整设置内存，线程来正确的运行程序。<br><strong>Keywords:</strong> CUDA编程模型，CUDA编程结构，内存管理，线程管理，CUDA核函数，CUDA错误处理</p><a id="more"></a><h1 id="CUDA编程模型概述（一）"><a href="#CUDA编程模型概述（一）" class="headerlink" title="CUDA编程模型概述（一）"></a>CUDA编程模型概述（一）</h1><p>过年了，祝大家新年快乐，新年希望自己学习的东西能都学会<br><img src="https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/CUDA-F-2-0-CUDA编程模型概述1/gou.png" alt=""><br>这是一只不爱学习的狗，总看电视！</p><p>编程模型就是告诉我们如何写CUDA程序，如果做过C开发的同学或者其他开发的同学都知道做个完整的项目不只是写代码，还有需求分析，调试，优化，部署等一些列步骤。CUDA平台也提供了着一些列的工具供我们使用，我们这一章主要就是讲解这些工具怎么用，如何编写调试CUDA程序。以及编写两个矩阵运算有关的CUDA应用，以供大家把玩。</p><h2 id="CUDA编程模型概述"><a href="#CUDA编程模型概述" class="headerlink" title="CUDA编程模型概述"></a>CUDA编程模型概述</h2><p>CUDA编程模型为应用和硬件设备之间的桥梁，所以CUDA C是编译型语言，不是解释型语言，OpenCL就有点类似于解释型语言，通过编译器和链接，给操作系统执行（操作系统包括GPU在内的系统），下面的结构图片能形象的表现他们之间的关系：<br><img src="https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/CUDA-F-2-0-CUDA编程模型概述1/1.png" alt=""></p><p>其中Communication Abstraction是编程模型和编译器，库函数之间的分界线。<br>可能大家还不太明白编程模型是啥，编程模型可以理解为，我们要用到的语法，内存结构，线程结构等这些我们写程序时我们自己控制的部分，这些部分控制了异构计算设备的工作模式，都是属于编程模型。<br>GPU中大致可以分为：</p><ul><li>核函数</li><li>内存管理</li><li>线程管理</li><li>流</li></ul><p>等几个关键部分。<br>以上这些理论同时也适用于其他非CPU+GPU异构的组合。<br>下面我们会说两个我们GPU架构下特有几个功能：</p><ul><li>通过组织层次结构在GPU上组织线程的方法</li><li>通过组织层次结构在GPU上组织内存的方法</li></ul><p>也就是对内存和线程的控制将伴随我们写完前十几篇。<br>从宏观上我们可以从以下几个环节完成CUDA应用开发：</p><ol><li>领域层</li><li>逻辑层</li><li>硬件层</li></ol><p>第一步就是在领域层（也就是你所要解决问题的条件）分析数据和函数，以便在并行运行环境中能正确，高效地解决问题。<br>当分析设计完程序就进入了编程阶段，我们关注点应转向如何组织并发进程，这个阶段要从逻辑层面思考。<br>CUDA模型主要的一个功能就是线程层结构抽象的概念，以允许控制线程行为。这个抽象为并行变成提供了良好的可扩展性（这个扩展性后面有提到，就是一个CUDA程序可以在不同的GPU机器上运行，即使计算能力不同）。<br>在硬件层上，通过理解线程如何映射到机器上，能充分帮助我们提高性能。</p><h2 id="CUDA编程结构"><a href="#CUDA编程结构" class="headerlink" title="CUDA编程结构"></a>CUDA编程结构</h2><p>一个异构环境，通常有多个CPU多个GPU，他们都通过PCIe总线相互通信，也是通过PCIe总线分隔开的。所以我们要区分一下两种设备的内存：</p><ul><li>主机：CPU及其内存</li><li>设备：GPU及其内存</li></ul><p>注意这两个内存从硬件到软件都是隔离的（CUDA6.0 以后支持统一寻址），我们目前先不研究统一寻址，我们现在还是用内存来回拷贝的方法来编写调试程序，以巩固大家对两个内存隔离这个事实的理解。</p><p>一个完整的CUDA应用可能的执行顺序如下图：<br><img src="https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/CUDA-F-2-0-CUDA编程模型概述1/2.png" alt=""><br>从host的串行到调用核函数（核函数被调用后控制马上归还主机线程，也就是在第一个并行代码执行时，很有可能第二段host代码已经开始同步执行了）。</p><p>我们接下来的研究层次是：</p><ul><li>内存</li><li>线程</li><li>核函数<ul><li>启动核函数</li><li>编写核函数</li><li>验证核函数</li></ul></li><li>错误处理<h2 id="内存管理"><a href="#内存管理" class="headerlink" title="内存管理"></a>内存管理</h2>内存管理在传统串行程序是非常常见的，寄存器空间，栈空间内的内存由机器自己管理，堆空间由用户控制分配和释放，CUDA程序同样，只是CUDA提供的API可以分配管理设备上的内存，当然也可以用CDUA管理主机上的内存，主机上的传统标准库也能完成主机内存管理。<br>下面表格有一些主机API和CUDA C的API的对比：</li></ul><table><thead><tr><th style="text-align:center">标准C函数</th><th style="text-align:center">CUDA C 函数</th><th style="text-align:center">说明</th></tr></thead><tbody><tr><td style="text-align:center">malloc</td><td style="text-align:center">cudaMalloc</td><td style="text-align:center">内存分配</td></tr><tr><td style="text-align:center">memcpy</td><td style="text-align:center">cudaMemcpy</td><td style="text-align:center">内存复制</td></tr><tr><td style="text-align:center">memset</td><td style="text-align:center">cudaMemset</td><td style="text-align:center">内存设置</td></tr><tr><td style="text-align:center">free</td><td style="text-align:center">cudaFree</td><td style="text-align:center">释放内存</td></tr></tbody></table><p>我们先研究最关键的一步，这一步要走总线的（郭德纲：我到底能不能走二环）</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaMemcpy</span><span class="params">(<span class="keyword">void</span> * dst,<span class="keyword">const</span> <span class="keyword">void</span> * src,<span class="keyword">size_t</span> count,</span></span></span><br><span class="line"><span class="function"><span class="params">  cudaMemcpyKind kind)</span></span></span><br></pre></td></tr></table></figure><p>这个函数是内存拷贝过程，可以完成以下几种过程（cudaMemcpyKind kind）</p><ul><li>cudaMemcpyHostToHost</li><li>cudaMemcpyHostToDevice</li><li>cudaMemcpyDeviceToHost</li><li>cudaMemcpyDeviceToDevice</li></ul><p>这四个过程的方向可以清楚的从字面上看出来，这里就不废话了，如果函数执行成功，则会返回 cudaSuccess 否则返回 cudaErrorMemoryAllocation</p><p>使用下面这个指令可以吧上面的错误代码翻译成详细信息：<br></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">char</span>* <span class="title">cudaGetErrorString</span><span class="params">(cudaError_t error)</span></span></span><br></pre></td></tr></table></figure><p></p><p>内存是分层次的，下图可以简单地描述，但是不够准确，后面我们会详细介绍每一个具体的环节：</p><p><img src="https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/CUDA-F-2-0-CUDA编程模型概述1/3.png" alt=""><br>共享内存（shared Memory）和全局内存（global Memory）后面我们会特别详细深入的研究，这里我们来个例子，两个向量的加法：</p><p>代码库：<a href="https://github.com/Tony-Tan/CUDA_Freshman" target="_blank" rel="noopener">https://github.com/Tony-Tan/CUDA_Freshman</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">* https://github.com/Tony-Tan/CUDA_Freshman</span></span><br><span class="line"><span class="comment">* 3_sum_arrays</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"freshman.h"</span></span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">sumArrays</span><span class="params">(<span class="keyword">float</span> * a,<span class="keyword">float</span> * b,<span class="keyword">float</span> * res,<span class="keyword">const</span> <span class="keyword">int</span> size)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;size;i+=<span class="number">4</span>)</span><br><span class="line">  &#123;</span><br><span class="line">    res[i]=a[i]+b[i];</span><br><span class="line">    res[i+<span class="number">1</span>]=a[i+<span class="number">1</span>]+b[i+<span class="number">1</span>];</span><br><span class="line">    res[i+<span class="number">2</span>]=a[i+<span class="number">2</span>]+b[i+<span class="number">2</span>];</span><br><span class="line">    res[i+<span class="number">3</span>]=a[i+<span class="number">3</span>]+b[i+<span class="number">3</span>];</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">sumArraysGPU</span><span class="params">(<span class="keyword">float</span>*a,<span class="keyword">float</span>*b,<span class="keyword">float</span>*res)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">int</span> i=threadIdx.x;</span><br><span class="line">  res[i]=a[i]+b[i];</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc,<span class="keyword">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">int</span> dev = <span class="number">0</span>;</span><br><span class="line">  cudaSetDevice(dev);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span> nElem=<span class="number">32</span>;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">"Vector size:%d\n"</span>,nElem);</span><br><span class="line">  <span class="keyword">int</span> nByte=<span class="keyword">sizeof</span>(<span class="keyword">float</span>)*nElem;</span><br><span class="line">  <span class="keyword">float</span> *a_h=(<span class="keyword">float</span>*)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="keyword">float</span> *b_h=(<span class="keyword">float</span>*)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="keyword">float</span> *res_h=(<span class="keyword">float</span>*)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="keyword">float</span> *res_from_gpu_h=(<span class="keyword">float</span>*)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="built_in">memset</span>(res_h,<span class="number">0</span>,nByte);</span><br><span class="line">  <span class="built_in">memset</span>(res_from_gpu_h,<span class="number">0</span>,nByte);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">float</span> *a_d,*b_d,*res_d;</span><br><span class="line">  CHECK(cudaMalloc((<span class="keyword">float</span>**)&amp;a_d,nByte));</span><br><span class="line">  CHECK(cudaMalloc((<span class="keyword">float</span>**)&amp;b_d,nByte));</span><br><span class="line">  CHECK(cudaMalloc((<span class="keyword">float</span>**)&amp;res_d,nByte));</span><br><span class="line"></span><br><span class="line">  initialData(a_h,nElem);</span><br><span class="line">  initialData(b_h,nElem);</span><br><span class="line"></span><br><span class="line">  CHECK(cudaMemcpy(a_d,a_h,nByte,cudaMemcpyHostToDevice));</span><br><span class="line">  CHECK(cudaMemcpy(b_d,b_h,nByte,cudaMemcpyHostToDevice));</span><br><span class="line"></span><br><span class="line">  <span class="function">dim3 <span class="title">block</span><span class="params">(nElem)</span></span>;</span><br><span class="line">  <span class="function">dim3 <span class="title">grid</span><span class="params">(nElem/block.x)</span></span>;</span><br><span class="line">  sumArraysGPU&lt;&lt;&lt;grid,block&gt;&gt;&gt;(a_d,b_d,res_d);</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">"Execution configuration&lt;&lt;&lt;%d,%d&gt;&gt;&gt;\n"</span>,block.x,grid.x);</span><br><span class="line"></span><br><span class="line">  CHECK(cudaMemcpy(res_from_gpu_h,res_d,nByte,cudaMemcpyDeviceToHost));</span><br><span class="line">  sumArrays(a_h,b_h,res_h,nElem);</span><br><span class="line"></span><br><span class="line">  checkResult(res_h,res_from_gpu_h,nElem);</span><br><span class="line">  cudaFree(a_d);</span><br><span class="line">  cudaFree(b_d);</span><br><span class="line">  cudaFree(res_d);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">free</span>(a_h);</span><br><span class="line">  <span class="built_in">free</span>(b_h);</span><br><span class="line">  <span class="built_in">free</span>(res_h);</span><br><span class="line">  <span class="built_in">free</span>(res_from_gpu_h);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后使用nvcc编译我们的程序（我们代码库用cmake管理工程，更方便）</p><p>解释下内存管理部分的代码：<br></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaMalloc((<span class="keyword">float</span>**)&amp;a_d,nByte);</span><br></pre></td></tr></table></figure><p></p><p>分配设备端的内存空间，为了区分设备和主机端内存，我们可以给变量加后缀或者前缀h_表示host，d_表示device</p><font color="FF0000">一个经常会发生的错误就是混用设备和主机的内存地址！！</font><h2 id="线程管理"><a href="#线程管理" class="headerlink" title="线程管理"></a>线程管理</h2><p>当内核函数开始执行，如何组织GPU的线程就变成了最主要的问题了，我们必须明确，一个核函数只能有一个grid，一个grid可以有很多个块，每个块可以有很多的线程，这种分层的组织结构使得我们的并行过程更加自如灵活：<br><img src="https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/CUDA-F-2-0-CUDA编程模型概述1/4.png" alt=""></p><p>一个线程块block中的线程可以完成下述协作：</p><ul><li>同步</li><li>共享内存</li></ul><p><font color="FF0000">不同块内线程不能相互影响！他们是物理隔离的！</font><br>接下来就是给每个线程一个编号了，我们知道每个线程都执行同样的一段串行代码，那么怎么让这段相同的代码对应不同的数据呢？首先第一步就是让这些线程彼此区分开，才能对应到相应从线程，使得这些线程也能区分自己的数据。如果线程本身没有任何标记，那么没办法确认其行为。<br>依靠下面两个内置结构体确定线程标号：</p><ul><li>blockIdx（线程块在线程网格内的位置索引）</li><li>threadIdx（线程在线程块内的位置索引）</li></ul><p>注意这里的Idx是index的缩写（我之前一直以为是identity x的缩写），这两个内置结构体基于 uint3 定义，包含三个无符号整数的结构，通过三个字段来指定：</p><ul><li>blockIdx.x</li><li>blockIdx.y</li><li>blockIdx.z</li><li>threadIdx.x</li><li>threadIdx.y</li><li>threadIdx.z</li></ul><p>上面这两个是坐标，当然我们要有同样对应的两个结构体来保存其范围，也就是blockIdx中三个字段的范围threadIdx中三个字段的范围：</p><ul><li>blockDim</li><li>gridDim</li></ul><p>他们是dim3类型(基于uint3定义的数据结构)的变量，也包含三个字段x,y,z.</p><ul><li>blockDim.x</li><li>blockDim.y</li><li>blockDim.z</li></ul><p>网格和块的维度一般是二维和三维的，也就是说一个网格通常被分成二维的块，而每个块常被分成三维的线程。<br>注意：dim3是手工定义的，主机端可见。uint3是设备端在执行的时候可见的，不可以在核函数运行时修改，初始化完成后uint3值就不变了。他们是有区别的！这一点必须要注意。</p><p>下面有一段代码，块的索引和维度：<br></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">*1_check_dimension</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">checkIndex</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">"threadIdx:(%d,%d,%d) blockIdx:(%d,%d,%d) blockDim:(%d,%d,%d)\</span></span><br><span class="line"><span class="string">  gridDim(%d,%d,%d)\n"</span>,threadIdx.x,threadIdx.y,threadIdx.z,</span><br><span class="line">  blockIdx.x,blockIdx.y,blockIdx.z,blockDim.x,blockDim.y,blockDim.z,</span><br><span class="line">  gridDim.x,gridDim.y,gridDim.z);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc,<span class="keyword">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">int</span> nElem=<span class="number">6</span>;</span><br><span class="line">  <span class="function">dim3 <span class="title">block</span><span class="params">(<span class="number">3</span>)</span></span>;</span><br><span class="line">  dim3 grid((nElem+block.x-1)/block.x);</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">"grid.x %d grid.y %d grid.z %d\n"</span>,grid.x,grid.y,grid.z);</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">"block.x %d block.y %d block.z %d\n"</span>,block.x,block.y,block.z);</span><br><span class="line">  checkIndex&lt;&lt;&lt;grid,block&gt;&gt;&gt;();</span><br><span class="line">  cudaDeviceReset();</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>可以运行得到不同线程分解方式</p><p>此处有图，明天补上！</p><p>接下来这段代码是检查网格和块的大小的：<br></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">*2_grid_block</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc,<span class="keyword">char</span> ** argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">int</span> nElem=<span class="number">1024</span>;</span><br><span class="line">  <span class="function">dim3 <span class="title">block</span><span class="params">(<span class="number">1024</span>)</span></span>;</span><br><span class="line">  dim3 grid((nElem-1)/block.x+1);</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">"grid.x %d block.x %d\n"</span>,grid.x,block.x);</span><br><span class="line"></span><br><span class="line">  block.x=<span class="number">512</span>;</span><br><span class="line">  grid.x=(nElem<span class="number">-1</span>)/block.x+<span class="number">1</span>;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">"grid.x %d block.x %d\n"</span>,grid.x,block.x);</span><br><span class="line"></span><br><span class="line">  block.x=<span class="number">256</span>;</span><br><span class="line">  grid.x=(nElem<span class="number">-1</span>)/block.x+<span class="number">1</span>;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">"grid.x %d block.x %d\n"</span>,grid.x,block.x);</span><br><span class="line"></span><br><span class="line">  block.x=<span class="number">128</span>;</span><br><span class="line">  grid.x=(nElem<span class="number">-1</span>)/block.x+<span class="number">1</span>;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">"grid.x %d block.x %d\n"</span>,grid.x,block.x);</span><br><span class="line"></span><br><span class="line">  cudaDeviceReset();</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>这里也有图，明天补上</p><p>网格和块的维度存在几个限制因素，块大小主要与可利用的计算资源有关，如寄存器共享内存。<br>分成网格和块的方式可以使得我们的CUDA程序可以在任意的设备上执行。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>今天先介绍第一部分，主要是从宏观上给出内存，线程以及核函数的相互作用，通过这些特征的相互配合形成可以高速正确执行的CUDA程序，下一篇概述一下核函数的一些特征，待续。。。</p><p>原文地址1：<a href="https://www.face2ai.com/CUDA-F-2-0-CUDA编程模型概述1">https://www.face2ai.com/CUDA-F-2-0-CUDA编程模型概述1</a>转载请标明出处</p></div><div><div style="padding:10px 0;margin:20px auto;width:90%;text-align:center"><div>关注公众号或添加博主微信，反馈问题，获取资讯（暗号:face2ai）</div><button id="rewardButton" disable="enable" onclick='var qr=document.getElementById("QR");"none"===qr.style.display?qr.style.display="block":qr.style.display="none"'><span>联系博主</span></button><div id="QR" style="display:none"><div id="wechat" style="display:inline-block"><img id="wechat_qr" src="/images/weixin.png" alt="谭升 博主微信"><p>博主微信</p></div><div id="alipay" style="display:inline-block"><img id="alipay_qr" src="/images/alipay.jpg" alt="谭升 博客公众号"><p>博客公众号</p></div></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者： </strong>谭升</li><li class="post-copyright-link"><strong>本文链接：</strong> <a href="http://www.face2ai.com/CUDA-F-2-0-CUDA编程模型概述1/" title="【CUDA 基础】2.0 CUDA编程模型概述（一）">http://www.face2ai.com/CUDA-F-2-0-CUDA编程模型概述1/</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/CUDA-F-1-1-异构计算-CUDA/" rel="next" title="【CUDA 基础】1.1 异构计算与CUDA"><i class="fa fa-chevron-left"></i> 【CUDA 基础】1.1 异构计算与CUDA</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/CUDA-F-2-1-CUDA编程模型概述2/" rel="prev" title="【CUDA 基础】2.1 CUDA编程模型概述（二）">【CUDA 基础】2.1 CUDA编程模型概述（二） <i class="fa fa-chevron-right"></i></a></div></div></footer></div></article></div><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-1194454329688573" data-ad-slot="2491973880" data-ad-format="auto" data-full-width-responsive="true"></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div id="sidebar-dimmer"></div><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">站点概览</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/avatar.gif" alt="谭升"><p class="site-author-name" itemprop="name">谭升</p><p class="site-description motion-element" itemprop="description">强化学习 机器学习 人工智能</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives"><span class="site-state-item-count">262</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/index.html"><span class="site-state-item-count">20</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/index.html"><span class="site-state-item-count">7</span> <span class="site-state-item-name">标签</span></a></div></nav><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/tony-tan" target="_blank" title="GitHub" rel="external nofollow"><i class="fa fa-fw fa-github"></i>GitHub</a> </span><span class="links-of-author-item"><a href="mailto:tony.face2ai@gmail.com" target="_blank" title="E-Mail" rel="external nofollow"><i class="fa fa-fw fa-envelope"></i>E-Mail</a> </span><span class="links-of-author-item"><a href="https://twitter.com/Tony_Face2AI" target="_blank" title="Twitter" rel="external nofollow"><i class="fa fa-fw fa-twitter"></i>Twitter</a></span></div><div class="links-of-blogroll motion-element links-of-blogroll-block"><div class="links-of-blogroll-title"><i class="fa fa-history fa-" aria-hidden="true"></i> 近期文章</div><ul class="links-of-blogroll-list"><li><a href="/Other-Website-Contents/" title="本站目录" target="_blank">本站目录</a></li><li><a href="/RL-RSAB-1-4-1-Connection-to-Optimization-Method/" title="【强化学习】 1.4.1 强化学习与优化方法" target="_blank">【强化学习】 1.4.1 强化学习与优化方法</a></li><li><a href="/Julia-Lang-0-Introduction/" title="【Julia】 Julia编程语言介绍" target="_blank">【Julia】 Julia编程语言介绍</a></li><li><a href="/ITILA-Introduction-to-This-Series/" title="【信息论、推理与学习算法】本系列博客介绍" target="_blank">【信息论、推理与学习算法】本系列博客介绍</a></li><li><a href="/RL-RSAB-1-4-0-Limitations-and-Scope/" title="【强化学习】 1.4.0 “进化方法”（Evolutionary Method）和 “决策梯度方法” (Policy Gradient Methods) 概论" target="_blank">【强化学习】 1.4.0 “进化方法”（Evolutionary Method）和 “决策梯度方法” (Policy Gradient Methods) 概论</a></li></ul></div></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#CUDA编程模型概述（一）"><span class="nav-number">1.</span> <span class="nav-text">CUDA编程模型概述（一）</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#CUDA编程模型概述"><span class="nav-number">1.1.</span> <span class="nav-text">CUDA编程模型概述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CUDA编程结构"><span class="nav-number">1.2.</span> <span class="nav-text">CUDA编程结构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#内存管理"><span class="nav-number">1.3.</span> <span class="nav-text">内存管理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#线程管理"><span class="nav-number">1.4.</span> <span class="nav-text">线程管理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#总结"><span class="nav-number">1.5.</span> <span class="nav-text">总结</span></a></li></ol></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2018</span> <span class="with-love" id="animate"><i class="fa fa-"></i> </span><span class="author" itemprop="copyrightHolder">谭升</span></div><div class="busuanzi-count"><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="site-uv" title="总访客量"><i class="fa fa-user"></i> <span class="busuanzi-value" id="busuanzi_value_site_uv"></span> </span><span class="site-pv" title="总访问量"><i class="fa fa-eye"></i> <span class="busuanzi-value" id="busuanzi_value_site_pv"></span></span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span id="scrollpercent"><span>0</span>%</span></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/js/src/utils.js?v=6.4.0"></script><script type="text/javascript" src="/js/src/motion.js?v=6.4.0"></script><script type="text/javascript" src="/js/src/affix.js?v=6.4.0"></script><script type="text/javascript" src="/js/src/schemes/pisces.js?v=6.4.0"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=6.4.0"></script><script type="text/javascript" src="/js/src/post-details.js?v=6.4.0"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=6.4.0"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      TeX: {equationNumbers: { autoNumber: "AMS" }}
    });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });</script><script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>