<!DOCTYPE html>












  


<html class="theme-next muse use-motion" lang="zh-CN">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2"/>
<meta name="theme-color" content="#222">












<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />






















<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=6.4.0" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png?v=6.4.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32.png?v=6.4.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16.png?v=6.4.0">


  <link rel="mask-icon" href="/images/logo.png?v=6.4.0" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '6.4.0',
    sidebar: {"position":"left","display":"hide","offset":12,"b2t":false,"scrollpercent":true,"onmobile":true},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="Abstract: 本文主要介绍CUDA内存管理，以及CUDA内存模型下的各种内存的特点。Keywords: CUDA内存管理，CUDA内存分配和释放，CUDA内存传输，固定内存，零拷贝内存，统一虚拟寻址，统一内存寻址">
<meta name="keywords" content="CUDA内存管理,CUDA内存分配和释放,CUDA内存传输,固定内存,零拷贝内存,统一虚拟寻址,统一内存寻址">
<meta property="og:type" content="article">
<meta property="og:title" content="【CUDA 基础】4.2 内存管理">
<meta property="og:url" content="http://www.face2ai.com/CUDA-F-4-2-内存管理/index.html">
<meta property="og:site_name" content="谭升的博客">
<meta property="og:description" content="Abstract: 本文主要介绍CUDA内存管理，以及CUDA内存模型下的各种内存的特点。Keywords: CUDA内存管理，CUDA内存分配和释放，CUDA内存传输，固定内存，零拷贝内存，统一虚拟寻址，统一内存寻址">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/CUDA-F-4-2-内存管理/4-3.png">
<meta property="og:image" content="https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/CUDA-F-4-2-内存管理/4-4.png">
<meta property="og:image" content="https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/CUDA-F-4-2-内存管理/res1.png">
<meta property="og:image" content="https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/CUDA-F-4-2-内存管理/res2.png">
<meta property="og:image" content="https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/CUDA-F-4-2-内存管理/res3.png">
<meta property="og:image" content="https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/CUDA-F-4-2-内存管理/4-5.png">
<meta property="og:image" content="https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/CUDA-F-4-2-内存管理/res4.png">
<meta property="og:updated_time" content="2018-09-26T12:01:15.203Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="【CUDA 基础】4.2 内存管理">
<meta name="twitter:description" content="Abstract: 本文主要介绍CUDA内存管理，以及CUDA内存模型下的各种内存的特点。Keywords: CUDA内存管理，CUDA内存分配和释放，CUDA内存传输，固定内存，零拷贝内存，统一虚拟寻址，统一内存寻址">
<meta name="twitter:image" content="https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/CUDA-F-4-2-内存管理/4-3.png">






  <link rel="canonical" href="http://www.face2ai.com/CUDA-F-4-2-内存管理/"/>



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>【CUDA 基础】4.2 内存管理 | 谭升的博客</title>
  




<script async src="https://www.googletagmanager.com/gtag/js?id=UA-105335860-3"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-105335860-3');
</script>






  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">谭升的博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <h1 class="site-subtitle" itemprop="description">人工智能基础</h1>
      
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-首页">
    <a href="/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-home"></i> <br />首页</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-数学目录">
    <a href="/math/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />数学目录</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-算法目录">
    <a href="/ai-blog/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-calendar"></i> <br />算法目录</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-编程目录">
    <a href="/program-blog/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br />编程目录</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-julia">
    <a href="/Julia/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />Julia</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-其他笔记">
    <a href="/other-note/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-th"></i> <br />其他笔记</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-face2ai社区">
    <a href="http:/bbs.face2ai.com" rel="section">
      <i class="menu-item-icon fa fa-fw fa-heartbeat"></i> <br />FACE2AI社区</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-阅读排行榜">
    <a href="/top/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-signal"></i> <br />阅读排行榜</a>
  </li>

      
      
    </ul>
  

  
    

  

  
</nav>



  


</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            



<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- footer -->
<ins class="adsbygoogle"
style="display:block"
data-ad-client="ca-pub-1194454329688573"
data-ad-slot="2491973880"
data-ad-format="auto"
data-full-width-responsive="true"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.face2ai.com/CUDA-F-4-2-内存管理/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="谭升">
      <meta itemprop="description" content="强化学习 机器学习 人工智能">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="谭升的博客">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">【CUDA 基础】4.2 内存管理
              
            
          </h2>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-05-01 21:39:47" itemprop="dateCreated datePublished" datetime="2018-05-01T21:39:47+08:00">2018-05-01</time>
            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/CUDA/" itemprop="url" rel="index"><span itemprop="name">CUDA</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/CUDA/Freshman/" itemprop="url" rel="index"><span itemprop="name">Freshman</span></a></span>

                
                
              
            </span>

          

          
            
          

          
          
             <span id="/CUDA-F-4-2-内存管理/" class="leancloud_visitors" data-flag-title="【CUDA 基础】4.2 内存管理">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数：</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          


        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p><strong>Abstract:</strong> 本文主要介绍CUDA内存管理，以及CUDA内存模型下的各种内存的特点。<br><strong>Keywords:</strong> CUDA内存管理，CUDA内存分配和释放，CUDA内存传输，固定内存，零拷贝内存，统一虚拟寻址，统一内存寻址</p>
<a id="more"></a>
<h1 id="内存管理"><a href="#内存管理" class="headerlink" title="内存管理"></a>内存管理</h1><p>迷茫和困惑会影响我们的前进，彻底摆脱也许不太可能，但是我们必须肯定信仰的力量，专注你所热爱的，就会走出迷雾。</p>
<p>CUDA编程的目的是给我们的程序加速，尤其是机器学习，人工智能类的计算，CPU不能高效完成，说白了，我们在控制硬件，控制硬件的语言属于底层语言，比如C语言，最头疼的就是管理内存，python，php这些语言有自己的内存管理机制，c语言的内存管理机制——程序员管理。这样的好处是学起来特别困难，但是学会了又会觉得特别爽，因为自由，你可以随意的控制计算机的计算过程。CUDA是C语言的扩展，内存方面基本集成了C语言的方式，由程序员控制CUDA内存，当然，这些内存的物理设备是在GPU上的，而且与CPU内存分配不同，CPU内存分配完就完事了，GPU还涉及到数据传输，主机和设备之间的传输。<br>接下来我们要了解的是：</p>
<ul>
<li>分配释放设备内存</li>
<li>在主机和设备间传输内存</li>
</ul>
<p>为达到最优性能，CUDA提供了在主机端准备设备内存的函数，并且显式地向设备传递数据，显式的从设备取回数据。</p>
<h2 id="内存分配和释放"><a href="#内存分配和释放" class="headerlink" title="内存分配和释放"></a>内存分配和释放</h2><p>内存的分配和释放我们在前面已经用过很多次了，前面所有的要计算的例子都包含这一步：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaMalloc</span><span class="params">(<span class="keyword">void</span> ** devPtr,<span class="keyword">size_t</span> count)</span></span></span><br></pre></td></tr></table></figure></p>
<p>这个函数用过很多次了，唯一要注意的是第一个参数，是指针的指针，一般的用法是首先我们生命一个指针变量，然后调用这个函数：<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">float</span> * devMem=<span class="literal">NULL</span>;</span><br><span class="line">cudaError_t cudaMalloc((float**) devMem, count)</span><br></pre></td></tr></table></figure></p>
<p>这里是这样的，devMem是一个指针，定义时初始化指向NULL，这样做是安全的，避免出现野指针，cudaMalloc函数要修改devMem的值，所以必须把他的指针传递给函数，如果把devMem当做参数传递，经过函数后，指针的内容还是NULL。<br>不知道这个解释有没有听明白，通俗的讲，如果一个参数想要在函数中被修改，那么一定要传递他的地址给函数，如果只传递本身，函数是值传递的，不会改变参数的值。<br>内存分配支持所有的数据类型，什么int，float。。。这些都无所谓，因为他是按照字节分配的，只要是正数字节的变量都能分配，当然我们根本没有半个字节的东西。<br>函数执行失败返回：cudaErrorMemoryAllocation.<br>当分配完地址后，可以使用下面函数进行初始化：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaMemset</span><span class="params">(<span class="keyword">void</span> * devPtr,<span class="keyword">int</span> value,<span class="keyword">size_t</span> count)</span></span></span><br></pre></td></tr></table></figure></p>
<p>用法和Memset类似，但是注意，这些被我们操作的内存对应的物理内存都在GPU上。<br>当分配的内存不被使用时，使用下面语句释放程序。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaFree</span><span class="params">(<span class="keyword">void</span> * devPtr)</span></span></span><br></pre></td></tr></table></figure></p>
<p>注意这个参数一定是前面cudaMalloc类的函数（还有其他分配函数）分配到空间，如果输入非法指针参数，会返回 cudaErrorInvalidDevicePointer 错误，如果重复释放一个空间，也会报错。<br>目前为止，套路基本和C语言一致。但是，设备内存的分配和释放非常影响性能，所以，尽量重复利用！</p>
<h2 id="内存传输"><a href="#内存传输" class="headerlink" title="内存传输"></a>内存传输</h2><p>下面介绍点C语言没有的，C语言的内存分配完成后就可以直接读写了，但是对于异构计算，这样是不行的，因为主机线程不能访问设备内存，设备线程也不能访问主机内存，这时候我们要传送数据了：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaMemcpy</span><span class="params">(<span class="keyword">void</span> *dst,<span class="keyword">const</span> <span class="keyword">void</span> * src,<span class="keyword">size_t</span> count,<span class="keyword">enum</span> cudaMemcpyKind kind)</span></span></span><br></pre></td></tr></table></figure></p>
<p>这个函数我们前面也反复用到，注意这里的参数是指针，而不是指针的指针，第一个参数dst是目标地址，第二个参数src是原始地址，然后是拷贝的内存大小，最后是传输类型，传输类型包括以下几种：</p>
<ul>
<li>cudaMemcpyHostToHost</li>
<li>cudaMemcpyHostToDevice</li>
<li>cudaMemcpyDeviceToHost</li>
<li>cudaMemcpyDeviceToDevice</li>
</ul>
<p>四种方式，都写在字面上来，唯一有点问题的就是有个host 到host，不知道为啥存在，估计很多人跟我想法一样，可能后面有什么高级的用法。<br>这个例子也不用说了，前面随便找个有数据传输的都有这两步：从主机到设备，然后计算，最后从设备到主机。<br>代码省略，来张图：<br><img src="https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/CUDA-F-4-2-内存管理/4-3.png" alt="4-3"><br>GPU的内存采用的DDR5制式，2011三星才做出来DDR4的主机内存，但是GPU却一直在使用DDR5，这个具体原因我也不清楚，有兴趣的同学自行去查询，但是我们要说的是GPU的内存理论峰值带宽非常高，对于Fermi C2050 有144GB/s，这个值估计现在的GPU应该都超过了，CPU和GPU之间通信要经过PCIe总线，总线的理论峰值要低很多——8GB/s左右，也就是说所，管理不当，算到半路需要从主机读数据，那效率瞬间全挂在PCIe上了。<br>CUDA编程需要大家减少主机和设备之间的内存传输。</p>
<h2 id="固定内存"><a href="#固定内存" class="headerlink" title="固定内存"></a>固定内存</h2><p>主机内存采用分页式管理，通俗的说法就是操作系统把物理内存分成一些“页”，然后给一个应用程序一大块内存，但是这一大块内存可能在一些不连续的页上，应用只能看到虚拟的内存地址，而操作系统可能随时更换物理地址的页（从原始地址复制到另一个地址）但是应用是不会差觉得，但是从主机传输到设备上的时候，如果此时发生了页面移动，对于传输操作来说是致命的，所以在数据传输之前，CUDA驱动会锁定页面，或者直接分配固定的主机内存，将主机源数据复制到固定内存上，然后从固定内存传输数据到设备上：<br><img src="https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/CUDA-F-4-2-内存管理/4-4.png" alt="4-4"><br>上图左边是正常分配内存，传输过程是：锁页-复制到固定内存-复制到设备<br>右边时分配时就是固定内存，直接传输到设备上。<br>下面函数用来分配固定内存：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaMallocHost</span><span class="params">(<span class="keyword">void</span> ** devPtr,<span class="keyword">size_t</span> count)</span></span></span><br></pre></td></tr></table></figure></p>
<p>分配count字节的固定内存，这些内存是页面锁定的，可以直接传输到设备的（翻译的原文写的是：设备可访问的，英文原文是：Since the pinned memory can be accessed directly by the device。应该是翻译问题）这样就是的传输带宽变得高很多。<br>固定的主机内存释放使用：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaFreeHost</span><span class="params">(<span class="keyword">void</span> *ptr)</span></span></span><br></pre></td></tr></table></figure></p>
<p>我们可以测试一下固定内存和分页内存的传输效率，代码如下<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"freshman.h"</span></span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">sumArrays</span><span class="params">(<span class="keyword">float</span> * a,<span class="keyword">float</span> * b,<span class="keyword">float</span> * res,<span class="keyword">const</span> <span class="keyword">int</span> size)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;size;i+=<span class="number">4</span>)</span><br><span class="line">  &#123;</span><br><span class="line">    res[i]=a[i]+b[i];</span><br><span class="line">    res[i+<span class="number">1</span>]=a[i+<span class="number">1</span>]+b[i+<span class="number">1</span>];</span><br><span class="line">    res[i+<span class="number">2</span>]=a[i+<span class="number">2</span>]+b[i+<span class="number">2</span>];</span><br><span class="line">    res[i+<span class="number">3</span>]=a[i+<span class="number">3</span>]+b[i+<span class="number">3</span>];</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">sumArraysGPU</span><span class="params">(<span class="keyword">float</span>*a,<span class="keyword">float</span>*b,<span class="keyword">float</span>*res)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">int</span> i=blockIdx.x*blockDim.x+threadIdx.x;</span><br><span class="line">  res[i]=a[i]+b[i];</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc,<span class="keyword">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">int</span> dev = <span class="number">0</span>;</span><br><span class="line">  cudaSetDevice(dev);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span> nElem=<span class="number">1</span>&lt;&lt;<span class="number">14</span>;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">"Vector size:%d\n"</span>,nElem);</span><br><span class="line">  <span class="keyword">int</span> nByte=<span class="keyword">sizeof</span>(<span class="keyword">float</span>)*nElem;</span><br><span class="line">  <span class="keyword">float</span> *a_h=(<span class="keyword">float</span>*)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="keyword">float</span> *b_h=(<span class="keyword">float</span>*)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="keyword">float</span> *res_h=(<span class="keyword">float</span>*)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="keyword">float</span> *res_from_gpu_h=(<span class="keyword">float</span>*)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="built_in">memset</span>(res_h,<span class="number">0</span>,nByte);</span><br><span class="line">  <span class="built_in">memset</span>(res_from_gpu_h,<span class="number">0</span>,nByte);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">float</span> *a_d,*b_d,*res_d;</span><br><span class="line">  <span class="comment">// pine memory malloc</span></span><br><span class="line">  CHECK(cudaMallocHost((<span class="keyword">float</span>**)&amp;a_d,nByte));</span><br><span class="line">  CHECK(cudaMallocHost((<span class="keyword">float</span>**)&amp;b_d,nByte));</span><br><span class="line">  CHECK(cudaMallocHost((<span class="keyword">float</span>**)&amp;res_d,nByte));</span><br><span class="line"></span><br><span class="line">  initialData(a_h,nElem);</span><br><span class="line">  initialData(b_h,nElem);</span><br><span class="line"></span><br><span class="line">  CHECK(cudaMemcpy(a_d,a_h,nByte,cudaMemcpyHostToDevice));</span><br><span class="line">  CHECK(cudaMemcpy(b_d,b_h,nByte,cudaMemcpyHostToDevice));</span><br><span class="line"></span><br><span class="line">  <span class="function">dim3 <span class="title">block</span><span class="params">(<span class="number">1024</span>)</span></span>;</span><br><span class="line">  <span class="function">dim3 <span class="title">grid</span><span class="params">(nElem/block.x)</span></span>;</span><br><span class="line">  sumArraysGPU&lt;&lt;&lt;grid,block&gt;&gt;&gt;(a_d,b_d,res_d);</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">"Execution configuration&lt;&lt;&lt;%d,%d&gt;&gt;&gt;\n"</span>,grid.x,block.x);</span><br><span class="line"></span><br><span class="line">  CHECK(cudaMemcpy(res_from_gpu_h,res_d,nByte,cudaMemcpyDeviceToHost));</span><br><span class="line">  sumArrays(a_h,b_h,res_h,nElem);</span><br><span class="line"></span><br><span class="line">  checkResult(res_h,res_from_gpu_h,nElem);</span><br><span class="line">  cudaFreeHost(a_d);</span><br><span class="line">  cudaFreeHost(b_d);</span><br><span class="line">  cudaFreeHost(res_d);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">free</span>(a_h);</span><br><span class="line">  <span class="built_in">free</span>(b_h);</span><br><span class="line">  <span class="built_in">free</span>(res_h);</span><br><span class="line">  <span class="built_in">free</span>(res_from_gpu_h);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>注意这个核函数将会被本篇所有程序使用，今天的关键在于主机分配内存部分，所以核函数就选个最简单的。大家看看效率就好。<br>使用<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvprof ./pine_memory</span><br></pre></td></tr></table></figure></p>
<p>如果提示错误：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Error: CUDA profiling error.</span><br></pre></td></tr></table></figure></p>
<p>可以改用root权限执行，这时候又发现root没有nvprof程序，所以如图一样，用完整路径执行就好，或者添加到你的path里面。<br>结果如下：<br><img src="https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/CUDA-F-4-2-内存管理/res1.png" alt="res1"></p>
<p>作为对比，我们改写了代码库中第三个里的参数，使用常规内存拷贝方法，得到的时间如下：</p>
<p><img src="https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/CUDA-F-4-2-内存管理/res2.png" alt="res2"></p>
<p>这个结果有点尴尬，固定内存的指标显示的是HtoH，也就是主机到主机的内存拷贝，而常规拷贝显示了HtoD。主机到设备但是看memcpy的速度能看出固定内存耗时确实少一些30:42。<br>同时也能看到cudaHostAlloc和cudaMalloc的时间接近，当数据增大的时候，这个就有区别了，cudaHostAlloc会慢很多。<br>结论：<br>固定内存的释放和分配成本比可分页内存要高很多，但是传输速度更快，所以对于大规模数据，固定内存效率更高。<br>尽量使用流来使内存传输和计算之间同时进行，第六章详细介绍这部分。</p>
<h2 id="零拷贝内存"><a href="#零拷贝内存" class="headerlink" title="零拷贝内存"></a>零拷贝内存</h2><p>截止到目前，我们所接触到的内存知识的基础都是：主机直接不能访问设备内存，设备不能直接访问主机内存。对于早期设备，这是肯定的，但是后来，一个例外出现了——零拷贝内存。<br>GPU线程可以直接访问零拷贝内存，这部分内存在主机内存里面，CUDA核函数使用零拷贝内存有以下几种情况：</p>
<ul>
<li>当设备内存不足的时候可以利用主机内存</li>
<li>避免主机和设备之间的显式内存传输</li>
<li>提高PCIe传输率</li>
</ul>
<p>前面我们讲，注意线程之间的内存竞争，因为他们可以同时访问同一个内存地址，现在设备和主机可以同时访问同一个设备地址了，所以，我们要注意主机和设备的内存竞争——当使用零拷贝内存的时候。<br>零拷贝内存是固定内存，不可分页。可以通过以下函数创建零拷贝内存：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaHostAlloc</span><span class="params">(<span class="keyword">void</span> ** pHost,<span class="keyword">size_t</span> count,<span class="keyword">unsigned</span> <span class="keyword">int</span> flags)</span></span></span><br></pre></td></tr></table></figure></p>
<p>最后一个标志参数，可以选择以下值：</p>
<ul>
<li>cudaHostAllocDefalt</li>
<li>cudaHostAllocPortable</li>
<li>cudaHostAllocWriteCombined</li>
<li>cudaHostAllocMapped<br>cudaHostAllocDefalt和cudaMallocHost函数一致，cudaHostAllocPortable函数返回能被所有CUDA上下文使用的固定内存，cudaHostAllocWriteCombined返回写结合内存，在某些设备上这种内存传输效率更高。cudaHostAllocMapped产生零拷贝内存。<br>注意，零拷贝内存虽然不需要显式的传递到设备上，但是设备还不能通过pHost直接访问对应的内存地址，设备需要访问主机上的零拷贝内存，需要先获得另一个地址，这个地址帮助设备访问到主机对应的内存，方法是：<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaHostGetDevicePointer</span><span class="params">(<span class="keyword">void</span> ** pDevice,<span class="keyword">void</span> * pHost,<span class="keyword">unsigned</span> flags)</span></span>;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>pDevice就是设备上访问主机零拷贝内存的指针了！<br>此处flag必须设置为0，具体内容后面有介绍。<br>零拷贝内存可以当做比设备主存储器更慢的一个设备。<br>频繁的读写，零拷贝内存效率极低，这个非常容易理解，因为每次都要经过PCIe，千军万马堵在独木桥上，速度肯定慢，要是再有人来来回回走，那就更要命了。我们下面进行一个小实验，数组加法，改编自前面的代码，然后我们看看效果：<br>主函数代码，核函数如上节代码：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc,<span class="keyword">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">int</span> dev = <span class="number">0</span>;</span><br><span class="line">  cudaSetDevice(dev);</span><br><span class="line">  <span class="keyword">int</span> power=<span class="number">10</span>;</span><br><span class="line">  <span class="keyword">if</span>(argc&gt;=<span class="number">2</span>)</span><br><span class="line">    power=atoi(argv[<span class="number">1</span>]);</span><br><span class="line">  <span class="keyword">int</span> nElem=<span class="number">1</span>&lt;&lt;power;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">"Vector size:%d\n"</span>,nElem);</span><br><span class="line">  <span class="keyword">int</span> nByte=<span class="keyword">sizeof</span>(<span class="keyword">float</span>)*nElem;</span><br><span class="line">  <span class="keyword">float</span> *res_from_gpu_h=(<span class="keyword">float</span>*)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="keyword">float</span> *res_h=(<span class="keyword">float</span>*)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="built_in">memset</span>(res_h,<span class="number">0</span>,nByte);</span><br><span class="line">  <span class="built_in">memset</span>(res_from_gpu_h,<span class="number">0</span>,nByte);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">float</span> *a_host,*b_host,*res_d;</span><br><span class="line">  <span class="keyword">double</span> iStart,iElaps;</span><br><span class="line">  <span class="function">dim3 <span class="title">block</span><span class="params">(<span class="number">1024</span>)</span></span>;</span><br><span class="line">  <span class="function">dim3 <span class="title">grid</span><span class="params">(nElem/block.x)</span></span>;</span><br><span class="line">  res_from_gpu_h=(<span class="keyword">float</span>*)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="keyword">float</span> *a_dev,*b_dev;</span><br><span class="line">  CHECK(cudaHostAlloc((<span class="keyword">float</span>**)&amp;a_host,nByte,cudaHostAllocMapped));</span><br><span class="line">  CHECK(cudaHostAlloc((<span class="keyword">float</span>**)&amp;b_host,nByte,cudaHostAllocMapped));</span><br><span class="line">  CHECK(cudaMalloc((<span class="keyword">float</span>**)&amp;res_d,nByte));</span><br><span class="line">  initialData(a_host,nElem);</span><br><span class="line">  initialData(b_host,nElem);</span><br><span class="line"></span><br><span class="line"> <span class="comment">//=============================================================//</span></span><br><span class="line">  iStart = cpuSecond();</span><br><span class="line">  CHECK(cudaHostGetDevicePointer((<span class="keyword">void</span>**)&amp;a_dev,(<span class="keyword">void</span>*) a_host,<span class="number">0</span>));</span><br><span class="line">  CHECK(cudaHostGetDevicePointer((<span class="keyword">void</span>**)&amp;b_dev,(<span class="keyword">void</span>*) b_host,<span class="number">0</span>));</span><br><span class="line">  sumArraysGPU&lt;&lt;&lt;grid,block&gt;&gt;&gt;(a_dev,b_dev,res_d);</span><br><span class="line">  CHECK(cudaMemcpy(res_from_gpu_h,res_d,nByte,cudaMemcpyDeviceToHost));</span><br><span class="line">  iElaps = cpuSecond() - iStart;</span><br><span class="line"> <span class="comment">//=============================================================//</span></span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">"zero copy memory elapsed %lf ms \n"</span>, iElaps);</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">"Execution configuration&lt;&lt;&lt;%d,%d&gt;&gt;&gt;\n"</span>,grid.x,block.x);</span><br><span class="line"><span class="comment">//-----------------------normal memory---------------------------</span></span><br><span class="line">  <span class="keyword">float</span> *a_h_n=(<span class="keyword">float</span>*)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="keyword">float</span> *b_h_n=(<span class="keyword">float</span>*)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="keyword">float</span> *res_h_n=(<span class="keyword">float</span>*)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="keyword">float</span> *res_from_gpu_h_n=(<span class="keyword">float</span>*)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="built_in">memset</span>(res_h_n,<span class="number">0</span>,nByte);</span><br><span class="line">  <span class="built_in">memset</span>(res_from_gpu_h_n,<span class="number">0</span>,nByte);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">float</span> *a_d_n,*b_d_n,*res_d_n;</span><br><span class="line">  CHECK(cudaMalloc((<span class="keyword">float</span>**)&amp;a_d_n,nByte));</span><br><span class="line">  CHECK(cudaMalloc((<span class="keyword">float</span>**)&amp;b_d_n,nByte));</span><br><span class="line">  CHECK(cudaMalloc((<span class="keyword">float</span>**)&amp;res_d_n,nByte));</span><br><span class="line"></span><br><span class="line">  initialData(a_h_n,nElem);</span><br><span class="line">  initialData(b_h_n,nElem);</span><br><span class="line"><span class="comment">//=============================================================//</span></span><br><span class="line">  iStart = cpuSecond();</span><br><span class="line">  CHECK(cudaMemcpy(a_d_n,a_h_n,nByte,cudaMemcpyHostToDevice));</span><br><span class="line">  CHECK(cudaMemcpy(b_d_n,b_h_n,nByte,cudaMemcpyHostToDevice));</span><br><span class="line">  sumArraysGPU&lt;&lt;&lt;grid,block&gt;&gt;&gt;(a_d_n,b_d_n,res_d_n);</span><br><span class="line">  CHECK(cudaMemcpy(res_from_gpu_h,res_d,nByte,cudaMemcpyDeviceToHost));</span><br><span class="line">  iElaps = cpuSecond() - iStart;</span><br><span class="line"><span class="comment">//=============================================================//</span></span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">"device memory elapsed %lf ms \n"</span>, iElaps);</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">"Execution configuration&lt;&lt;&lt;%d,%d&gt;&gt;&gt;\n"</span>,grid.x,block.x);</span><br><span class="line"><span class="comment">//--------------------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line">  sumArrays(a_host,b_host,res_h,nElem);</span><br><span class="line">  checkResult(res_h,res_from_gpu_h,nElem);</span><br><span class="line"></span><br><span class="line">  cudaFreeHost(a_host);</span><br><span class="line">  cudaFreeHost(b_host);</span><br><span class="line">  cudaFree(res_d);</span><br><span class="line">  <span class="built_in">free</span>(res_h);</span><br><span class="line">  <span class="built_in">free</span>(res_from_gpu_h);</span><br><span class="line"></span><br><span class="line">  cudaFree(a_d_n);</span><br><span class="line">  cudaFree(b_d_n);</span><br><span class="line">  cudaFree(res_d_n);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">free</span>(a_h_n);</span><br><span class="line">  <span class="built_in">free</span>(b_h_n);</span><br><span class="line">  <span class="built_in">free</span>(res_h_n);</span><br><span class="line">  <span class="built_in">free</span>(res_from_gpu_h_n);</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>结果：<br><img src="https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/CUDA-F-4-2-内存管理/res3.png" alt="res3"></p>
<p>我们把结果写在一个表里面：</p>
<table>
<thead>
<tr>
<th style="text-align:center">数据规模n( $2^n$ )</th>
<th style="text-align:center">常规内存（us）</th>
<th style="text-align:center">零拷贝内存（us）</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">10</td>
<td style="text-align:center">2.5</td>
<td style="text-align:center">3.0</td>
</tr>
<tr>
<td style="text-align:center">12</td>
<td style="text-align:center">3.0</td>
<td style="text-align:center">4.1</td>
</tr>
<tr>
<td style="text-align:center">14</td>
<td style="text-align:center">7.8</td>
<td style="text-align:center">8.6</td>
</tr>
<tr>
<td style="text-align:center">16</td>
<td style="text-align:center">23.1</td>
<td style="text-align:center">25.8</td>
</tr>
<tr>
<td style="text-align:center">18</td>
<td style="text-align:center">86.5</td>
<td style="text-align:center">98.2</td>
</tr>
<tr>
<td style="text-align:center">20</td>
<td style="text-align:center">290.9</td>
<td style="text-align:center">310.5</td>
</tr>
</tbody>
</table>
<p>这是通过观察运行时间得到的，当然也可以通过我们上面的nvprof得到内核执行时间：</p>
<table>
<thead>
<tr>
<th style="text-align:center">数据规模n( $2^n$ )</th>
<th style="text-align:center">常规内存（us）</th>
<th style="text-align:center">零拷贝内存（us）</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">10</td>
<td style="text-align:center">1.088</td>
<td style="text-align:center">4.257</td>
</tr>
<tr>
<td style="text-align:center">12</td>
<td style="text-align:center">1.056</td>
<td style="text-align:center">8.00</td>
</tr>
<tr>
<td style="text-align:center">14</td>
<td style="text-align:center">1，920</td>
<td style="text-align:center">24.578</td>
</tr>
<tr>
<td style="text-align:center">16</td>
<td style="text-align:center">4.544</td>
<td style="text-align:center">86.63</td>
</tr>
</tbody>
</table>
<p>直接上数据，图太多，没办法贴了，但是这种比较方法有点问题，因为零拷贝内存在执行内核的时候相当于还执行了内存传输工作，所以我觉得应该把内存传输也加上，那样看速度就基本差不多了，但是如果常规内存完成传输后可以重复利用，那又是另一回事了。</p>
<p>但是零拷贝内存也有例外的时候，比如当CPU和GPU继承在一起的时候，你别不信，我手里就有一个，Nvidia的平板，ARM+GPU的架构，这时候，他们的物理内存公用的，这时候零拷贝内存，效果相当不错。但是如果离散架构，主机和设备之间通过PCIe连接，那么零拷贝内存将会非常耗时。</p>
<h2 id="统一虚拟寻址"><a href="#统一虚拟寻址" class="headerlink" title="统一虚拟寻址"></a>统一虚拟寻址</h2><p>设备架构2.0以后，Nvida又有新创意，他们搞了一套称为同一寻址方式（UVA）的内存机制，这样，设备内存和主机内存被映射到同一虚拟内存地址中。如图<br><img src="https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/CUDA-F-4-2-内存管理/4-5.png" alt="4-5"></p>
<p>UVA之前，我们要管理所有的设备和主机内存，尤其是他们的指针，零拷贝内存尤其麻烦，很容易乱的，写过c的人都知道，弄个五六个指针在哪其中一部分还指向相同的数据不同的地址的，十几行之后必然会混乱。有了UVA再也不用怕，一个人一个名，走到哪里都能用，通过UVA，cudaHostAlloc函数分配的固定主机内存具有相同的主机和设备地址，可以直接将返回的地址传递给核函数。<br>前面的零拷贝内存，可以知道以下几个方面：</p>
<ul>
<li>分配映射的固定主机内存</li>
<li>使用CUDA运行时函数获取映射到固定内存的设备指针</li>
<li>将设备指针传递给核函数</li>
</ul>
<p>有了UVA，可以不用上面的那个获得设备上访问零拷贝内存的函数了：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaHostGetDevicePointer</span><span class="params">(<span class="keyword">void</span> ** pDevice,<span class="keyword">void</span> * pHost,<span class="keyword">unsigned</span> flags)</span></span>;</span><br></pre></td></tr></table></figure></p>
<p>UVA来了以后，此函数基本失业了。<br>试验，代码：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="keyword">float</span> *a_host,*b_host,*res_d;</span><br><span class="line">  CHECK(cudaHostAlloc((<span class="keyword">float</span>**)&amp;a_host,nByte,cudaHostAllocMapped));</span><br><span class="line">  CHECK(cudaHostAlloc((<span class="keyword">float</span>**)&amp;b_host,nByte,cudaHostAllocMapped));</span><br><span class="line">  CHECK(cudaMalloc((<span class="keyword">float</span>**)&amp;res_d,nByte));</span><br><span class="line">  res_from_gpu_h=(<span class="keyword">float</span>*)<span class="built_in">malloc</span>(nByte);</span><br><span class="line"></span><br><span class="line">  initialData(a_host,nElem);</span><br><span class="line">  initialData(b_host,nElem);</span><br><span class="line"></span><br><span class="line">  <span class="function">dim3 <span class="title">block</span><span class="params">(<span class="number">1024</span>)</span></span>;</span><br><span class="line">  <span class="function">dim3 <span class="title">grid</span><span class="params">(nElem/block.x)</span></span>;</span><br><span class="line">  sumArraysGPU&lt;&lt;&lt;grid,block&gt;&gt;&gt;(a_host,b_host,res_d);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>UVA代码主要就是差个获取指针，UVA可以直接使用主机端的地址。</p>
<p>结果：</p>
<p><img src="https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/CUDA-F-4-2-内存管理/res4.png" alt="res4"></p>
<h2 id="统一内存寻址"><a href="#统一内存寻址" class="headerlink" title="统一内存寻址"></a>统一内存寻址</h2><p>Nvidia的同志们还是不停的搞出新花样，CUDA6.0的时候又来了个统一内存寻址，注意不是同一虚拟寻址，提出的目的也是为了简化内存管理（我感觉是越简化越困难，因为套路多了）统一内存中创建一个托管内存池（CPU上有，GPU上也有），内存池中已分配的空间可以通过相同的指针直接被CPU和GPU访问，底层系统在统一的内存空间中自动的进行设备和主机间的传输。数据传输对应用是透明的，大大简化了代码。<br>就是搞个内存池，这部分内存用一个指针同时表示主机和设备内存地址，依赖于UVA但是是完全不同的技术。<br>统一内存寻址提供了一个“指针到数据”的编程模型，概念上类似于零拷贝，但是零拷贝内存的分配是在主机上完成的，而且需要互相传输，但是统一寻址不同。<br>托管内存是指底层系统自动分配的统一内存，未托管内存就是我们自己分配的内存，这时候对于核函数，可以传递给他两种类型的内存，已托管和未托管内存，可以同时传递。<br>托管内存可以是静态的，也可以是动态的，添加 <strong>managed</strong> 关键字修饰托管内存变量。静态声明的托管内存作用域是文件，这一点可以注意一下。<br>托管内存分配方式：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaMallocManaged</span><span class="params">(<span class="keyword">void</span> ** devPtr,<span class="keyword">size_t</span> size,<span class="keyword">unsigned</span> <span class="keyword">int</span> flags=<span class="number">0</span>)</span></span></span><br></pre></td></tr></table></figure></p>
<p>这个函数和前面函数结构一致，注意函数名就好了，参数就不解释了，很明显了已经。<br>CUDA6.0中设备代码不能调用cudaMallocManaged，只能主机调用，所有托管内存必须在主机代码上动态声明，或者全局静态声明<br>。后面4.5 我们会详细的研究统一内存寻址。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文介绍了CUDA内存管理中几种技术，注意区别他们的相同点和不同点。<br>代码库：<a href="https://github.com/tony-tan" target="_blank" rel="noopener">https://github.com/tony-tan</a></p>
<p>原文地址1：<a href="https://www.face2ai.com/CUDA-F-4-2-内存管理">https://www.face2ai.com/CUDA-F-4-2-内存管理</a>转载请标明出处</p>

      
    </div>









    
    
    



    
    
      

<div class="post-body">
<div class="note info">
  
  <rl class="recommended">相关文章</rl>
  <ul class="popular-posts">
  
    <li class="popular-posts-item">
      
      
      <div class="popular-posts-title"><a href="/Julia-Lang-1-Install/" rel="bookmark">【Julia】Julia环境搭建（Mac,Windows,Linux）</a></div>
      
    </li>
  
    <li class="popular-posts-item">
      
      
      <div class="popular-posts-title"><a href="/CUDA-F-6-5-流回调/" rel="bookmark">【CUDA 基础】6.5 流回调</a></div>
      
    </li>
  
  </ul>
</div>
</div>


    


    <footer class="post-footer">
      

      
      
        <div class="post-widgets">
        
          <div class="wp_rating" >
          <div style="color: rgba(0, 0, 0, 1); font-size:16px; letter-spacing:3px">(&gt;壮士！看完给洒家个五星，你看行不行！&lt;)</div>
            <div id="wpac-rating"></div>
          </div>
        

        
        
        </div>
      
      

      

      
        <div>
          <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>关注公众号或添加博主微信，反馈问题，获取资讯（暗号:face2ai）</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>联系博主</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/weixin.png" alt="谭升 博主微信"/>
        <p>博主微信</p>
      </div>
    

    
      <div id="alipay" style="display: inline-block">
        <img id="alipay_qr" src="/images/alipay.jpg" alt="谭升 博客公众号"/>
        <p>博客公众号</p>
      </div>
    

    

  </div>
</div>

        </div>
      



      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/Math-Statistics-2-0-De-Moivre-Binomial-Distribution/" rel="next" title="【数理统计学简史】2.0 狄莫弗的二项概率逼近">
                <i class="fa fa-chevron-left"></i> 【数理统计学简史】2.0 狄莫弗的二项概率逼近
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/Math-Statistics-2-1-De-Moivre-Motive/" rel="prev" title="【数理统计学简史】2.1 狄莫弗研究的动因">
                【数理统计学简史】2.1 狄莫弗研究的动因 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <!-- footer -->
  <ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-1194454329688573"
     data-ad-slot="2491973880"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
  <script>
  (adsbygoogle = window.adsbygoogle || []).push({});
  </script>



          </div>
          

  
  


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
      <div id="sidebar-dimmer"></div>
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.gif"
                alt="谭升" />
            
              <p class="site-author-name" itemprop="name">谭升</p>
              <p class="site-description motion-element" itemprop="description">强化学习 机器学习 人工智能</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives">
                
                    <span class="site-state-item-count">264</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">20</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">8</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  <a href="https://github.com/tony-tan" target="_blank" title="GitHub" rel="external nofollow"><i class="fa fa-fw fa-github"></i>GitHub</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="mailto:tony.face2ai@gmail.com" target="_blank" title="E-Mail" rel="external nofollow"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://twitter.com/Tony_Face2AI" target="_blank" title="Twitter" rel="external nofollow"><i class="fa fa-fw fa-twitter"></i>Twitter</a>
                  
                </span>
              
            </div>
          

          
          

          
          

          
              <div class="links-of-blogroll motion-element links-of-blogroll-block"  >
                <div class="links-of-blogroll-title" align="left">
                  <!-- modify icon to fire by szw -->
                  <i class="fa fa-history fa-" aria-hidden="true"></i>
                  近期文章
                </div>
                <ul class="links-of-blogroll-list" align="left">
                  
                  
                    <li>
                      <a href="/Julia-Lang-1-Install/" title="【Julia】Julia环境搭建（Mac,Windows,Linux）" target="_blank">【Julia】Julia环境搭建（Mac,Windows,Linux）</a>
                    </li>
                  
                    <li>
                      <a href="/RL-RSAB-1-5-An-Extended-Example/" title="【强化学习】 1.5 强化学习的一个扩展举例" target="_blank">【强化学习】 1.5 强化学习的一个扩展举例</a>
                    </li>
                  
                    <li>
                      <a href="/RL-RSAB-1-4-1-Connection-to-Optimization-Method/" title="【强化学习】 1.4.1 强化学习与优化方法" target="_blank">【强化学习】 1.4.1 强化学习与优化方法</a>
                    </li>
                  
                    <li>
                      <a href="/Julia-Lang-0-Introduction/" title="【Julia】Julia编程语言介绍" target="_blank">【Julia】Julia编程语言介绍</a>
                    </li>
                  
                    <li>
                      <a href="/ITILA-Introduction-to-This-Series/" title="【信息论、推理与学习算法】本系列博客介绍" target="_blank">【信息论、推理与学习算法】本系列博客介绍</a>
                    </li>
                  
                </ul>
          


          
            
          
          <div id="days"></div>
<script>
function show_date_time(){
window.setTimeout("show_date_time()", 1000);
BirthDay=new Date(" 11/27/2013 00:00:00");
today=new Date();
timeold=(today.getTime()-BirthDay.getTime());
sectimeold=timeold/1000
secondsold=Math.floor(sectimeold);
msPerDay=24*60*60*1000
e_daysold=timeold/msPerDay
daysold=Math.floor(e_daysold);
e_hrsold=(e_daysold-daysold)*24;
hrsold=setzero(Math.floor(e_hrsold));
e_minsold=(e_hrsold-hrsold)*60;
minsold=setzero(Math.floor((e_hrsold-hrsold)*60));
seconds=setzero(Math.floor((e_minsold-minsold)*60));
document.getElementById('days').innerHTML="第一篇博客至今已创作"+daysold+"天"+hrsold+"小时"+minsold+"分"+seconds+"秒";
}
function setzero(i){
if (i<10)
{i="0" + i};
return i;
}
show_date_time();
</script>

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#内存管理"><span class="nav-number">1.</span> <span class="nav-text">内存管理</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#内存分配和释放"><span class="nav-number">1.1.</span> <span class="nav-text">内存分配和释放</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#内存传输"><span class="nav-number">1.2.</span> <span class="nav-text">内存传输</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#固定内存"><span class="nav-number">1.3.</span> <span class="nav-text">固定内存</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#零拷贝内存"><span class="nav-number">1.4.</span> <span class="nav-text">零拷贝内存</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#统一虚拟寻址"><span class="nav-number">1.5.</span> <span class="nav-text">统一虚拟寻址</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#统一内存寻址"><span class="nav-number">1.6.</span> <span class="nav-text">统一内存寻址</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#总结"><span class="nav-number">1.7.</span> <span class="nav-text">总结</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love" id="animate">
    <i class="fa fa-"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">谭升</span>

  

  
</div>


  










        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv" title="总访客量">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
    </span>
  

  
    <span class="site-pv" title="总访问量">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
    </span>
  
</div>









        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
          <span id="scrollpercent"><span>0</span>%</span>
        
      </div>
    

    

    

    
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.4.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.4.0"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=6.4.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=6.4.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.4.0"></script>



  



  










  





  

  
  <script>
    
    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();

      Counter('get', '/classes/Counter', { where: JSON.stringify({ url }) })
        .done(function ({ results }) {
          if (results.length > 0) {
            var counter = results[0];
            
            Counter('put', `/classes/Counter/${counter.objectId}`, JSON.stringify({ time: { "__op":"Increment", "amount":1 } }))
            
            .done(function () {
                  var $element = $(document.getElementById(url));
                  $element.find('.leancloud-visitors-count').text(counter.time + 1);
            })
            
            .fail(function ({ responseJSON }) {
                console.log('Failed to save Visitor num, with error message: ' + responseJSON.error);
            })
          } else {
            
              Counter('post', '/classes/Counter', JSON.stringify({ title: title, url: url, time: 1}))
                .done(function () {
                  var $element = $(document.getElementById(url));
                  $element.find('.leancloud-visitors-count').text(1);
                })
                .fail(function () {
                  console.log('Failed to create');
                });
            
          }
        })
      .fail(function ({ responseJSON }) {
        console.log('LeanCloud Counter Error:' + responseJSON.code + " " + responseJSON.error);
      });
    }
    

    $(function() {
      $.get('https://app-router.leancloud.cn/2/route?appId=' + "99mvzicXt5ABYCU385fe5J1B-gzGzoHsz")
        .done(function ({ api_server }) {
          var Counter = function (method, url, data) {
            return $.ajax({
              method: method,
              url: `https://${api_server}/1.1${url}`,
              headers: {
                'X-LC-Id': "99mvzicXt5ABYCU385fe5J1B-gzGzoHsz",
                'X-LC-Key': "JcquIPFohejA5x6cn1xIVomJ",
                'Content-Type': 'application/json',
              },
              data: data,
            });
          };
          
          addCount(Counter);
          
        })
    });
  </script>



  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  

  
    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      TeX: {equationNumbers: { autoNumber: "AMS" }}
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    
  


  
  
  <script type="text/javascript">
  wpac_init = window.wpac_init || [];
  wpac_init.push({widget: 'Rating', id: 14057,
    el: 'wpac-rating',
    color: 'fc6423'
  });
  (function() {
    if ('WIDGETPACK_LOADED' in window) return;
    WIDGETPACK_LOADED = true;
    var mc = document.createElement('script');
    mc.type = 'text/javascript';
    mc.async = true;
    mc.src = '//embed.widgetpack.com/widget.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(mc, s.nextSibling);
  })();
  </script>


  

  

  

  

  

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-backstretch/2.0.4/jquery.backstretch.min.js"></script>
  <script>
    $("body").backstretch("https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/background.jpg")
  </script>
</body>
</html>
