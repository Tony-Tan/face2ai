<!DOCTYPE html><html class="theme-next pisces use-motion" lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=6.4.0" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png?v=6.4.0"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32.png?v=6.4.0"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16.png?v=6.4.0"><link rel="mask-icon" href="/images/logo.png?v=6.4.0" color="#222"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Pisces",version:"6.4.0",sidebar:{position:"left",display:"post",offset:12,b2t:!1,scrollpercent:!0,onmobile:!1},fancybox:!1,fastclick:!1,lazyload:!1,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><meta name="description" content="Abstract: 本篇有点长，内容及其丰富，包括线性变换的矩阵形式以及相关例子（导数和积分），然后详细的讲解了下怎么构造矩阵，也就是矩阵的来源，之后是矩阵相乘的原理，基的变换，最后一波大应用，小波变换和离散傅里叶变换Keywords: Matrix,Matrix for the Derivate,Matrix for the Integral,Construction of the Matrix"><meta name="keywords" content="Matrix,Matrix for the Derivate,Matrix for the Integral,Construction of the Matrix,$AB$ Match $TS$,Multiplication,Change of Basis Matrix,Wavelet Transform,Fourier Transform(DFT)"><meta property="og:type" content="article"><meta property="og:title" content="【线性代数】7-2:线性变化的矩阵(The Matrix of a Linear Transformation)"><meta property="og:url" content="http://www.face2ai.com/Math-Linear-Algebra-Chapter-7-2/index.html"><meta property="og:site_name" content="谭升的博客"><meta property="og:description" content="Abstract: 本篇有点长，内容及其丰富，包括线性变换的矩阵形式以及相关例子（导数和积分），然后详细的讲解了下怎么构造矩阵，也就是矩阵的来源，之后是矩阵相乘的原理，基的变换，最后一波大应用，小波变换和离散傅里叶变换Keywords: Matrix,Matrix for the Derivate,Matrix for the Integral,Construction of the Matrix"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/Math-Linear-Algebra-Chapter-7-2/compression.png"><meta property="og:updated_time" content="2018-07-12T15:45:31.655Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="【线性代数】7-2:线性变化的矩阵(The Matrix of a Linear Transformation)"><meta name="twitter:description" content="Abstract: 本篇有点长，内容及其丰富，包括线性变换的矩阵形式以及相关例子（导数和积分），然后详细的讲解了下怎么构造矩阵，也就是矩阵的来源，之后是矩阵相乘的原理，基的变换，最后一波大应用，小波变换和离散傅里叶变换Keywords: Matrix,Matrix for the Derivate,Matrix for the Integral,Construction of the Matrix"><meta name="twitter:image" content="https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/Math-Linear-Algebra-Chapter-7-2/compression.png"><link rel="canonical" href="http://www.face2ai.com/Math-Linear-Algebra-Chapter-7-2/"><script type="text/javascript" id="page.configurations">CONFIG.page={sidebar:""}</script><title>【线性代数】7-2:线性变化的矩阵(The Matrix of a Linear Transformation) | 谭升的博客</title><script async src="https://www.googletagmanager.com/gtag/js?id=UA-105335860-3"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-105335860-3")</script><noscript><style type="text/css">.sidebar-inner,.use-motion .brand,.use-motion .collection-title,.use-motion .comments,.use-motion .menu-item,.use-motion .motion-element,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .logo,.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">谭升的博客</span> <span class="logo-line-after"><i></i></span></a></div><h1 class="site-subtitle" itemprop="description">Machine Learning & Computer Vision</h1></div><div class="site-nav-toggle"><button aria-label="切换导航栏"><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-首页"><a href="/" rel="section">首页</a></li><li class="menu-item menu-item-数学"><a href="/categories/Mathematic/" rel="section">数学</a></li><li class="menu-item menu-item-···-集合论"><a href="/categories/Mathematic/Set-Theory/" rel="section">··· 集合论</a></li><li class="menu-item menu-item-···-线性代数基础"><a href="/categories/Mathematic/Linear-Algebra/" rel="section">··· 线性代数基础</a></li><li class="menu-item menu-item-···-概率论基础"><a href="/categories/Mathematic/Probability/" rel="section">··· 概率论基础</a></li><li class="menu-item menu-item-···-数理统计学"><a href="/categories/Mathematic/Statistics/" rel="section">··· 数理统计学</a></li><li class="menu-item menu-item-···-数值分析"><a href="/categories/Mathematic/Numerical-Analysis/" rel="section">··· 数值分析</a></li><li class="menu-item menu-item-机器学习算法"><a href="/categories/Machine-Learning/" rel="section">机器学习算法</a></li><li class="menu-item menu-item-强化学习"><a href="/categories/Reinforcement-Learning/" rel="section">强化学习</a></li><li class="menu-item menu-item-深度学习算法"><a href="/categories/Deep-Learning/" rel="section">深度学习算法</a></li><li class="menu-item menu-item-数字图像处理"><a href="/categories/DIP/" rel="section">数字图像处理</a></li><li class="menu-item menu-item-30天自制操作系统"><a href="/categories/30天自制操作系统/" rel="section">30天自制操作系统</a></li><li class="menu-item menu-item-cuda"><a href="/categories/CUDA/" rel="section">CUDA</a></li><li class="menu-item menu-item-网络爬虫"><a href="/categories/Crawler/" rel="section">网络爬虫</a></li><li class="menu-item menu-item-乱七八糟"><a href="/categories/Other/" rel="section">乱七八糟</a></li></ul></nav><img src="https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/Wechat.jpeg" alt="wechat"> <a href="http://www.daily-ai.site"><img border="0" src="/images/daily-ai.site.jpg"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-1194454329688573" data-ad-slot="2491973880" data-ad-format="auto" data-full-width-responsive="true"></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></a></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-1194454329688573" data-ad-slot="9135658886" data-ad-format="auto" data-full-width-responsive="true"></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://www.face2ai.com/Math-Linear-Algebra-Chapter-7-2/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Tony"><meta itemprop="description" content="关注机器学习，深度学习，机器视觉，模式识别"><meta itemprop="image" content="/images/avatar.gif"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="谭升的博客"></span><header class="post-header"><h2 class="post-title" itemprop="name headline">【线性代数】7-2:线性变化的矩阵(The Matrix of a Linear Transformation)</h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2017-12-04 12:52:03" itemprop="dateCreated datePublished" datetime="2017-12-04T12:52:03+08:00">2017-12-04</time> </span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Mathematic/" itemprop="url" rel="index"><span itemprop="name">Mathematic</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Mathematic/Linear-Algebra/" itemprop="url" rel="index"><span itemprop="name">Linear Algebra</span></a></span> </span><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> 阅读次数： <span class="busuanzi-value" id="busuanzi_value_page_pv"></span></span></div></header><div class="post-body" itemprop="articleBody"><p><strong>Abstract:</strong> 本篇有点长，内容及其丰富，包括线性变换的矩阵形式以及相关例子（导数和积分），然后详细的讲解了下怎么构造矩阵，也就是矩阵的来源，之后是矩阵相乘的原理，基的变换，最后一波大应用，小波变换和离散傅里叶变换<br><strong>Keywords:</strong> Matrix,Matrix for the Derivate,Matrix for the Integral,Construction of the Matrix,$AB$ Match $TS$,Multiplication,Change of Basis Matrix,Wavelet Transform,Fourier Transform(DFT)</p><a id="more"></a><h2 id="开篇废话"><a href="#开篇废话" class="headerlink" title="开篇废话"></a>开篇废话</h2><p>今天没啥废话，感觉之前废话多就是总是对一些事有看法，现在一个是事少了，尽量躲开那些扯淡的人和扯淡的事，第二可能是习惯了，蓝老师说过人要过百形形色色（saisai三声），确实是这样，子非鱼焉知鱼之乐。<br>线性代数过了今天可能就剩下下一篇的一点剩下的基础理论了，从开始写到现在，已经三个月了，速度确实太慢了；而且没什么人看，但是我觉得我敢自称会线性代数了，当然考试的话可能还得不了几分，但起码我能说出来一些很关键的知识，下一步就是机器学习最关键也是我之前完全没学会的概率了，概率和数理统计对于机器学习可能更重要一些，所以后面的博客继续更新概率论，矩阵分析可能要提上日程了，但是目前不确定什么时候写。<br><strong><em>注意：下文中线性变换和线性组合是有区别的，请区分对待</em></strong></p><h2 id="The-Matrix-of-a-Linear-Transformation"><a href="#The-Matrix-of-a-Linear-Transformation" class="headerlink" title="The Matrix of a Linear Transformation"></a>The Matrix of a Linear Transformation</h2><p>如果我们不去回想第一张的矩阵乘法，矩阵向量相乘，我们只从上一篇的思路继续，当时我们假定线性变换$T$ 对$v_1 \in \Re^n$ 的变换结果是 $w_1 \in \Re^m$ ，如果$w_1 \neq v_1$，那么就是空间发生了变换，我们假定存在矩阵A满足这个变换，也就是 $T(v_1)=Av_1=w_1$ 那么矩阵规模是 $m\times n$ 的，等等，如果$v_1$ 所在的空间V和 $w_1$ 所在的空间W 已经确定知道，那么能确定矩阵$A$么？答案是不确定的，也就是说输入空间输出空间即便确定了，我们也不能肯定之间的对应关系，那么还需要什么条件呢？答案是空间的基向量，我们知道基向量可以确定出整个空间（子空间）但是已知空间，却可以对应无数组各种各样的基向量，所以同样的空间，不同的基应该对应着不同的线性变换矩阵$A$ 。<br>线性代数的另一个重要任务就是通过找到最完美的基来得到最完美的矩阵 $A$ 。<br>下面我们研究一下基，我们假设空间V有n个线性独立的向量组成的一组基<br>$\vec{v_1},\vec{v_2},\dots ,\vec{v_n}$<br>，那么空间内任一向量均可表示为 $\vec{v}=c_1\vec{v_1}+c_2\vec{v_2}+\dots +c_n\vec{v_n}$</p><blockquote><p>Key idea of this section：<br>Suppose we know $T(\vec{v_1}),\dots,T(\vec{v_n})$ for the basis vectors $v_1,\dots,v_n$<br>Then linearity produces $T(\vec{v})$ for every other input vector $v$</p></blockquote><p>翻译一下，也就是我们知道，空间中的任一向量都是通过基向量的线性组合出来的，经过线性变换$T$ 会得到新空间的一组向量然后进行线性组合就能得到结果：<br>$$<br>T(\vec{v})=T(c_1\vec{v_1}+c_2\vec{v_2}+\dots +c_n\vec{v_n})=c_1T(\vec{v_1})+c_2T(\vec{v_2})+\dots +c_nT(\vec{v_n})<br>$$<br>也就是线性组合的线性变换等于线性变换后的线性组合，由于上式的表示的是输入空间的任一向量，所以从输入空间到输出空间的映射就此完成，有点拗口但是看上面的公式一目了然，彪悍的逻辑，不需要解释。<br>这里会举一个🌰 ，这个🌰 很重要，虽然简单，但是值得拥有：</p><blockquote><p>eg1. suppose $v_1=(1,0),transforms\,T(v_1)=(2,3,4)$ and $v_2=(0,1),transforms\,T(v_2)=(5,5,5)$<br>If $T$ is linear from $\Re^2$ and $\Re^3$ then the “Standard matrix “ is 3 by 2,Those output $T(v_1)$ and T(v_2) go into the column $A=\begin{bmatrix}2&amp;5\\3&amp;5\\4&amp;5\end{bmatrix}$<br>so $v=(1,1)$ transformation $T(v)=\begin{bmatrix}2&amp;5\\3&amp;5\\4&amp;5\end{bmatrix}\begin{bmatrix}1\\1\end{bmatrix}=\begin{bmatrix}7\\8\\9\end{bmatrix}$</p></blockquote><p>这个例子的简单在于先给出v的基是标准基，也就是我们平时说坐标张嘴就来的坐标系下的，单位正交基，写成矩阵就是$I$ 的那组基，我们前面从来没说过这个事，也就是说默认都是他，当然我们现在要研究他了，所以必须强调，而且这个例子很轻松的就给出了矩阵 $A$ 也是因为这个标准基的特殊性质带来的方便，那么当我们把基变了以后会得到什么样的结果呢？<br>下面我们看另一个例子，这个🌰可以引出线性代数基本定理（Fundamental Theorem）</p><blockquote><p>eg2. 函数$1,x,x^2,x^3$ 的导数是$0,1,2x,3x^2$ 这些是求导这个线性变换$T$ 的四个因素，输入输出都是函数，更关键的因素是求导是线性的：<br>$$<br>\frac{d(cv+dw)}{dt}=c\frac{dv}{dt}+d\frac{dw}{dt}<br>$$<br>所以我们可以根据以上两个因素求得多项式$4+x+x^2+x^3$的导数<br>$$<br>\frac{d(4+x+x^2+x^3)}{dx}=1+2x+3x^2<br>$$</p></blockquote><p>这个例子厉害就厉害在了T，这个不在局限于乘加计算，很多计算都可以满足线性的要求，所以之前说求导是线性的，我是拒绝的，包括后面的积分老师说他是线性的，我也是拒绝的。继续分析这个例子，输入空间是个n维的向量，当然你也可以叫他函数，不过老爷子（prof. Strang）说我认为他是个向量，所以我们就按照向量来，如果我们观察输入输出维度我们发现，输出的维度都要小于输入维度，比如输入最高是4次，那么输出最高是3次，也就是说，输出是输入的一个subspace，少一维，例如输入空间$m=4$ 那么 输出空间是$n=3$ 所以根据🌰 1 或者前面基本公式，都能确定矩阵A是个$n\times m$的 输出空间$n=3$ 那么矩阵$rank(A)=3$ 维度是3<br>什么情况下输出是 $\vec{0}$ 呢，根据导数性质，我们知道常数的导数是0 ，也就是我们只能保留第一项（基为1的项可以有，其他$x,x^2,x^3,\dots$ 必须不存在）那么也就是说Nullspace应该是$V_{null}=(a,0,0,0,\dots)$ 维度是1<br>那么我们就能得出线性代数基本定理了:</p><blockquote><p>dimension(nullspace)+dimension(columnspace)=n</p></blockquote><p>or</p><blockquote><p>dimension of range(columnspace)+dimension of kernel(nullspace)=dimension of input inputspace</p></blockquote><p>接下来还是🌰 今天的例子怎么这么多呢，因为以前🌰 也多但是我试图用文字描述了例子中的关键点，但是这篇必须要有🌰，不然会非常抽象。</p><blockquote><p>eg3. 积分是求导的逆运算(微积分的基本定理Fundamental theorem)，我们先用符号 $T^{-1}$来表示积分计算，我们还没有说他是线性变换，所以我们继续我们的例子，我们有：<br>$$<br>\int_0^x1dx=x\\<br>\int_0^xxdx=\frac{1}{2}x^2\\<br>\int_0^xx^2dx=\frac{1}{3}x^3\\<br>\vdots<br>$$<br>这个是其中的一个因素，另一个重要因素，积分也是线性的，不信？<br>$$<br>\int_0^xc_1g(t)+c_2f(t)dt=<br>c_1\int_0^x g(t)dt+c_2\int_0^x f(t)dt<br>$$<br>信不信，不信去看calculus的教材吧，包您满意，接下来我们就进行上面🌰 2 的逆过程，已知$w=B+Cx+Dx^2$<br>那么$T^{-1}(w)=Bx+\frac{1}{2}Cx^2+\frac{1}{3}Dx^3+E$ 可见积分计算相当于把🌰 2中的W空间重新变换回了V空间，所以$T^{-1}$ 是个$4\times3$ 的矩阵</p></blockquote><p>例子一直观的通过一个代数计算过程告诉我们输入空间到输出空间线性变换过程跟一个矩阵有关系，跟基有关系，并且矩阵的尺寸和输入输出维度有关系，例子二和例子三则是告诉我们线性变换可以存在逆运算，下面要研究的就是求导和积分这两个变换的矩阵形式是什么样子的。</p><h2 id="Matrices-for-the-Derivative-and-Integral"><a href="#Matrices-for-the-Derivative-and-Integral" class="headerlink" title="Matrices for the Derivative and Integral"></a>Matrices for the Derivative and Integral</h2><p>上面我们从函数的角度研究了一下多项式的求导和积分，在上面老爷子一直强调他把多项式看成向量，也就是变量 $x^n,n=0,1,2,3\dots$ 作为基，分别分析了求导和积分后的dimension的变化和相互的关系，我们接下来研究的是如何把求导过程矩阵化，值得强调的是，我们从此开始只研究基，对于基前面的系数（上述求导和求积分的例子中多项式的系数）只需作为验证，我们对基进行变换，后带入系数验证结果，既有理论的严谨又有直观的感受。</p><ol><li>对于求导，我们从输入空间 $V=(1,x,x^2,x^3)$ 求导后输出向量的空间基是 $W=(1,x,x^2)$ 对应的是 $3\times 4$ 的矩阵</li></ol><p>$$<br>A=<br>\begin{bmatrix}<br>0&amp;1&amp;0&amp;0\\<br>0&amp;0&amp;2&amp;0\\<br>0&amp;0&amp;0&amp;3<br>\end{bmatrix}<br>$$<br>我们并不知道求A的方法或者一般过程，上面🌰 1 中通过标准基的线性变换得到的矩阵对于标准基适用，但在这里也不太好用，所以我们不知道A是怎么来的，那么我们来看看他是怎么没的，如果我们输入向量是$A+Bx+Cx^2+Dx^3$ 也就是向量 $v=(A,B,C,D)$ 线性变换 $T(v)$ 可以写成下面的方式:<br>$$<br>T(v)=Av=<br>\begin{bmatrix}<br>0&amp;1&amp;0&amp;0\\<br>0&amp;0&amp;2&amp;0\\<br>0&amp;0&amp;0&amp;3<br>\end{bmatrix}<br>\begin{bmatrix}A\\B\\C\\D\end{bmatrix}=<br>\begin{bmatrix}B\\2C\\3D\end{bmatrix}<br>$$<br>如果我们用微积分的方法直接求导呢：<br>$$<br>\frac{d(A+Bx+Cx^2+Dx^3)}{dx}=B+2Cx+3Dx^2<br>$$<br>可见与矩阵方法一致，证明了A的正确性。<br>与此同理，积分我们就不在详细叙述了，我们还是把积分称为 $T^{-1}$ 对应的矩阵 $A^{-1}$ 输入空间基 $(1,x,x^2)$ 输出空间基 $(1,x,x^2,x^3)$ 那么：<br>$$<br>A^{-1}=<br>\begin{bmatrix}<br>0&amp;0&amp;0\\<br>1&amp;0&amp;0\\<br>0&amp;\frac{1}{2}&amp;0\\<br>0&amp;0&amp;\frac{1}{3}<br>\end{bmatrix}<br>$$<br>对于输入$v=(B,C,D)$ 我们可以得到 $A^{-1}v=\begin{bmatrix}0\\B\\\frac{1}{2}C\\\frac{1}{3}D\end{bmatrix}$ 接下来我们要分析下，我们把这个线性变换矩阵称为$A^{-1}$ 的原因是因为他的线性变换与上面的求导是互为逆过程的，但是我们知道只有方阵有逆运算，这种长的矩阵不会有逆的，但是如果我们计算$AA^{-1}$ 会发现其结果是 $\begin{bmatrix}1&amp;0&amp;0\\0&amp;1&amp;0\\0&amp;0&amp;1\end{bmatrix}$ 这就很有灵性了，两个非方阵相乘结果是单位矩阵，但是按照逆矩阵的要求 $A^{-1}A$ 却等于 $\begin{bmatrix}0&amp;0&amp;0&amp;0\\0&amp;1&amp;0&amp;0\\0&amp;0&amp;1&amp;0\\0&amp;0&amp;0&amp;1\end{bmatrix}$ 不是单位矩阵，但是对角线上只有0和1。</p><p>从微积分的角度继续思考如果一个函数先求导再求积分(对应的矩阵乘法是$A^{-1}A$，问我为什么，如果输入是$v$ 的话，输出$w=A^{-1}Av$ A首先对v进行线性变换，所以应该从右往左算)，丢失的信息是常数项，因为求导后常数项是0，求积分后变成未知常数，那么也就是有一位是有损失的，所以对应的就是第一列的那个0就是损失的常数，相反的，先求积分再求导就不会有数据损失，也就是常数项也完好无损的回来了，所以$AA^{-1}=I$ 至此上面的整个变换过程从数字上和逻辑上是没有问题的。<br>那么对于非方阵如果存在$AA^{-1}=I$ 这种情况，我们称之为 one-side inverse，就是一边的逆，另一边不是逆。</p><p>小结下上面这两个矩阵，这两个矩阵告诉我们的内容就是，兄弟，线性变换好像真的和矩阵一一对应呢，而且我们主要研究的是基，这个基可以是向量，也可以是像$x^n,n=0,1,2,3\dots$ 这样的因子，所以我们接下来就研究下如何通过已知输入输出空间的基来求矩阵A。</p><h2 id="Construction-of-the-Matrix"><a href="#Construction-of-the-Matrix" class="headerlink" title="Construction of the Matrix"></a>Construction of the Matrix</h2><p>现在我们就要研究最核心的部分了，构造矩阵，如何为任意一个线性变换构造一个矩阵A，通过A来连接输入空间V（dim=n）和输出空间W(dim=m)，方式是$\vec{w}=A\vec{v}$ ,所以矩阵规模与输入空间输出空间对应为$m\times n$<br>下面我们可能要改道了，因为老爷子和《linear algebra done right》都是用了下面这套方法来确定矩阵的：<br>我们选定输入空间基 $\vec{v_1},\vec{v_2},\dots ,\vec{v_n}$ ,输出空间基 $\vec{w_1},\vec{w_2},\dots ,\vec{w_m}$ 我们来分析，V和W各代表一个空间，当$v_i$ 经过T后将会得到W空间一个向量，这个向量是基么？<strong><em>不一定，必须强调一下，这个问题刚刚想明白，我一直在思考如何让$v_i$变成$w_j$</em></strong>，但是这个向量既然是W中的一个向量，就可以用$w_j$ 的线性组合得出来 线性变换后的结果：<br>$$<br>T(v_i)=a_{1i}w_1+\dots + a_{mi}w_m<br>$$<br>这个不要通过代数的方式看，你就从空间的角度思考，因为如果按照代数的思维就凌乱了，所以我们通过上面的这个式子就能得到一个矩阵，这个矩阵每列对应一个输入空间的基，每行对应一个输出空间的基，矩阵的元素就是上面的a们：<br>$$<br>\begin{array}{lc}<br>\mbox{}&amp;<br>\begin{array}{cc}\vec{v_1}&amp;\dots&amp; \vec{v_n}\end{array}\\<br>\begin{array}{c}\vec{w_1}\\\vdots\\\vec{w_m}\end{array}&amp;<br>\left[\begin{array}{cc}<br>a_{11}&amp;\dots&amp;a_{1n}\\<br>\vdots&amp;\ddots&amp;\vdots\\<br>a_{m1}&amp;\dots&amp;a_{mn}<br>\end{array}\right]<br>\end{array}<br>$$</p><p>这个矩阵就是我们梦寐以求的A，当然上面的过程并没有说明为啥排列好了就是A这个说明白，但是我们回忆，我们最初通过方程组定义矩阵不也是没证明么？为啥，因为定义不需要证明啊，我们证明定理，定义只是定理公理结合后的一个新的名字，或者说我们如果没学前面的所有课程，这个A就可以当做矩阵的定义，所有的矩阵都对应一个线性变换，任何一个线性变换都对应一个矩阵（在输入输出空间的基确定以后，不同的基产生不同的矩阵）。</p><p>上面这个矩阵定义非常的重要，因为他是矩阵和线性变换的纽带，必须再强调一遍，线性变换是空间到空间的变换，对于线性变换我们只能知道规模，也就是只能确定$m\times n$ 但是我们不知道其中的具体内容，只有当输入输出空间的基确定了（包括顺序，不同的顺序相当于不同的基），才能确定A。<br>举个🌰 ，还是上面求导的🌰 如果我们交换一下多项式中各项的位置，$Bx+Cx^2+Dx^3+A$ 从代数的角度看这个没有变化，但是老爷子说要把它看成向量，那就麻烦了，新的输入空间V $(x,x^2,x^3,1)$ 输出空间W的基不变 $(1,x,x^2)$<br>那么我们根据定义来求一下A，首先是第一列<br>$$<br>T(v_1)=a_{11}\cdot 1+a_{21}\cdot x+a_{31}\cdot x^2\\<br>\frac{dx}{dx}=1\\<br>so:\\<br>T(v_1)=1\cdot 1+0\cdot x+0\cdot x^2<br>$$<br>所以对应的，矩阵的第一列就是 $\begin{bmatrix}1\\0\\0\end{bmatrix}$<br>迭代下去得到其他元素<br>$$<br>\begin{bmatrix}<br>1&amp;0&amp;0&amp;0\\<br>0&amp;2&amp;0&amp;0\\<br>0&amp;0&amp;3&amp;0<br>\end{bmatrix}<br>$$</p><p>可见当基发生了变化的时候矩阵随之发生变化，但规模不变，仔细观察发现是顺序改变了。<br>求导和积分的🌰给我们了下面这些信息：</p><ol><li>线性变换在微积分，微分方程和线性代数中都有应用</li><li>空间很重要</li><li>线性变换（根据基）都对应一个矩阵并且我们学会了如何求这个矩阵</li></ol><h2 id="Products-AB-Match-Transformations-TS"><a href="#Products-AB-Match-Transformations-TS" class="headerlink" title="Products $AB$ Match Transformations $TS$"></a>Products $AB$ Match Transformations $TS$</h2><p>这个section我们会从线性变换的角度讲解矩阵乘法，我们前面是通过线性组合，然后把矩阵与多个向量相乘合并起来形成了矩阵乘法，但是我们如果从线性变换角度来看，那么矩阵乘法一切顺理成章。<br>如果我们考虑两个线性变换$TS$，和对应的两个矩阵$AB$ 如果矩阵$B$ 将输入空间V线性变换到U那么我们得到$u=S(v)=Bv$，我们继续把u变换到空间W，$w=T(S(v))=T(u)=Au=ABv$ 可见连续的线性变换对应的是矩阵相乘并且规模上是正确的。<br>我们把这个过程拆分开来对应矩阵乘法：<br>在输入空间中，我们只研究一个基$v_1$，他对应的在S中的向量是$b_{11}s_1+\dots b_{m1}s_m$<br>接下来进一步线性变换，<br>基$s_1$ 对应的是$a_{11}t_1+\dots a_{q1}t_q$<br>基$s_m$ 对应的是$a_{1m}t_1+\dots a_{qm}t_q$<br>代入后能得到：<br>$$<br>(a_{11}t_1+\dots a_{q1}t_q)b_{11}<br>\\\vdots\\<br>+(a_{1m}t_1+\dots a_{qm}t_q)b_{m1}<br>$$<br>竖着看，这就是A中各行，和B中的一列相乘得到的是AB的第一列$T(s_1)$ ，确实这个地方有点抽象，但是结合<br>$$<br>\begin{array}{lc}<br>\mbox{}&amp;<br>\begin{array}{cc}\vec{v_1}&amp;\dots&amp; \vec{v_n}\end{array}\\<br>\begin{array}{c}\vec{w_1}\\\vdots\\\vec{w_m}\end{array}&amp;<br>\left[\begin{array}{cc}<br>a_{11}&amp;\dots&amp;a_{1n}\\<br>\vdots&amp;\ddots&amp;\vdots\\<br>a_{m1}&amp;\dots&amp;a_{mn}<br>\end{array}\right]<br>\end{array}<br>$$<br>来看的话连续的线性变换就是多个矩阵乘法，并且前一个矩阵的输入空间维度（列的数量）要和后面一个矩阵的输出维度一致（行的数量），所以矩阵乘法的基本规模要求就是这样确定的。</p><p>这里举个例子，旋转矩阵R，假设旋转了 $\theta$ 角度，那么<br>$$<br>R=<br>\begin{bmatrix}<br>cos(\theta)&amp;-sin(\theta)\\<br>sin(\theta)&amp;cos(\theta)<br>\end{bmatrix}<br>$$<br>如果旋转两次呢，会得到对应的角度 $2\theta$ 么<br>$$<br>RR=\begin{bmatrix}<br>cos(\theta)&amp;-sin(\theta)\\<br>sin(\theta)&amp;cos(\theta)<br>\end{bmatrix}<br>\begin{bmatrix}<br>cos(\theta)&amp;-sin(\theta)\\<br>sin(\theta)&amp;cos(\theta)<br>\end{bmatrix}\\<br>=<br>\begin{bmatrix}<br>cos^2(\theta)-sin^2(\theta)&amp;-2sin(\theta)cos(\theta)\\<br>2sin(\theta)cos(\theta)&amp;-sin^2(\theta)+cos^2(\theta)<br>\end{bmatrix}\\<br>=\begin{bmatrix}<br>cos(2\theta)&amp;-sin(2\theta)\\<br>sin(2\theta)&amp;cos(2\theta)<br>\end{bmatrix}<br>$$<br>计算验证表示没有问题。</p><h2 id="The-Identity-Transformation-and-the-Change-of-basis-Matrix"><a href="#The-Identity-Transformation-and-the-Change-of-basis-Matrix" class="headerlink" title="The Identity Transformation and the Change of basis Matrix"></a>The Identity Transformation and the Change of basis Matrix</h2><p>我们前面反复说明线性变换是空间到空间的，那么我们这个section就来研究一下，如果空间不变，也就是说输入输出空间一致，那么会发生什么，从最表面的来说，应该是方阵，因为输入输出维数相同，不考虑子空间，只考虑完整的空间，那么这个矩阵必然是方阵，所以最特殊的线性变换就是自己到自己，也就是单位矩阵 Identity矩阵，这个矩阵不会对空间以及基做任何变换。<br>上面说的最简单的情况就是基不变空间不变，那么如果空间不变但是我们想要换组基呢？那么我们就按照上面介绍的一般方法做就能求出矩阵M，但是这个M被称为换基矩阵。</p><p>举个🌰 ：<br>输入基 $\vec{v_1}=\begin{bmatrix}3\\7\end{bmatrix},\vec{v_2}=\begin{bmatrix}2\\5\end{bmatrix}$输出基是 $\vec{w_1}=\begin{bmatrix}1\\0\end{bmatrix},\vec{w_2}=\begin{bmatrix}0\\1\end{bmatrix}$<br>那么对应的矩阵中：$\vec{v_1}=a_{11}\vec{w_1}+a_{21}\vec{w_2}$ 这样就可以解得$a_{11}=3,a_{21}=7$,同样的过程解得$a_{12}=2,a_{22}=5$ ，综合上述换基矩阵A:<br>$$<br>A=\begin{bmatrix}3&amp;2\\7&amp;5\end{bmatrix}<br>$$<br>同样的过程如果我们想把矩阵重新换基回到原始基，那么就是上面的逆过程：</p><p>举第二个🌰 ：<br>输入基 $\vec{v_1}=\begin{bmatrix}1\\0\end{bmatrix},\vec{v_2}=\begin{bmatrix}0\\1\end{bmatrix}$ 输出基是 $\vec{w_1}=\begin{bmatrix}3\\7\end{bmatrix},\vec{w_2}=\begin{bmatrix}2\\5\end{bmatrix}$<br>那么对应的矩阵中：$\vec{v_1}=a_{11}\vec{w_1}+a_{21}\vec{w_2}$ 这样就可以解得$a_{11}=5,a_{21}=-7$,同样的过程解得$a_{12}=-2,a_{22}=5$ ，综合上述换基矩阵A:<br>$$<br>B=\begin{bmatrix}3&amp;-2\newline -7&amp;5\end{bmatrix}<br>$$</p><p>可以看出$AB=BA=I$ 互逆的两个换基操作得到对应的互逆矩阵，互逆的线性变换可以保证one-side 逆（上面非方阵的结论）。<br>换基矩阵在同一空间下的基的互相转换。</p><h2 id="Wavelet-Transform-Change-to-Wavelet-Basis"><a href="#Wavelet-Transform-Change-to-Wavelet-Basis" class="headerlink" title="Wavelet Transform = Change to Wavelet Basis"></a>Wavelet Transform = Change to Wavelet Basis</h2><p>标题小波变换，等于换成小波基，这里对小波的介绍并不多只是介绍了最简单的haar小波基的简单介绍，haar小波基主要用于数据压缩，压缩图像等数据，原理和前面的SVD压缩图像差不多，只是那个基是$U$ 和$V^T$ 中的特征向量的乘积，不同的奇异值代表这个向量在原始数据中的重要性，小波基与他不同点在于小波基的基是确定，压缩过程基本一致，无损的换基操作，然后丢弃影响小的部分（有损）完成压缩，然后对压缩后的数据进行逆变换恢复重建原始数据，压缩前后数据肯定是有损失的，那么这项研究的目的就是高压缩比例，小的数据损失。</p><p><img src="https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/Math-Linear-Algebra-Chapter-7-2/compression.png" alt=""></p><h2 id="Fourier-Transform-DFT-Change-to-Fourier-Basis"><a href="#Fourier-Transform-DFT-Change-to-Fourier-Basis" class="headerlink" title="Fourier Transform(DFT)=Change to Fourier Basis"></a>Fourier Transform(DFT)=Change to Fourier Basis</h2><p>离散傅里叶变换包含虚数部分，他的基是：<br>$$<br>F=<br>\begin{bmatrix}<br>1&amp;1&amp;1&amp;1\\<br>1&amp;i&amp;i^2&amp;i^3\\<br>1&amp;i^2&amp;i^4&amp;i^6\\<br>1&amp;i^3&amp;i^6&amp;i^9\\<br>\end{bmatrix}<br>$$<br>这里举了两个应用的例子，说的都不是很详细，只是让我们记住，线性代数在很多地方非常有用，不然人家怎么称得上基础。</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>本文历时24小时终于完成，可能稍微有点点混乱，但是最精彩的部分是构造线性变换矩阵那部分，如何通过空间中的向量和基的关系来表示变换前后的关系，下一篇简答介绍下线性变换在伪逆和对角化中的意义，然后基础理论基本上就算讲完了，后面一波应用等着我们呢，应该可以快些了。</p></div><div><div style="padding:10px 0;margin:20px auto;width:90%;text-align:center"><div>博主全职进行博客创作，您的捐赠是博主的生活来源，如果您认可博客质量，请您多多支持！</div><button id="rewardButton" disable="enable" onclick='var qr=document.getElementById("QR");"none"===qr.style.display?qr.style.display="block":qr.style.display="none"'><span>打赏</span></button><div id="QR" style="display:none"><div id="wechat" style="display:inline-block"><img id="wechat_qr" src="/images/weixin.png" alt="Tony 微信支付"><p>微信支付</p></div><div id="alipay" style="display:inline-block"><img id="alipay_qr" src="/images/alipay.png" alt="Tony 支付宝"><p>支付宝</p></div></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者： </strong>Tony</li><li class="post-copyright-link"><strong>本文链接：</strong> <a href="http://www.face2ai.com/Math-Linear-Algebra-Chapter-7-2/" title="【线性代数】7-2:线性变化的矩阵(The Matrix of a Linear Transformation)">http://www.face2ai.com/Math-Linear-Algebra-Chapter-7-2/</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/Math-Linear-Algebra-Chapter-7-1/" rel="next" title="【线性代数】7-1:线性变换思想(The Idea of a Linear Transformation)"><i class="fa fa-chevron-left"></i> 【线性代数】7-1:线性变换思想(The Idea of a Linear Transformation)</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/Math-Linear-Algebra-Chapter-7-3/" rel="prev" title="【线性代数】7-3:对角化和伪逆(Diagonalization and the Pseudoinverse)">【线性代数】7-3:对角化和伪逆(Diagonalization and the Pseudoinverse) <i class="fa fa-chevron-right"></i></a></div></div></footer></div></article></div><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-1194454329688573" data-ad-slot="2491973880" data-ad-format="auto" data-full-width-responsive="true"></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">站点概览</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/avatar.gif" alt="Tony"><p class="site-author-name" itemprop="name">Tony</p><p class="site-description motion-element" itemprop="description">关注机器学习，深度学习，机器视觉，模式识别</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives"><span class="site-state-item-count">258</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/index.html"><span class="site-state-item-count">17</span> <span class="site-state-item-name">分类</span></a></div></nav><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/tony-tan" target="_blank" title="GitHub" rel="external nofollow"><i class="fa fa-fw fa-github"></i>GitHub</a> </span><span class="links-of-author-item"><a href="mailto:tony.face2ai@gmail.com" target="_blank" title="E-Mail" rel="external nofollow"><i class="fa fa-fw fa-envelope"></i>E-Mail</a> </span><span class="links-of-author-item"><a href="https://twitter.com/Tony_Face2AI" target="_blank" title="Twitter" rel="external nofollow"><i class="fa fa-fw fa-twitter"></i>Twitter</a></span></div><div class="links-of-blogroll motion-element links-of-blogroll-inline"><div class="links-of-blogroll-title"><i class="fa fa-fw fa-link"></i> [object Object]</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"><a href="http://daily-ai.site/" title="更多人工智能博客请关注“每日AI”" target="_blank">更多人工智能博客请关注“每日AI”</a></li></ul></div></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#开篇废话"><span class="nav-number">1.</span> <span class="nav-text">开篇废话</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#The-Matrix-of-a-Linear-Transformation"><span class="nav-number">2.</span> <span class="nav-text">The Matrix of a Linear Transformation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Matrices-for-the-Derivative-and-Integral"><span class="nav-number">3.</span> <span class="nav-text">Matrices for the Derivative and Integral</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Construction-of-the-Matrix"><span class="nav-number">4.</span> <span class="nav-text">Construction of the Matrix</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Products-AB-Match-Transformations-TS"><span class="nav-number">5.</span> <span class="nav-text">Products $AB$ Match Transformations $TS$</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#The-Identity-Transformation-and-the-Change-of-basis-Matrix"><span class="nav-number">6.</span> <span class="nav-text">The Identity Transformation and the Change of basis Matrix</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Wavelet-Transform-Change-to-Wavelet-Basis"><span class="nav-number">7.</span> <span class="nav-text">Wavelet Transform = Change to Wavelet Basis</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Fourier-Transform-DFT-Change-to-Fourier-Basis"><span class="nav-number">8.</span> <span class="nav-text">Fourier Transform(DFT)=Change to Fourier Basis</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Conclusion"><span class="nav-number">9.</span> <span class="nav-text">Conclusion</span></a></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2018</span> <span class="with-love" id="animate"><i class="fa fa-"></i> </span><span class="author" itemprop="copyrightHolder">Tony</span></div><div class="busuanzi-count"><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="site-uv" title="总访客量"><i class="fa fa-user"></i> <span class="busuanzi-value" id="busuanzi_value_site_uv"></span> </span><span class="site-pv" title="总访问量"><i class="fa fa-eye"></i> <span class="busuanzi-value" id="busuanzi_value_site_pv"></span></span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span id="scrollpercent"><span>0</span>%</span></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/js/src/utils.js?v=6.4.0"></script><script type="text/javascript" src="/js/src/motion.js?v=6.4.0"></script><script type="text/javascript" src="/js/src/affix.js?v=6.4.0"></script><script type="text/javascript" src="/js/src/schemes/pisces.js?v=6.4.0"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=6.4.0"></script><script type="text/javascript" src="/js/src/post-details.js?v=6.4.0"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=6.4.0"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      TeX: {equationNumbers: { autoNumber: "AMS" }}
    });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });</script><script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>